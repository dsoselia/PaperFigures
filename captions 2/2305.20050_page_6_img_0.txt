101
102
103
N = number of solutions per problem
62
Outcome Supervised RM
Majority Voting
Figure 3: A comparison of outcome-supervised and process-supervised reward
models, evaluated by their ability to search over many test solutions. Majority
voting is shown as a strong baseline. For N â‰¤ 1000, we visualize the variance
across many subsamples of the 1860 solutions we generated in total per problem.
3
Large-scale Supervision
We train the large-scale PRM using the step-level labels in PRM800K. To ensure
the large-scale ORM baseline is as strong as possible, we train on 100 uniform
samples per problem from the generator. This means the ORM training set has
no overlap with PRM800K, and it is an order of magnitude larger. Although
these two training sets are not directly comparable, each represents our best
attempt to advance the state-of-the-art with each form of supervision. We note
that training the ORM solely on PRM800K solutions would be problematic,
since our active learning strategy has heavily biased the dataset towards wrong-
answer solutions. We did explore training the ORM on a superset of PRM800K
solutions, by mixing in uniformly sampled solutions, but we found that this did
not improve ORM performance.
Figure 3 shows how the best-of-N performance of each reward model varies
as a function of N. Since majority voting is known to be a strong baseline (Wang
et al., 2022; Lewkowycz et al., 2022), we also include this method as a point of
comparison. While the ORM performs slightly better than the majority voting
baseline, the PRM strongly outperforms both. Not only does the PRM reach