Summary of ChatGPT-Related Research
Figure 2: Word cloud analysis of all the 194 papers.
2.1. Application of ChatGPT
2.1.1. Question And Answering
In the field of education
ChatGPT is commonly used for question and answers testing in the education sector. Users can use ChatGPT to
learn, compare and verify answers for different academic subjects such as physics, mathematics, and chemistry, and/or
conceptual subjects such as philosophy and religion. Additionally, users can ask open-ended and analytical questions
to understand the capabilities of ChatGPT.
In the field of mathematics, Frieder et al. [17] constructed the GHOSTS natural language dataset, which consists
of graduate-level math test questions. The authors tested ChatGPT’s math abilities on the GHOSTS dataset using
a question-and-answer format and evaluated it according to fine-grained standards.In the Grad Text dataset, which
covers simple set theory and logic problems, ChatGPT performed the best. However, in the Olympiad-Problem-Solving
dataset, ChatGPT performed poorly, receiving only two 4-point scores (out of a total of 5), with the majority of scores
being 2 points. In the Holes-in-Proofs dataset, ChatGPT received the lowest score of 1 point. In the MATH dataset,
ChatGPT only scored impressively in 26% of cases. These results suggest that ChatGPT’s math abilities are clearly
lower than those of ordinary math graduate students. Although ChatGPT can generally understand math problems, it
fails to provide the correct solutions. Pardos et al. [74] used the Open Adaptive Tutoring system (OATutor) to investigate
whether prompts generated by ChatGPT were helpful for learning algebra, with 77 participants from Mechanical
Turk taking part in the experiment. The experiment used questions from OpenStax’s Elementary and Intermediate
Algebra textbooks. These participants were randomly assigned to either a control group (with manual prompts) or an
experimental group (with ChatGPT prompts). For each question in both courses, the authors obtained answers from
ChatGPT through a question-and-answer format and evaluated scores according to three criteria: ChatGPT provided
an answer, the answer was correct, and inappropriate language was not used in the answer. The study found that 70%
of prompts generated by ChatGPT passed manual quality checks, and both humans and ChatGPT produced positive
learning gains. However, the scores of human prompts ranged from 74.59% to 84.32%, significantly higher than those
of ChatGPT prompts. Shakarian et al. [82] studied the performance of ChatGPT on math word problems (MWPs),
using the DRAW-1K dataset for experimentation. The dataset consists of 1000 MWPs and their answers, along with
algebraic equation templates for solving such problems. The authors used the idea of machine learning introspection
and built performance prediction models using random forests and XGBoost, and evaluated them on the dataset using
Yiheng Liu et al.: Preprint submitted to Elsevier
Page 3 of 21