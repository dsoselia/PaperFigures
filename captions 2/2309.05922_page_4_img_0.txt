Figure 4: Instances of object hallucination within LVLMs (Li et al., 2023e). Ground-truth objects in annotations
are indicated in bold, while red objects represent hallucinated objects by LVLMs. The left case occurs in the
conventional instruction-based evaluation approach, while the right cases occur in three variations of POPE.
challenges, we introduce the Hallucinator, which
efficiently generates additional positive samples
to enhance contrast. The Hallucinator is differ-
entiable, operating in the feature space, making
it amenable to direct optimization within the pre-
training task and incurring minimal computational
overhead.
Efforts to enhance LVLMs for complex multi-
modal tasks, inspired by LLMs, face a significant
challenge: object hallucination, where LVLMs gen-
erate inconsistent objects in descriptions. This
study (Li et al., 2023e) systematically investigates
object hallucination in LVLMs and finds itâ€™s a
common issue. Visual instructions, especially fre-
quently occurring or co-occurring objects, influ-
ence this problem. Existing evaluation methods
are also affected by input instructions and LVLM
generation styles. To address this, the study intro-
duces an improved evaluation method called POPE,
providing a more stable and flexible assessment of
object hallucination in LVLMs.
Instruction-tuned Large Vision Language Mod-
l (LVLM ) h
d
i
ifi
i h
prehensive dataset for detecting hallucinations in
detailed image descriptions.
4
Hallucination in Large Video Models
Hallucinations can occur when the model makes in-
correct or imaginative assumptions about the video
frames, leading to the creation of artificial or erro-
neous visual information Fig. 5.