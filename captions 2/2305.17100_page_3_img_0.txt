,
y
g
g
pi
p
encoding. This can result in reduced model flexibility and suboptimal performance when faced with novel
or complex tasks. Thus, we propose the adoption of an encoder-decoder architecture for BiomedGPT due
to its exceptional ability to effectively map various modalities into a unified semantic representation space
while successfully handling a wide range of diverse tasks.
We follow OFA (Wang et al., 2022b) to design BiomedGPT, which takes BART (Lewis et al., 2019) as the
backbone that is implemented as a sequence-to-sequence model with a BERT-style encoder over corrupted
text and a GPT-style left-to-right autoregressive decoder. We make a few architectural changes to adapt
the BART architecture for BiomedGPT. First, to improve the convergence efficiency and stability in the
pretraining, we add three normalization operations to each layer: a post-attention Layer Norm (LN) (Ba
et al., 2016), post-first-FFN LN, and head-wise scaling within self-attention, following (Shleifer et al., 2021).
To encode positional information, we incorporate two sets of absolute position embeddings for both text and
images. Rather than merely combining these embeddings with token and patch embeddings, we implement
a decoupling method to separate position correlation (Kitaev & Klein, 2018; Ke et al., 2019). Furthermore,
we also incorporate 1D relative position bias for text and 2D relative position bias for image, as described
in previous works (Raffel et al., 2020; Dai et al., 2021; Wang et al., 2022d).
2.2
Input/Output Unification
To enable inputs with a wide range of modalities, including images, language, and bounding boxes, to be
processed within a single model, it is necessary to embed them in a shared and unified space. For visual inputs,
we directly apply CNN backbones to relax the heavy image feature extraction process, including object
detection, following (Kim et al., 2021). Specifically, BiomedGPT receives the raw image xv ∈ RH×W ×C
and maps it into a flattened 1D sequence of patches xp ∈ RN×D via a ResNet module as input for the
transformer, where N = HW
P 2
is the number of patches given the patch size of P × P, and D is the fixed
hidden size of the transformer layers. We choose the first three blocks of ResNet layers due to their observed
4