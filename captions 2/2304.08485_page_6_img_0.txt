GPT-4 [32]
The unusual thing about this image is that a man is ironing clothes on an ironing board
attached to the roof of a moving taxi.
User
Can you explain this meme in detail?
BLIP-2
a man is sitting on the back of a yellow cab
User
Can you explain this meme in detail?
OpenFlamingo
The man is drying his clothes on the hood of his car.
Table 4: Example prompt demonstrating LLaVA and GPT-4’s visual input capability. The prompt
requires image understanding.
complementary to its conversational capabilities. Finally, we show that having all three types of the
data yields the best performance of 85.1%. We hope this evaluation protocol serves as a starting point
for a comprehensive evaluation and understanding of the capability of large multimodal models.
5.2
ScienceQA
ScienceQA [30] contains 21k multimodal multiple choice questions with rich domain diversity across
3 subjects, 26 topics, 127 categories, and 379 skills. The benchmark dataset is split into training,
validation, and test splits with 12726, 4241, and 4241 examples, respectively. We consider two
representative methods, including GPT-3.5 model (text-davinci-002) with and without chain-of-
thoughts (CoT), LLaMA-Adapter [55], as well as multimodal chain-of-thoughts (MM-CoT) [57],
which is the current SoTA method on this dataset. For more baseline numbers, please see [30].
The results are reported in Table 6. For LLaVA, we use the visual features before the last layer,
ask the model to ﬁrst predict reasons then the answer, and train it for 12 epochs. It yields 90.92%
accuracy, which is quite close to the SoTA 91.68%. To explore the limit of LLMs, we also prompt
GPT-4 using 2-shot in-context-learning and achieve 82.69% accuracy, which is a 7.52% absolute
gain compared with 75.17% from GPT-3.5. For a substantial number of questions, we note that
GPT-4 fails simply because it reports that there is insufﬁcient context such as images or plots. We
consider two schemes to combine the outcomes from our model and GPT-4. (i) A GPT-4 complement.
7