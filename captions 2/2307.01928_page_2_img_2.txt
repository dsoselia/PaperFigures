……
ption is correct?
Augmented Context
a few-shot prompt that includes possible next steps in a few scenarios (Fig. A1), the LLM generates a
set {yi} of candidate next steps (e.g., “Put plastic bowl in microwave”, “Put metal bowl in microwave”,
etc., in Fig. 1) that are semantically different. Then the task of choosing among them is formatted as
multiple-choice Q&A (MCQA). This eliminates plans that the LLM considers unlikely and reduces
the problem of next-step prediction down to a single next-token prediction — aligning with LLM
log-likelihood loss functions and LLM training data (e.g., MCQA datasets [7, 8]). These probabilities
can serve as normalized scores that can be used by various uncertainty quantification methods such as
thresholding and ensemble methods. In this work, we use these normalized scores within a conformal
prediction (CP) framework. Specifically, CP uses a held-out calibration set of example plans in different
scenarios to generate a reduced prediction set of plans among {yi} (Fig. 2). The LLM is certain if this
prediction set is a singleton, and triggers help from a human otherwise. Section A1 details additional
rationale of applying MCQA to evaluate the semantic uncertainty of the LLM.
Robots that ask for help.
In this work, we show that LLM planning — combined with CP for
uncertainty estimation — can effectively enable robots to interact with an environment, and ask for help
when needed. The environment e can be formulated as a partially observable Markov decision process
(POMDP): at any given state st at time t, given a user instruction ℓ, the robot executes an action at
according to a policy π, then transitions to a new state st+1. Our policy π is composed of four parts (Fig. 1):
1. Multiple-choice generation: An LLM generates a diverse set of candidate plans labeled with ‘A’, ‘B’,
‘C’, ‘D’ , and an additional possible plan, ‘E) an option not listed here’, which is appended post-hoc.
We denote the set of labels by Y :={‘A’,‘B’,‘C’,‘D’,‘E’}. These plans are generated by prompting
the LLM with context xt, which is text that includes (1) the robot observation at each time step (e.g.,
using a vision-based object detector or an oracle; see Fig. 1), (2) the user instruction, and (3) few-shot