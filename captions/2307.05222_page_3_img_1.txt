Encoder
（EVA-CLIP）
Multimodal Modeling with LLM
(LLaMA)
…
Classification
[/IMG] An
[IMG]
egg that will
</s>
<s>
emu
hatch into a
baby emu
[/IMG]
[IMG]
…
emu egg that will
hatch into a
baby emu
An
</s>
[IMG]
[IMG]
Regression
…
…
Decoder
(Stable Diffusion)
Causal Transformer
Figure 2: Emu unifies the modeling of different modalities in an auto-regressive manner. Visual
signals are first encoded into embeddings, and together with text tokens form an interleaved sequence.
The training objective is to either classify the next text token or regress the next visual embedding.
In inference, regressed visual embeddings are decoded into a realistic image via a fine-tuned latent
diffusion model.
append <s> and </s> tokens to the start and the end of each sequence. In inference, we fine-tune the
Visual Decoder to decode the visual embeddings into a realistic image.
Causal Image-text Transformer. Auto-regressively modeling images in raster order is counter-
intuitive and has not demonstrated satisfactory performance, which may be attributed to the fact
that images naturally possess 2D structures and are not perceived as sequential signals like text. To
better capture the characteristics of images and achieve unified modeling of different modalities, we
propose a Causal Transformer module to transform 2D spatial visual signals to 1D causal sequences
in a latent space Z. Specifically, given an image I with its encodings g(I) from EVA-CLIP, Causal
Transformer accepts randomly initialized embeddings {e1, e2, . . . , eN} as input, and outputs N
embeddings {z1, z2, . . . , zN} that capture the causal dependency of the given image:
z1, z2, . . . , zN = CausalTransformer (g(I), {e1, e2, . . . , eN})
(1)
The architecture of Causal Transformer is similar to the decoder of Transformer [58], with each
block consisting of a causal self-attention layer, a cross-attention layer, and a feed-forward layer.
Different from Q-Former [33] that captures bi-directional relations of input tokens, we use a causal
self-attention layer to capture the causal dependency among the input latent embeddings for further
unified causal modeling of vision and language modalities. The cross-attention layer aggregates visual
information from the image embeddings extracted from EVA-CLIP, where the visual embeddings are
treated as keys and values, and the outputs from the previous causal attention layer serve as queries.
Visual Decoder. We use a latent diffusion model to decode visual embeddings into images, and adopt
the weights of Stable Diffusion [51] as initialization. Specifically, we feed N visual embeddings
generated by Emu into the diffusion model as conditions for image decoding. We replace the
linear projections of the cross-attention modules in Stable Diffusion with new linear layers that
accommodate the dimension of Emu and Stable Diffusion.
2.2
Training Objective
Given an unlabeled web-scale corpora D consisting of interleaved multimodal sequences x =
(x1, x2, . . . , xn), where x can be vision-language sequences of various forms, such as image-text
pairs, image-text interleaved documents, or videos with subtitles. xi can be a signal unit (text or
image token) from any arbitrary modality. We first convert all continuous 2D signals (images and
video frames) into 1D causal latent embedding sequences using Causal Transformer, then insert
them back into the corresponding places in the sequence x. The resulting sequence is represented
as u = (u1, u2, . . . , um), where ui can be either a discrete text token, or a visual embedding that
captures causal dependency with neighboring visual embeddings.
4