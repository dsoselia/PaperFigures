g
(
g
g
)
the context of the prompt for generation. Table 1 shows the statistics of the top N retrieved chunks
while Figure 1 gives more details of the token length distribution of all seven datasets. Note that,
some dataset like Qasper (QASP) is relatively short and don’t have up to 20 chunks, so the average
length of top-10 chunks and top-20 chunks are close. We can see that top-5 chunks can all fit into
4k sequence length (except few outliers) while top-10 and top-20 chunks can fit into 16k sequence
length.
3.5
INSTRUCTION TUNING
To train the pretrained LLMs to follow instructions for question answering or text summarization, we
also performed instruction tuning. We first construct a blend of instruction tuning datasets consisting
of 102K training samples from the Soda dataset (Kim et al., 2022), ELI5 dataset (Fan et al., 2019),
FLAN dataset (Wei et al., 2021) , Open Assistatant dataset (Köpf et al., 2023), Dolly (Conover et al.,
2023) and a proprietary sourced conversational dataset, to adapt both GPT-43B and LLaMA2-70B
to follow instructions. In terms of the template, we use "System: {System}\n\nUser: {Ques-
tion}\n\nAssistant: {Answer}" as the format to support multi-turn dialogue training. As all of the
tasks contain the context information for reasoning over at inference time, we add the context before
the dialogue, i.e. "System: {System}\n\n{Context}\n\nUser: {Question}\n\nAssistant: {Answer}".
We finetune the LLM by taking the loss only on the {Answer} part with batch size 128 and learning
rate of 5e-6 for 1000 steps. For the rest of the paper, results are all reported using the instruction
tuned chat model on top of the foundational GPT-43B and LLaMA2-70B.
6