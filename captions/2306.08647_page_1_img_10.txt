User
The robot dog's torso is upright, 
balanced over its hind feet, which 
are ﬂat and shoulder-width apart. 
The front legs hang loosely, poised 
mid-air, mimicking a human's 
relaxed arms.
# Set torso rewards
set_torso_rewards(height=0.7, pitch=np.deg2rad(90))
# Set feet rewards
set_feet_pos_rewards('front_left', height=0.7)
set_feet_pos_rewards('back_left', height=0.0)
set_feet_pos_rewards('front_right', height=0.7)
set_feet_pos_rewards('back_right', height=0.0)
Make robot dog 
stand up on two feet.
LLM
User
Make robot dog 
stand up on two feet.
Reward Translator
(LLM)
Motion Controller
set_joint_target(0.0, 0.2, 0.7, 
0.0, -0.3, 0.8, 0.0, 0.2, 0.7, 
0.0, -0.3, 0.8)
User
Make robot dog 
stand up on two feet.
LLM
Low-level action
Motion description
Reward code
Optimized 
low-level actions
Figure 1: LLMs have some internal knowledge about robot motions, but cannot directly translate them into actions
(left). Low-level action code can be executed on robots, but LLMs know little about them (mid). We attempt to bridge
this gap, by proposing a system (right) consisting of the Reward Translatorthat interprets the user input and transform
it into a reward specification. The reward specification is then consumed by a Motion Controller that interactively
synthesizes a robot motion which optimizes the given reward.
bridges the gap between language and low-level robot actions. This is motivated by the fact that language
instructions from humans often tend to describe behavioral outcomes instead of low-level behavioral details
(e.g. “robot standing up” versus “applying 15 Nm to hip motor”), and therefore we posit that it would be
easier to connect instructions to rewards than low-level actions given the richness of semantics in rewards.
In addition, reward terms are usually modular and compositional, which enables concise representations
of complex behaviors, goals, and constraints. This modularity further creates an opportunity for the user
to interactively steer the robot behavior. However, in many previous works in reinforcement learning (RL)
or model predictive control (MPC), manual reward design requires extensive domain expertise [17, 18, 19].
While reward design can be automated, these techniques are sample-inefficient and still requires manual
specification of an objective indicator function for each task [20]. This points to a missing link between
the reward structures and task specification which is often in natural language. As such, we propose to
utilize LLMs to automatically generate rewards, and leverage online optimization techniques to solve them.
Concretely, we explore the code-writing capabilities of LLMs to translate task semantics to reward functions,
and use MuJoCo MPC, a real-time optimization tool to synthesize robot behavior in real-time [21]. Thus
reward functions generated by LLMs can enable non-technical users to generate and steer novel and intricate
robot behaviors without the need for vast amounts of data nor the expertise to engineer low-level primitives.
The idea of grounding language to reward has been explored by prior work for extracting user preferences
and task knowledge [22, 23, 24, 25, 26, 27]. Despite promising results, they usually require training
data to learn the mapping between language to reward. With our proposed method, we enable a data
efficient interactive system where the human engages in a dialogue with the LLM to guide the generation
of rewards and, consequently, robot behaviors (Fig. 1).
Across a span of 17 control problems on a simulated quadruped and a dexterous manipulator robot, we show
that this formulation delivers diverse and challenging locomotion and manipulation skills. Examples include
getting a quadruped robot to stand up, asking it to do a moonwalk, or tasking a manipulator with dexterous
hand to open a faucet. We perform a large-scale evaluation to measure the overall performance of our
proposed method. We compare our method to a baseline that uses a fixed set of primitive skills and an alter-
native formulation of grounding language to reward. We show that our proposed formulation can solve 40%
more skills than baselines and is more stable in solving individual skills. We further deploy our approach
to a real robot manipulator and demonstrate complex manipulation skills through language instructions.
2
Related Work
Here we discuss relevant prior work that reason about language instructions to generate robot actions,
code, or rewards to ground natural language to low-level robotic control. We then discuss work focused
on responding to interactive human feedback such as language corrections.
Language to Actions. Directly predicting low-level control actions based on a language instruction has
been studied using various robot learning frameworks. Early work in the language community studied
mapping templated language to controllers with temporal logic [28] or learning a parser to motion prim-
2