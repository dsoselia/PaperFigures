Figure 4: Comparison of our method and alternative methods in terms of pass rate: if we generate N pieces of code
for each task and pick the best performing one, whatâ€™s the percentage of tasks that the system can successfully tackle.
When compared to the CaP baseline, our method achieves better success rate in almost all tasks. This is
due to that CaP can perform well on tasks that can be expressed by the given primitives (e.g. Touch object)
or very close to the given examples in prompt (e.g. Sit down), but fails to generalize to novel low-level
skills. On the other hand, using Reward Coder only can achieve success on some tasks but fails in ones that
requires more reasoning. For example, when asked to open a drawer, the Reward Coder only baseline often
forget to task the robot hand to get closer to the drawer handle and only design the reward for encouraging
the drawer to be open. Sampled responses from different method can be found in Appendix A.8.
To further understand the overall performance of different systems, we also show the pass rate in Fig. 4
right, which is a standard metric for analyzing code generation performance [8]. For each point in the plot,
it represents the percentage of tasks the system can solve, given that it can generate N pieces of code for
each task and pick the best performing one. As such, the pass rate curve measures the stability of the system
(the more flat it is, the more stable the system is) as well as the task coverage of the system (the converged
point represents how many tasks the system can solve given sufficient number of trials). It is clear from the
result that for both embodiments, using reward as the interface empowers LLMs to solve more tasks more
reliably, and the use of Structured Motion Description further boosts the system performance significantly.
4.5
Interactive Motion Synthesis Results
One benefit of using a real time optimization tool like MJPC is that humans can observe the motion being
synthesized in real time and provide feedback. We showcase two examples where we teach the robot to
perform complex tasks through multiple rounds of interactions. In the first example, we task the quadruped
robot to stand up and perform a moon-walk skill (Fig. 5a). We give four instructions to achieve the task,
as shown in Fig. 5. Each instruction improves the behavior towards the desired behavior based on the
interactively synthesized results. This showcase that users can interactively shape the behavior of the robot
in natural language. In the second example, we showcase a different way of leveraging the interactivity
of our system by sequentially commanding the dexterous manipulator robot to place an apple in a drawer,
as seen in Fig. 5b. Results of the interactive results are best viewed in the supplementary video and full
code output from our method can be found in Appendix A.9.
4.6
Real-robot experiments
We implement a version of our method onto a mobile manipulator, and tested it on nonprehensile
manipulation tasks in the real world. In simulation, we have access to the ground-truth state for objects
in the scene. In the real world, we detect objects in image-space using an open-vocabulary detector:
F-VLM [54]. We extract the associated points from point cloud behind the mask and perform outlier
rejection for points that might belong to the background. From a birds-eye view, we fit a minimum volume
rectangle and take the extremes to determine the extent in the z-axis. We use this 3D bounding box as
state estimation for corresponding object in simulation. To detect the surface of the table with proper
orientation, we use an AprilTag [55]. In addition, as seen in the supplementary video, MJPC can discover
highly dexterous and dyanmic maneuvers to accomplish the desired task. However, these movements are
7