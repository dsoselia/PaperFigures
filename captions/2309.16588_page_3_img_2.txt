(a) Cosine similarity to neighbors.
(b) Linear probing for local information.
Figure 5: (a): Distribution of cosine similarity between input patches and their 4 neighbors. We
plot separately artifact patches (norm of the output token over 150) and normal patches. (b): Local
information probing on normal and outlier patch tokens. We train two models: one for predicting
position, and one for reconstructing the input patch. Outlier tokens have much lower scores than the
other tokens, suggesting they are storing less local patch information.
High-norm tokens appear where patch information is redundant.
To verify this, we measure
the cosine similarity between high-norm tokens and their 4 neighbors right after the patch em-
bedding layer (at the beginning of the vision transformer). We illustrate the density plot in Fig.
5a. We observe that high-norm tokens appear on patches that are very similar to their neighbors.
This suggests that these patches contrain redundant information and that the model could discard
their information without hurting the quality of the image representation. This matches qualitative
observations (see Fig. 2) that they often appear in uniform, background areas.
High-norm tokens hold little local information.
In order to better understand the nature of these
tokens, we propose to probe the patch embeddings for different types of information. For that we
consider two different tasks: position prediction and pixel reconstruction. For each of these tasks,
we train a linear model on top of the patch embeddings, and measure the performance of this
model. We compare the performance achieved with high-norm tokens and with other tokens, to see
if hi h
k
i diff
i f
i
h
“
l”
k