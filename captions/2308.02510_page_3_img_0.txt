are the sampled anchor, positive, and negative EEG signal
segments, respectively. The objective of eq. (1) is to mini-
mize the distance between xa and xp with the same labels
(the class of viewed visual stimuli) while maximizing the
distance between xa and xn with different labels. To avoid
the compression of data representations into a small cluster
by the feature extraction network, a margin term β is incor-
porated into the triplet loss.
Estimation of Saliency Map
After we obtain the feature
of EEG signal fθ(x), we can now generate the saliency map
of silhouette information from it and a random sampled la-
tent z ∼ N(0, 1), i.e.,
Mp(x) = G(z, fθ(x)).
G denotes for the saliency map generator. In this paper,
we use the generator from the Generative Adversarial Net-
work(GAN) (Goodfellow et al. 2020) framework to generate
the saliency map and the adversarial loss is defined as fol-
lows:
LD
adv = max(0, 1 − D(A(y), fθ(x)))+
max(0, 1 + D(A(Mp(x))), fθ(x))),
(2)
LG
adv = − D(A(Mp(x)), fθ(x)).
(3)
In GAN, besides the generator G, a discriminator D is in-
troduced to distinguish between images from the generator
and ground truth images x. It is optimized by minimizing the
hinge loss (Lim and Ye 2017) defined in Equation (2). A is
the differentiable augmentation function (Zhao et al. 2020).
To stabilize the adversarial training process and alleviate the
problem of mode collapse, we add the mode seeking regu-
larization (Mao et al. 2019):
Lms = −
�dx (G (z1, fθ(x)) , G (z2, fθ(x)))
dz (z1, z2)
�
,
(4)
where d∗(·) denotes the distance metric in image space x or
latent space z and z1, z2 ∼ N(0, 1) are two different sam-
pled latent vectors.
To enforce the accuracy of the generated saliency map
from the visual stimuli, we use the observed image as super-
vision and incorporate the Structural Similarity Index Mea-
sure (SSIM) as well:
LSSIM = 1 −
�
2µxµMp(x) + C1
� �
2σxσMp(x) + C2
�
�
µ2x + µ2
Mp(x) + C1
� �
σ2x + σ2
Mp(x) + C2
�,
(5)
where µx, µMp(x), σx, and σMp(x) represent the mean and
standard values of the ground truth images and reconstructed
saliency maps of the generator. C1 and C2 are constants to
stabilize the calculation.
The final loss for the generator is the weighted sum of the
losses:
LG = α1 · LG
adv + α2 · Lms + α3 · LSSIM,
(6)
and αi∈{1,2,3} are hyperparameters to balance the loss terms.
Sample-level Semantics Extraction
As aforementioned, the EEG signals are notorious for their
inherent noise, making it challenging to extract both pre-
cise and fine-grained information. Therefore, besides fine-
grained pixel-level semantics, we also involve sample-level
semantic extraction methods to derive some coarse-grained
information such as the category of the main objects of the
image content. These features have a relatively lower rank
and are easier to be aligned. Despite being less detailed,
these features can still provide accurate coarse-grained in-
formation, which is meaningful to reconstruct the observed
visual stimuli.
Specifically, the process Ms will try to align the infor-
mation decoded from the input EEG signals to some gen-
erated image captions, which are generated by some other
additional annotation model such as Contrastive Language-
Image Pretraining (CLIP) model (Radford et al. 2021). Be-
low we detail the processes of image caption ground-truth
generation and semantic decoding with alignment.
An image of 
african elephant
An elephant 
standing next to a 
large rock
An image of 
mountain bike
A man riding a 
mountain bike down 
a trail in the woods
An image of
parachute
A person flying 
a parachute in 
the air with a 
banner
An image 
of daisy
A red and white 
flower with 
yellow center
GT images
Label captions
BLIP captions
Figure 2: Examples of ground-truth images, label captions,
and BLIP captions, respectively.
Generation of Image Captions
We propose two methods
to generate the caption for each image to help supervise the
decoding procedure of semantic information from EEG sig-
nals. Since the observed images are from ImageNet dataset
containing the class of the image, we define a straightfor-
ward and heuristic method of label caption, which utilizes
the class name of each image as the caption, as illustrated in
the middle column of Figure 2. The second method is that
we use an image caption model BLIP (Li et al. 2023), which
is a generic and computation-efficient vision-language pre-
training (VLP) model utilizing the pretrained vision model