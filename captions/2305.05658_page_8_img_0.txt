that improvements to LLM summarization could
enable further gains for our method in the future.
Different
LLMs. Table 5 reports our per-
formance
on
the
benchmark
using
different
LLMs.
We
find
that
text-davinci-002
and
code-davinci-002 (Chen et al., 2021), which are
older variants of GPT-3, are not as good as
1. Evaluate whether humans prefer the object
placements generated by our LLM summariza-
tion method over those of the CLIP embed-
dings baseline
2. Evaluate
whether
human-preferred
object
placements align with the ground truth place-
ments in our benchmark
9