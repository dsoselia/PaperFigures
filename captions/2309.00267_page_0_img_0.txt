Abstract
Reinforcement learning from human feedback
(RLHF) is effective at aligning large language
models (LLMs) to human preferences, but gath-
ering high-quality human preference labels is
a key bottleneck. We conduct a head-to-head
comparison of RLHF vs. RL from AI Feed-
back (RLAIF) - a technique where preferences
are labeled by an off-the-shelf LLM in lieu of
humans, and we find that they result in similar
improvements. On the task of summarization,
human evaluators prefer generations from both
RLAIF and RLHF over a baseline supervised
fine-tuned model in âˆ¼70% of cases. Further-
more, when asked to rate RLAIF vs. RLHF
summaries, humans prefer both at equal rates.
These results suggest that RLAIF can yield
human-level performance, offering a potential
solution to the scalability limitations of RLHF.
1
Introduction
Reinforcement Learning from Human Feedback
(RLHF) is an effective technique for aligning lan-
guage models to human preferences (Stiennon
et al., 2020; Ouyang et al., 2022) and is cited as
one of the key drivers of success in modern conver-
sational language models like ChatGPT and Bard
(Liu et al., 2023; Manyika, 2023). By training
with reinforcement learning (RL), language mod-
els can be optimized on complex, sequence-level
objectives that are not easily differentiable with
traditional supervised fine-tuning.
The need for high-quality human labels is an
obstacle for scaling up RLHF, and one natural
question is whether artificially generated labels can
achieve comparable results. Several works have
shown that large language models (LLMs) exhibit
a high degree of alignment with human judgment -
Figure 1: Human evaluators strongly prefer RLHF and
RLAIF summaries over the supervised fine-tuned (SFT)
baseline. The differences in win rates between RLAIF vs.
SFT and RLHF vs. SFT are not statistically significant.
Additionally, when compared head-to-head, RLAIF is
equally preferred to RLHF by human evaluators. Error
bars denote 95% confidence intervals.
technique called "Reinforcement Learning from
AI Feedback" (RLAIF)1. While they showed that
utilizing a hybrid of human and AI preferences
in conjunction with the "Constitutional AI" self-
revision technique outperforms a supervised fine-
tuned baseline, their work did not directly compare
the efficacy of human vs. AI feedback, leaving
the question unanswered whether RLAIF can be a
suitable alternative to RLHF.
In this work, we directly compare RLAIF against
RLHF on the task of summarization. Given a text
and two candidate responses, we assign a prefer-
ence label using an off-the-shelf LLM. We then
train a reward model (RM) on the LLM prefer-
ences with a contrastive loss. Finally, we fine-tune
a policy model with reinforcement learning, using
1We use "RLAIF" to denote training a reward model on AI-
l b l d
f
f ll
d b
d
i
RL fi
i
arXiv:2309.00267v1  [cs.CL]  1 Sep 2023