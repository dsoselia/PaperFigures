Vision Encoder
Resampler
text tokens
image tokens
[task prompt 1] 
+
<bbox> [111,73,447,124] </bbox> 
<text> PEGGY CARTER </text> <bbox> 
… </bbox>
…
text tokens
image tokens
[task prompt 2] 
+
<text> … **List Price:** $499.00 
**Price:**  $457.38 **You Save:** 
… </text>
Render
Render
shared decoder-only Transformer
Figure 2: Model architecture of KOSMOS-2.5. A shared decoder-only Transformer model generates
the output text sequence based on the input image from a vision encoder and different task prompts.
2
KOSMOS-2.5
2.1
Model Architecture
The model architecture of KOSMOS-2.5 consists of a pre-trained vision encoder and a language
decoder connected with a resampler module, shown in Figure 2. We adopt the pre-trained vision
encoder based on the Vision Transformer (ViT) [DBK+21]. We further adapt a Perceiver Resampler
module with an attentive pooling mechanism to reduce the size of image embeddings [ADL+22].
The language decoder is built upon the Transformer-based decoder to condition on image and text
context for the next token prediction.
2.2
Image and Text Representations
KOSMOS-2.5 takes a composite input consisting of an image and a text representation. The image
representation is uniform across various configurations and leverages a variable-resolution input
strategy following Pix2Struct [LJT+23]. Precisely, we extract the maximum number of fixed-size
patches (16×16) that can fit within a predefined sequence length L. In addition, Resampler [ADL+22]
is used as an attentive pooling mechanism to reduce the number of image embeddings. The text
representation, however, is more versatile and can be one of two types: text lines with bounding
boxes or plain markdown texts.