We find that the PRM outperforms the ORM at large scale, but this result alone
paints an incomplete picture. To better compare outcome and process supervi-
sion, there are two confounding factors that must be isolated. First, the training
sets for the ORM and the PRM are not directly comparable: the PRM training
set was constructed using active learning, is biased towards answer-incorrect
solutions, and is an order of magnitude smaller. Second, the final-answer grad-
ing will provide positive labels to spurious solutions that reach the correct final
answer despite incorrect reasoning. This could damage ORM performance, an
effect we may or may not want to attribute to outcome supervision more gen-
erally.
Due to the high cost of collecting human feedback, we cannot easily ablate
these factors using human labelers. We instead perform the relevant ablations