‚Ñ∞
ùíü
Cross 
Attn
Cross 
Attn
as an oil painting in the style of
üî• Multimodal Large Language Model
üî• AlignerNet
Interleaved Vision-Language Prompt
‚ùÑ
‚ùÑ
Figure 2: KOSMOS-G comprises an MLLM for multimodal perception, coupled with an AlignerNet
that bridges the MLLM to the diffusion U-Net image decoder. KOSMOS-G can pass the fine concept-
level guidance from interleaved input to image decoder, and offer a seamless alternative to CLIP.
Orange denotes the trainable modules; Blue denotes the frozen ones.
from the multimodal language modeling stage, leading to the KOSMOS-1 [HDW+23] MLLM. It
envisions language models as a universal task layer, perceiving free-form interleaved vision-language
inputs and consolidating various task predictions into textual formats. Given the aligned vision-
language representation, we then use the language modality as an anchor and align the output space
of the MLLM with the CLIP text encoder. Finally, we perform instruction tuning on the curated data.
KOSMOS-G accepts captions as input, where each entity is followed by its segmented image. The
model is trained to faithfully reproduce all entities, render the text content, and follow the instructions.
In this process, the frozen pre-trained diffusion image decoder serves as a score metric. We distill the
learned data distribution to pass the differentiable gradient to the MLLM. This enables KOSMOS-G to
harness rich features from the image encoder to generate images faithfully reproducing the contents
across various contexts (see Figure 1).
Benefiting from general-purpose pre-training, KOSMOS-G approaches the objective of ‚Äúimage as a
foreign language in image generation.‚Äù This means KOSMOS-G can capture novel concepts from
input images and guide personalized creations in a zero-shot setting. Notably, KOSMOS-G also
stands as the first model to master zero-shot multi-entity subject-driven generation. Owing to the
score distillation instruction tuning, KOSMOS-G do not need to modify any parameters of the image