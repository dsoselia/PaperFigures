ï¿½ï¿½ï¿½
p
gy
( g
)
p
y
selected joint angles of the human to conceptually similar joints of the creature. For joints where
no correspondence can be found, we just set them to their default pose (e.g. ears and tails). This
provides a rough estimate of the creatureâ€™s motion. However, this motion has many artifacts, such
a feet sliding due to different leg lengths, self-collisions, floor collisions, and no motion of the tail
and ears. Nonetheless, we can still use it as a reward signal to train our simulated character. The
physical constraints imposed by the simulation then remove remaining artifacts. Importantly, after
the simulated character is trained, it is driven only by a headset and controllers, without requiring
any full-body information of the user or any kinematic retargeting.
3.5
Reward
The goal for the simulated character is to imitate the human motion as closely as possible, while
respecting all the constraints imposed by physics. Our reward function includes a component for
imitation, contact, and action regularization:
ğ´ï¿½ï¿½ï¿½ğ´ï¿½ï¿½ï¿½ = ğ´ï¿½ï¿½ï¿½ğ´ï¿½ï¿½ï¿½ (imitation) + ğ´ï¿½ï¿½ï¿½ğ´ï¿½ï¿½ï¿½ (contact) + ğ´ï¿½ï¿½ï¿½ğ´ï¿½ï¿½ï¿½ (action)
(5)
ğ´ï¿½ï¿½ï¿½ğ´ï¿½ï¿½ï¿½ (imitation) = ğ´ï¿½ï¿½ï¿½ğ´ï¿½ï¿½ï¿½ (ğ´ï¿½ï¿½ï¿½) + ğ´ï¿½ï¿½ï¿½ğ´ï¿½ï¿½ï¿½ ( ï¿½ğ´ï¿½ï¿½ï¿½) + ğ´ï¿½ï¿½ï¿½ğ´ï¿½ï¿½ï¿½ (ğ´ï¿½ï¿½ï¿½) + ğ´ï¿½ï¿½ï¿½ğ´ï¿½ï¿½ï¿½ ( ï¿½ğ´ï¿½ï¿½ï¿½) + ğ´ï¿½ï¿½ï¿½ğ´ï¿½ï¿½ï¿½ (orientation)
(6)
ğ´ï¿½ï¿½ï¿½ğ´ï¿½ï¿½ï¿½ (action) = ğ´ï¿½ï¿½ï¿½ğ´ï¿½ï¿½ï¿½ (action diff) + ğ´ï¿½ï¿½ï¿½ğ´ï¿½ï¿½ï¿½ (action min).
(7)
Each of the reward terms is expressed using a weighted Gaussian kernel: