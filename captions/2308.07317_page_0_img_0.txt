Platypus: Quick, Cheap, and Powerful
Refinement of LLMs
Ariel N. Lee∗
Boston University
ariellee@bu.edu
Cole J. Hunter∗
Boston University
colejh@bu.edu
Nataniel Ruiz†
Boston University
nruiz9@bu.edu
Abstract
We present Platypus, a family of fine-tuned and merged Large Language Models
(LLMs) that achieves the strongest performance and currently stands at first place
in HuggingFace’s Open LLM Leaderboard ‡ as of the release date of this work.
In this work we describe (1) our curated dataset Open-Platypus, that is a subset
of other open datasets and which we release to the public (2) our process of
fine-tuning and merging LoRA modules in order to conserve the strong prior of
pretrained LLMs, while bringing specific domain knowledge to the surface (3)
our efforts in checking for test data leaks and contamination in the training data,
which can inform future research. Specifically, the Platypus family achieves strong
performance in quantitative LLM metrics across model sizes, topping the global
Open LLM leaderboard while using just a fraction of the fine-tuning data and
overall compute that are required for other state-of-the-art fine-tuned LLMs. In
particular, a 13B Platypus model can be trained on a single A100 GPU using
25k questions in 5 hours. This is a testament of the quality of our Open-Platypus
dataset, and opens opportunities for more improvements in the field. Project page:
https://platypus-llm.github.io
*Equal Contribution.
†NR is currently at Google and his contributions were done as work at BU prior to his tenure at the company.
‡https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard
arXiv:2308.07317v1  [cs.CL]  14 Aug 2023