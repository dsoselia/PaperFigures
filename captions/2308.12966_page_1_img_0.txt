Figure 2: Some qualitative examples generated by our Qwen-VL-Chat. Qwen-VL-Chat supports multiple
image inputs, multi-round dialogue, multilingual conversation, text-reading, localization, fine-grained
recognition and understanding ability.
1
Introduction
Recently, Large Language Models (LLMs) (Brown et al., 2020; OpenAI, 2023; Anil et al., 2023; Gao et al.,
2023; Qwen, 2023) have attracted wide attention due to their powerful capabilities in text generation and
comprehension. These models can be further aligned with user intent through fine-tuning instructions,
showcasing strong interactive capabilities and the potential to enhance productivity as intelligent assistants.
However, native large language models only live in the pure-text world, lacking the ability to handle other
common modalities (such as images, speech, and videos), resulting in great restrictions on their application
scope. Motivated by this, a group of Large Vision Language Models (LVLMs) (Alayrac et al., 2022; Chen
et al., 2022; Li et al., 2023c; Dai et al., 2023; Huang et al., 2023; Peng et al., 2023; Zhu et al., 2023; Liu et al.,
2023; Ye et al., 2023b,a; Chen et al., 2023a; Li et al., 2023a; Zhang et al., 2023; Sun et al., 2023; OpenAI, 2023)
have been developed to enhance large language models with the ability to perceive and understand visual
signals. These large-scale vision-language models demonstrate promising potential in solving real-world
vision-central problems.
Nevertheless, despite that lots of works have been conducted to explore the limitation and potency of LVLMs,
current open-source LVLMs always suffer from inadequate training and optimization, thus lag far behind
the proprietary models (Chen et al., 2022, 2023b; OpenAI, 2023), which hinders further exploration and
application of LVLMs in open-source community. Whatâ€™s more, as real-world visual scenarios are quite
complicated, fine-grained visual understanding plays a crucial role for LVLMs to assist people effectively
and precisely. But only a few attempts had been made toward this direction (Peng et al., 2023; Chen et al.,
2023a), the majority of open-source LVLMs remain perceiving the image in a coarse-grained approach and
lacking the ability to execute fine-grained perception such as object grounding or text reading.
2