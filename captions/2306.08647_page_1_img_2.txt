this gap, by proposing a system (right) consisting of the Reward Translatorthat interprets the user input and transform
it into a reward specification. The reward specification is then consumed by a Motion Controller that interactively
synthesizes a robot motion which optimizes the given reward.
bridges the gap between language and low-level robot actions. This is motivated by the fact that language
instructions from humans often tend to describe behavioral outcomes instead of low-level behavioral details
(e.g. “robot standing up” versus “applying 15 Nm to hip motor”), and therefore we posit that it would be
easier to connect instructions to rewards than low-level actions given the richness of semantics in rewards.
In addition, reward terms are usually modular and compositional, which enables concise representations
of complex behaviors, goals, and constraints. This modularity further creates an opportunity for the user
to interactively steer the robot behavior. However, in many previous works in reinforcement learning (RL)
or model predictive control (MPC), manual reward design requires extensive domain expertise [17, 18, 19].
While reward design can be automated, these techniques are sample-inefficient and still requires manual
specification of an objective indicator function for each task [20]. This points to a missing link between
the reward structures and task specification which is often in natural language. As such, we propose to
utilize LLMs to automatically generate rewards, and leverage online optimization techniques to solve them.
Concretely, we explore the code-writing capabilities of LLMs to translate task semantics to reward functions,
and use MuJoCo MPC, a real-time optimization tool to synthesize robot behavior in real-time [21]. Thus
reward functions generated by LLMs can enable non-technical users to generate and steer novel and intricate
robot behaviors without the need for vast amounts of data nor the expertise to engineer low-level primitives.
The idea of grounding language to reward has been explored by prior work for extracting user preferences
and task knowledge [22, 23, 24, 25, 26, 27]. Despite promising results, they usually require training
data to learn the mapping between language to reward. With our proposed method, we enable a data
efficient interactive system where the human engages in a dialogue with the LLM to guide the generation
f
d
d
tl
b t b h
i
(Fi
1)