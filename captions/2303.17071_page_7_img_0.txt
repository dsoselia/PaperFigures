Figure 4: Care plan generation task: Results from physician-expert evaluations. (Left) Physicians choose the
ﬁnal care plan produced by DERA over the initial GPT-4 generated care plan 84% to 16%. (Center) Final DERA
care plans capture far more of the necessary care management steps than initial GPT-4 generated care plans, with
physicians rating "All" relevant steps inferred from the patient-physician chat generated in 92% of DERA care
plans vs. 64% of initial GPT-4 care plans. (Right) For care plan correction suggestions in the scratchpad, physicians
rate agreement with "All" suggestions in 72% of encounters, Most" in 14%, "Some" in 0%, and "None" in 14%.
In addition to these questions, we also asked
the physician-experts the following: If this care
plan were acted upon by the patient, does this care
plan contain information that could potentially be
harmful to the patient given their presentation?
(Options: Yes, No). The amount of careplan con-
taining "harmful" information drops from 2% in the
initial careplan to 0% in the ﬁnal DERA summary.
We caution against drawing generalizations from
these harmfulness numbers. Our evaluations are
both limited in number and drawn from a patient
population speciﬁc to the telehealth platform; thus
cannot predict the generalizability of these ﬁndings
in other settings.
Qualitative Examples
We show a qualitative ex-
ample of the care plan generation task with DERA
in Figure 3. The initial care plan generated by the
Decider was originally rated as containing "Most"
necessary care management steps by our physician-
expert evaluator, suggesting there were still some
improvements possible. In the DERA dialog, the
Researcher highlights potential drug interactions
with the patient’s current medications and the rec-
ommendation to educate the patient on safe sexual
practices. These corrections were accepted by the
Decider, as evidenced by the notes written to the
scratchpad. In turn, the corrections were mani-
fested in the ﬁnal care plan, with the three changes
bolded. This ﬁnal care plan was rated as contain-
ing "All" necessary care management steps by our
physician-expert evaluator.
5
Open-Ended Medical Question
Answering
Overview
We
also
investigate
the
use
of
DERA for short-form medical reasoning. A com-
monly used dataset for this task is MedQA (Jin
et al., 2021) which consists of USMLE-style
practice multiple-choice questions. Previous ap-
proaches for this dataset have included using
RoBERTa (Liu et al., 2019), reﬁning chain-of-
thought using GPT-3 (Liévin et al., 2022), and
ﬁne-tuning PaLM (Chowdhery et al., 2022; Sing-
hal et al., 2022). While most previously-reported
results achieved passing results, recent GPT-4 is
shown to work at a near-expert level (Nori et al.,
2023).
In all previous work, the primary focus was on
the multiple-choice question format which has lim-
ited applicability in the real world. If these models
are to support doctors in decision-making, these
models need to operate without any options pro-
vided. To mimic this setting, we extend the MedQA
dataset to be open-ended to evaluate the model in a
more realistic and harder setting. In an open-ended
form, the model must generate the correct answer
free-form and not choose from a given bank of
options. We also evaluate a set of continuing edu-
cation questions from the New England Journal of
Medicine (NEJM), again in an open-ended setting.
A method that can perform at a high level on
this task requires several attributes. First, it must
be able to recall a large set of knowledge across
multiple domains of medicine. Second, it must be