(a) Input
(b) Using the layout prompt
(c) Using the markup prompt
Figure 3: Model outputs from KOSMOS-2.5 with different task prompts given the same input text
image.
LLMs, enhancing their capabilities through further prompt engineering. This approach empowers
LLMs with robust text image understanding capabilities. Thirdly, we have the potential to augment
the pre-training with textual data, transforming it into a general-purpose MLLM. This expanded
model not only processes visual signals but also possesses strong language understanding capabilities.
4
Related Work
4.1
Multimodal Large Language Models
The flourishing blossom of large language models (LLM), represented by ChatGPT [Cha22], has
revolutionized artificial intelligence and significantly impacted numerous downstream tasks such
as text translation, code generation, question answering, etc. Despite the rapid development, it is
significant to recognize that the human perception of the world is not limited to language alone but
encompasses a wide range of modalities, with particular emphasis on the visual modality. Many
research works attempt to “bring eyes” to LLM and develop multimodal large language models
(MLLM), which can be categorized into LLM-centric scheduling systems and end-to-end trainable
multimodal systems.
The LLM-centric scheduling system [WYQ+23, YLW+23, LWS+23, SST+23, LHW+23, SMV23,
CLS+23] takes advantage of many vision foundation models (e.g., Stable Diffusion [RBL+22],
ControlNet [ZA23], BLIP [LLXH22], etc.), and schedules these models in a language-centric manner.
For example, Visual ChatGPT [WYQ+23] develops a set of prompts to incorporate visual information
into ChatGPT, enabling users to draw or edit images through chatting. MM-REACT [YLW+23]
leverages vision experts to augment its multimodal capabilities by incorporating a textual prompt
design that can effectively represent various visual signals, including text descriptions, coordinates,
and aligned file names, for images and videos. HuggingGPT [SST+23] connects LLMs with extensive
AI models in machine learning communities, tackling user requests through ChatGPT’s task planning,
model selection, and response summarization capabilities. Further, TaskMatrix.AI [LWS+23] largely
extends the scale and connects foundation models with millions of APIs for solving tasks in both
digital and physical domains. Differently, InternGPT [LHW+23] incorporates pointing instructions
(e.g., clicking and dragging) for better communication between chatbots and users, while also
improving the accuracy of chatbots in performing vision-centric tasks. Nevertheless, this approach
has several limitations, such as the expenses associated with API calls or the storage space required
for the pre-trained weights of foundation models.
End-to-end trainable multimodal system [HSD+22, ADL+22, HDW+23, PWD+23, HZH+21,
XHL+21, ZCS+23, HML+23, LLSH23, DLL+23, LLWL23, LZR+23, WCC+23, SLL+23,
ZHZ+23, GHZ+23, KSF23, LZC+23] integrates vision and language models into a unified model,
which are further trained on multimodal datasets. For instance, Flamingo [ADL+22] leverages
9