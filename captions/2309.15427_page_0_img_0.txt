Graph Neural Prompting with Large Language Models
Yijun Tian1, Huan Song2, Zichen Wang2, Haozhu Wang2,
Ziqing Hu2, Fang Wang2, Nitesh V. Chawla1, Panpan Xu2
1University of Notre Dame
2Amazon
{yijun.tian, nchawla}@nd.edu, {huanso, zichewan, haozhuw, ziqinghu, fwfang, xupanpan}@amazon.com
Abstract
Large Language Models (LLMs) have shown remarkable
generalization capability with exceptional performance in
various language modeling tasks. However, they still exhibit
inherent limitations in precisely capturing and returning
grounded knowledge. While existing work has explored
utilizing knowledge graphs to enhance language modeling via
joint training and customized model architectures, applying
this to LLMs is problematic owing to their large number of
parameters and high computational cost. In addition, how to
leverage the pre-trained LLMs and avoid training a customized
model from scratch remains an open question. In this work,
we propose Graph Neural Prompting (GNP), a novel plug-and-
play method to assist pre-trained LLMs in learning beneficial
knowledge from KGs. GNP encompasses various designs,
including a standard graph neural network encoder, a cross-
modality pooling module, a domain projector, and a self-
supervised link prediction objective. Extensive experiments on
multiple datasets demonstrate the superiority of GNP on both
commonsense and biomedical reasoning tasks across different
LLM sizes and settings.
Introduction
Large Language Models (LLMs) have demonstrated
exceptional performance and general capability in various
NLP tasks and use cases such as question answering
(Robinson, Rytting, and Wingate 2023), translation (Moslem,
Haque, and Way 2023), and text summarization (Zhang et al.
2023). Moreover, the significant growth in model size has
further endowed LLMs with emergent capabilities (Wei et al.
2022b), laying the groundwork for exploring artificial general
intelligence (Bubeck et al. 2023). Accordingly, LLMs have
attracted tremendous interest from both academia (Wei et al.
2022a; Zhao et al. 2023) and industry (Anil et al. 2023;
OpenAI 2023).
Given the broad success of LLMs, many techniques
have emerged to adapt these general-purpose models to
downstream tasks. Beyond the conventional approach of
model fine-tuning where all model parameters are adjusted
(Howard and Ruder 2018), prompt-based adaptation methods
are proposed to modulate a frozen LLMâ€™s behavior through
prompts (Brown et al. 2020; Lester, Al-Rfou, and Constant
2021; Li and Liang 2021). Rather than adapt the parameters
in LLMs, these methods freeze the LLMs and typically
introduce additional trainable parameters. The idea of
Figure 1: Result comparison across LLM Frozen (parameters
unchanged) and LLM Tuned (parameters updated) settings.
The proposed Graph Neural Prompting significantly
improves the performance. Reported results are averaged
across six datasets on two tasks for an 11B FLAN-T5 model.
freezing LLMs is appealing, especially as the model size
grows and the training resource dependency intensifies.
On the other hand, despite the success of LLMs in
handling different real-world applications and the feasibility
of adapting to specific downstream tasks, they still exhibit
the inherent limitations of language modeling in accurately
capturing and returning grounded knowledge (Lewis et al.
2020; Pan et al. 2023). Knowledge graphs (KGs), storing
enormous facts, serve as a structured and systematic way of
representing knowledge (Ji et al. 2021; Hogan et al. 2021).
Consequently, existing methods have incorporated KGs to
assist language modeling for the task of question answering,
often by designing customized model architectures to
accommodate both KGs and textual data, followed by
joint training sessions (Yasunaga et al. 2022; Zhang et al.
2022). Nonetheless, joint training KGs and text for LLMs is
challenging due to the extensive parameters LLMs contain
and the substantial computation resources they require.
In addition, numerous pre-trained LLMs with exceptional
capabilities are released. It becomes advantageous to employ
these pre-existing LLMs, particularly beneficial if we can
sidestep the need to craft a specialized model and train it from
scratch. A direct approach to employing KGs is to feed the
triples from KGs into LLMs directly (Baek, Aji, and Saffari
arXiv:2309.15427v1  [cs.CL]  27 Sep 2023