original SDS
+ timestep annealing
+ neg prompts
+ rescale
Figure 7: Effects of different techniques that we apply in multi-view SDS. The input text is "a DSLR
photo of a frog wearing a sweater".
Photo of a [v] dog
DreamBooth3D
Ours
Figure 9: Illustration of MVDreamBooth Results. From the inputs, we show the generated multi-view
images given a description prompt at bottom. On the right, we show comparison of NeRF rendered
results from ours with and DreamBooth3D (Raj et al., 2023). Notice ours perform better on the object
details such as furry skin. See Appendix for more results.
among all. 914 feedbacks from 38 users were collected and the result is shown in Fig. (8). On average,
78% users prefer our model over others. That is, our model is preferred over the best of all baselines
in most cases. We believe this is a strong proof of the robustness and quality of the proposed method.
Please see the supplementary materials for more visual results.
4.3
MULTI-VIEW DREAMBOOTH.
In Fig. (9), we compare 3D models generated from MVDream and DreamBooth3D (Raj et al., 2023).
We found that our results have higher quality with better object details such as the curly hair and
fur texture on the dog. This is because during the NeRF training process with SDS loss, our MV
DreamBooth diffusion models produce higher geometry consistency. We provide additional results in
our project page and supplementary materials.
5
DISCUSSION AND CONCLUSION
Conclusion
In this paper, we present the first multi-view diffusion model that is able to generate a
set of multi-view images of an object from any given text. By fine-tuning a pre-trained text-to-image
diffusion model on a mixture of 3D rendered data and large scale text-to-image data, our model is able
to maintain the generalizability of the base model while achieving multi-view consistent generation.
We show that the multi-view diffusion model can serve as a good 3D prior and can be applied to
3D generation via SDS, which leads to better stability and quality than current open-sourced 2D
lifting methods. Finally, the multi-view diffusion model can also be trained in a few-shot setting for
personalized 3D generation (multi-view DreamBooth)