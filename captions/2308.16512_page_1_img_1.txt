Inspired by DreamBooth (Ruiz et al., 2023), we can also employ our multi-view diffusion model to
assimilate identity information from a collection of 2D images and it demonstrates robust multi-view
consistency after fine-tuning. Overall, our model, namely MVDream, successfully generates 3D Nerf
models without the multi-view consistency issue. It either surpasses or matches the diversity seen in
other state-of-the-art methods.
2
RELATED WORK AND BACKGROUND
2.1
3D GENERATIVE MODELS
The significance of 3D generation has driven the application of nearly all deep generative models
to this task. Handerson et al. explored Variational Auto Encoders (Kingma & Welling, 2014)
for textured 3D generation (Henderson & Ferrari, 2020; Henderson et al., 2020). However, their
studies mainly addressed simpler models leveraging multi-view data. With Generative Adversarial
Networks (GANs) yielding improved results in image synthesis, numerous studies have investigated
3D-aware GANs (Nguyen-Phuoc et al., 2019; 2020; Niemeyer & Geiger, 2021; Deng et al., 2022;
Chan et al., 2022; Gao et al., 2022). These methods, attributed to the absence of reconstruction loss
involving ground-truth 3D or multi-view data, can train solely on monocular images. Yet, akin to 2D
GANs, they face challenges with generalizability and training stability for general objects and scenes.
Consequently, diffusion models, which have shown marked advancements in general image synthesis,
have become recent focal points in 3D generation studies. Various 3D diffusion models have been
introduced for tri-planes (Shue et al., 2023; Wang et al., 2023b) or feature grids (Karnewar et al.,
2023). Nevertheless, these models primarily cater to specific objects like faces and ShapeNet objects.
Their generalizability to the scope of their 2D counterparts remains unverified, possibly due to 3D
representation constraints or architectural design. It is pertinent to note ongoing research endeavors
to reconstruct object shapes directly from monocular image inputs (Wu et al., 2023; Nichol et al.,
2022; Jun & Nichol, 2023), aligning with the increasing stability of image generation techniques.
2.2
DIFFUSION MODELS FOR OBJECT NOVEL VIEW SYNTHESIS
Recent research has also pursued the direct synthesis of novel 3D views without undergoing recon-
struction. For instance, Watson et al. (2023) first applied diffusion models to view synthesis using the
ShapeNet dataset (Sitzmann et al., 2019). Zhou & Tulsiani (2023) extended the idea to the latent space
of stable diffusion models (Rombach et al., 2022) with an epipolar feature transformer. Chan et al.
(2023) enhanced the view consistency by re-projecting the latent features during diffusion denoising.
Szymanowicz et al. (2023) proposed a multi-view reconstructer using unprojected feature grids. A
limitation shared across these approaches is the boundedness to their respective training data, with no
established evidence of generalizing to diverse image inputs. Liu et al. (2023) propose to fine-tune a
pre-trained image variation diffusion model (sdv) on an extensive 3D render dataset for novel view
synthesis. Notwithstanding, the synthesized images from such studies still grapple with geometric
consistency, leading to a discernible blurriness in the output 3D models. Recently, Tang et al. (2023b)
proposed a multi-view diffusion model for panorama with homography-guided attention, which is
different from ours where explicit 3D correspondence is not available.
2