Model size
7B
13B
13B
30B
65B
Compute
8x A100
8x A100
32x A100
TPUv4-1024
TPUv4-1024
Memory efficient
attention & FFN
Context size
(×1e3)
32
16
64
16
8
Ring Attention
Context size
(×1e3)
256
128
2048
2048
1024
Figure 3: Comparison of different models on the long-range line retrieval task.
finetuning on the LLaMA-13B model, limiting the context length to 512K tokens due to constraints
on our cloud compute budget. This finetuning was carried out on 32 A100 GPUs, using the ShareGPT
dataset, following methodologies as outlined in prior works [6, 13]. We then evaluated our finetuned
model on the line retrieval test [21]. In this test, the model needs to precisely retrieve a number
from a long document, the task can effectively capture the abilities of text generation, retrieval, and
information association at long context, reflected by the retrieving accuracy. Figure 3 presents the
accuracy results for different models across varying context lengths (measured in tokens). Notably,
our model, Ring Attention-13B-512K, stands out as it maintains high accuracy levels even with
long contexts. GPT3.5-turbo-16K, Vicuna-16B-16K, and Claude-2-100K demonstrate competitive
accuracy within short context lengths. However, they cannot handle extended context lengths.
6
Related Work
Transformers have garnered significant attention in the field of AI and have become the backbone
for numerous state-of-the-art models. Several works have explored memory-efficient techniques
to address the memory limitations of Transformers and enable their application to a wider range
8