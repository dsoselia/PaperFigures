Multi-face Janus Problem
Content Drift Problem
Figure 1: Typical multi-view consistency problems of 2D-lifting methods for 3D generation. Left: “A
photo of a horse walking” where the horse has two faces. Right: “a DSLR photo of a plate of fried
chicken and waffles with maple syrup on them” where the chicken gradually becomes a waffle.
model on multi-view images (from 3D assets) and 2D image-text pairs, we find that it can achieve
both good consistency and generalizability. When applied to 3D generation through score distillation,
our multi-view supervision proves significantly more stable than that of single-view 2D diffusion
models. And we can still create unseen, counterfactual 3D contents as from pure 2D diffusion models.
Inspired by DreamBooth (Ruiz et al., 2023), we can also employ our multi-view diffusion model to
assimilate identity information from a collection of 2D images and it demonstrates robust multi-view
consistency after fine-tuning. Overall, our model, namely MVDream, successfully generates 3D Nerf
models without the multi-view consistency issue. It either surpasses or matches the diversity seen in
other state-of-the-art methods.
2
RELATED WORK AND BACKGROUND
2.1
3D GENERATIVE MODELS
The significance of 3D generation has driven the application of nearly all deep generative models
to this task. Handerson et al. explored Variational Auto Encoders (Kingma & Welling, 2014)
for textured 3D generation (Henderson & Ferrari, 2020; Henderson et al., 2020). However, their
studies mainly addressed simpler models leveraging multi-view data. With Generative Adversarial
Networks (GANs) yielding improved results in image synthesis, numerous studies have investigated
3D-aware GANs (Nguyen-Phuoc et al., 2019; 2020; Niemeyer & Geiger, 2021; Deng et al., 2022;
Chan et al., 2022; Gao et al., 2022). These methods, attributed to the absence of reconstruction loss
involving ground-truth 3D or multi-view data, can train solely on monocular images. Yet, akin to 2D
GANs, they face challenges with generalizability and training stability for general objects and scenes.
Consequently, diffusion models, which have shown marked advancements in general image synthesis,
have become recent focal points in 3D generation studies. Various 3D diffusion models have been
introduced for tri-planes (Shue et al., 2023; Wang et al., 2023b) or feature grids (Karnewar et al.,
2023). Nevertheless, these models primarily cater to specific objects like faces and ShapeNet objects.
Their generalizability to the scope of their 2D counterparts remains unverified, possibly due to 3D
representation constraints or architectural design. It is pertinent to note ongoing research endeavors
to reconstruct object shapes directly from monocular image inputs (Wu et al., 2023; Nichol et al.,
2022; Jun & Nichol, 2023), aligning with the increasing stability of image generation techniques.
2.2
DIFFUSION MODELS FOR OBJECT NOVEL VIEW SYNTHESIS
Recent research has also pursued the direct synthesis of novel 3D views without undergoing recon-
struction. For instance, Watson et al. (2023) first applied diffusion models to view synthesis using the