A
Supplementary Material
A.1
Hyperparameters
The settings of hyperparameters are demonstrated in Table 4.
Hyperparameters
Number of layers
24
Hidden size
1,536
FFN inner hidden size
6,144
Attention heads
16
Activation function
GeLU [HG16]
Vocabulary size
108,481
Soft tokens V size
2,048
Max sequence length
4,096
Initialization
Magneto [WMH+22]
(a) Basic hyperparameters of KOSMOS-2.5
Hyperparameters
Training steps
200,000
Warmup steps
375
Batch size
1,024
Optimizer
AdamW
Learning rate
2e-4
Learning rate decay
Linear
Adam Î²
(0.9, 0.98)
Weight decay
0.01
Dropout
0.1
(b) Training hyperparameters of KOSMOS-2.5
Table 4: Hyperparameters of KOSMOS-2.5
A.2
Data Samples
We demonstrate some of the training samples in KOSMOS-2.5, which include the input and output
from IIT-CDIP, arXiv papers, PowerPoint slides, general PDFs, web screenshots, README, DOCX,
LATEX and HTML.
(a) Input
(b) Rendered output
Figure 4: A training sample for the layout-based task from IIT-CDIP
A.3
Examples of Model Inference
16