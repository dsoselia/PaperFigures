Figure 2: Top (a): We use the same model architecture as the original Transformer but reorganize
the compute. In the diagram, we explain this by showing that in a ring of hosts, each host holds one
query block, and key-value blocks traverse through a ring of hosts for attention and feedforward
computations in a block-by-block fashion. As we compute attention, each host sends key-value blocks
to the next host while receives key-value blocks from the preceding host. The communication is
overlapped with the computation of blockwise attention and feedforward. Bottom (b): We compute
the original Transformer block-by-block. Each host is responsible for one iteration of the query’s outer
loop, while the key-value blocks rotate among the hosts. As visualized, a device starts with the first
query block on the left; then we iterate over the key-value blocks sequence positioned horizontally.
The query block, combined with the key-value blocks, are used to compute self-attention (yellow
box), whose output is pass to feedforward network (cyan box).
computation, the following condition must hold: 4dc2/F ≥ 4cd/B. This implies that the block size,
denoted as c, should be greater than or equal to F/B. Effectively, this means that the block size needs
to be larger than the ratio of FLOPs over bandwidth.
Memory Requirement. A host needs to store multiple blocks, including one block size to store
the current query block, two block sizes for the current key and value blocks, and two block sizes
for receiving key and value blocks. Furthermore, storing the output of blockwise attention and
feedforward necessitates one block size, as the output retains the shape of the query block. Therefore,
a total of six blocks are required, which translates to 6bch bytes of memory. It’s worth noting that
the blockwise feedforward network has a maximum activation size of 2bch [24]. Consequently, the
total maximum activation size remains at 6bch bytes. Table 1 provides a detailed comparison of the
4