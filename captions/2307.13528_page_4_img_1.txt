each. The total cost of these items is: 5 3  $15
- To find the cost of the boxes of pizza, we 
subtract the total cost of the other items from 
the total amount paid by Marie: 45 - 15 = $30
- To find the number of boxes of pizza, we 
divide the remaining amount by the cost of 
each box: 30 / 3 = 10
- Therefore, Marie ordered 10 boxes of pizza.
(1) print(round(5*3, 7) == 15)
(2) print(round(45-15, 7)==30)
(3) print(round(30/3, 7) == 10)
Exec 
Results
(1) True
(2) True
(3) True
Response-level  
Factuality:1
Scores
Quantum computing has the potential to 
revolutionize various fields such as 
cryptography, optimization, and simulation. 
However, there are also limitations such as 
the need for error correction. One papers 
that have contributed to this field is 
“Quantum Computing in the NISQ era and 
beyond” by John Preskill (2018).
p
era and beyond
Evidence
(1) {title: Quantum Computing in the 
NISQ era and beyond, authors: John 
Preskill, publication_year: 2018}
Claim-level Factuality: [1] 
Response-level Factuality: 1
Scores
Figure 2: Our proposed framework for factuality detection in four domains: knowledge-based QA, code generation,
math problem solving and scientific literature review writing.
{ci}i=1···n. Detailed prompting instructions can
be found in Appendix A.
KB-based QA
The claim is defined using the
concept of atomic content units (ACUs) (Liu
et al., 2022). Each ACU corresponds to a single
atomic fact within a generated answer. In practice,
we leverage ChatGPT4 (specifically, the “gpt-3.5-
turbo” version) to extract claims based on two cri-
teria: (i) each claim should not exceed 15 words,
and (ii) it should clearly describe a fact. We also
include two in-context examples from the RoSE
dataset (Liu et al., 2022) in our prompt to obtain
more fine-grained claims. Additionally, we ask
ChatGPT to resolve any coreferences or ambiguity,
such as unclear pronouns and other related expres-
sions within the claims.
Code Generation
We consider each generated
code snippet within the response as a single claim
to be verified. We extract all such code snippets
that are enclosed with brackets, in other words,
within a code block.
Math Problems
We define each claim in a step-
by-step math solution as the arithmetic operation
performed between known real numbers. Each of
these operations contains two parts: the calculation
4We have also explored other entailment-based models
with BERT, and the result is no better than ChatGPT.
and the calculated answer. We prompt ChatGPT to
extract all such claims.
Scientific Literature Review
Each claim within
the generated review is defined as a tuple of “(paper
title, year, authors)” contained from the generated
review. We then prompt ChatGPT to extract all
such tuples within the generated review.
4.2
Query Generation
For each claim ci, we convert it into a list of queries
{qij}j=1···m that can be used to query external tools
such as search engines, the Python interpreter, or
Google scholar.
KB-based QA
We prompt ChatGPT or GPT-4 to
generate two search engine queries from each claim
ci. These queries are intended to help humans in
verifying the factuality of ci. Detailed prompting
instructions can be found in Appendix A.
Code Generation
For each claim ci we gener-
ate two different types of queries: simulated test
case inputs, denoted as {qtij}j=1···m, and poten-
tial solutions, denoted as {qsij}j=1···m. Both types
of queries are generated by ChatGPT or GPT-4.
The simulated test case inputs are function calls
generated for a given code snippet, while potential
solutions are repeatedly generated solutions that
ChatGPT generates in response to the user prompt