Figure 1: The proposed Spectron model, connects the encoder of a speech recognition model with a
pre-trained Transformer decoder language model. At training time, we take speech utterances and
split their audio into a prompt and its continuation. From the prompt speech features, the full (prompt
and continuation’s) transcript must be reconstructed, as well as the continuation’s speech features via
newly introduced pre- and post-net speech modules. At inference time, only a prompt is provided; the
prompt’s transcription, text continuation, and speech continuations are all generated by the model.
• Directly process spectrograms as both input and output. Spectron leverages the audio
capabilities of a pre-trained speech encoder through the use of intermediate projection
layers.
• Demonstrably transfer generative ability from a pre-trained LLM, as shown by competitive
performance in semantic coherence and spoken question answering over other end-to-end
spoken language models.
Our work shows that the inductive biases from a pre-trained speech encoder and a language model
decoder enable end-to-end training and state-of-the-art performance without sacrificing representa-
tional fidelity. Key to this is a novel end-to-end training objective which implicitly supervises speech
recognition, text continuation, and conditional speech synthesis in a joint manner. The language
model transcribes and generates text continuations, acting as an ‘intermediate scratchpad’ (Nye et al.,
2021; Wei et al., 2022) to be conditioned on for audio generation. A novel spectrogram regression
loss also supervises the model to match the higher-order temporal and feature deltas of the ground
truth, based on the idea that the derivatives of the ground truth express rich, longer-range information
about the shape of the signal. Our overall scheme is summarized in Figure 1 and described in the rest
of this work.
2
RELATED WORK
The dominant approach to spoken language modeling is to use compact discrete speech representa-
tions. This allows the application of text-based language modeling methods to speech data. These
representations are typically obtained by clustering the outputs of a speech encoder using the K-means
algorithm and using the centroids as tokens. The resulting discrete speech sequences can then be
easily modeled using transformer architectures (Vaswani et al., 2017). Some notable examples of
works using this approach include:
Generative Spoken Language Modeling (GSLM) (Lakhotia et al., 2021) offers a baseline system
that operate on units quantized from pre-trained audio representations, such as HuBERT (Hsu et al.,
2021). These quantized units are modeled by a Transformer decoder. A unit-to-speech model is
2