1
INTRODUCTION
Intelligent agents are broadly conceptualized as autonomous problem solvers with the ability to
sense their environment, decide, and act upon that environment (Wooldridge & Jennings, 1995;
Sutton & Barto, 2005; Russell, 2010). With the advent of large language models (LLMs) (Brown
et al., 2020; Chen et al., 2021; Chowdhery et al., 2022; OpenAI, 2023b; Touvron et al., 2023), recent
implementations from the academic, industry, and open-source communities have leveraged this
concept to create language agents. These agents are capable of utilizing natural language to perform
a variety of intricate tasks in diverse environments, showcasing notable potentials (Yao et al., 2022b;
Chase, 2022; Gravitas, 2023; OpenAI, 2023a; Wang et al., 2023a).
Meanwhile, current agent frameworks such as Chase (2022), Gravitas (2023), Xu et al. (2023a),
Nakajima (2023), Chen et al. (2023), Zhou et al. (2023c) provide proof-of-concept implementa-
tions and console interfaces largely tailored for developers. This often restricts access to a wider
audience, particularly those not versed in programming or consoles. Current agent benchmarks are
constructed within specific environments for deterministic evaluation, especially in scenarios in-
volving coding (Yang et al., 2023a), tool utilization (Li et al., 2023c; Patil et al., 2023; Qin et al.,
2023b), web browsing (Shi et al., 2017; Yao et al., 2022a; Deng et al., 2023; Zhou et al., 2023b)
or a combination of above (Liu et al., 2023). These offer initiatives toward agent evaluations while
general agent usage is bound to be connected to more open or unlimited environments and real users
with their special needs and materials.
Aiming to develop LLM-powered offerings for a broader user base, OpenAI has crafted and de-
ployed well-designed products1, specifically Advanced Data Analysis (previously known as Code
Interpreter), Plugins, and Browse with
Bing, leveraging their further trained models (which has
been reverse engineered and accepted by many Zheng et al. (2023b) and Gou et al. (2023)), business
logic code, and a nurtured software community (e.g., OpenAI plugins store). Despite the significant
success attained, the models and business logic code have not been open-sourced due to business
considerations, hindering not only users from free access but also developers and researchers from
further exploring, evaluating, and improving upon them.
Recognizing this, OpenAgents stems from the motivation to democratize agent access as an open-
source platform for using and hosting agents, currently encompassing three integral agents: Data
Agent for data analysis with Python and SQL, Plugins Agent for 200+ tool usages, and Web Agent
for autonomous web browsing. We believe that for LLMs to reach their full potential, they must tran-
sition from purely theoretical or developer-centric tools to dynamic, interactive systems that cater
to a diverse user base. As depicted in Figure 1, general users can readily explore agent capabilities
via the online Web UI without any coding expertise required. In addition, OpenAgents provides full
business logic and research codes for developers to easily deploy it locally and researchers to further
introspect and build language agents. Lastly, given all provided above, OpenAgents is meant to be a
realistic and holistic human-in-the-loop agent evaluation platform: out of real needs, real users inter-
act with agents to fulfill their tasks, and the whole human-agent interaction traces and user feedback
are recorded for further evaluation. Compared to existing benchmarks and platforms, OpenAgents
provides an in-the-wild environment where agents tackle a variety of genuine user needs.
During building OpenAgents, we first underscore the significance of effectively specifying applica-
tion requirements via LLM prompting, a process that often requires crafting instructions that cater
to backend logic, enhance output aesthetics, and safeguard against adversarial inputs. Our find-
ings indicate that the build-up of such instructions can, at times, be substantial, posing challenges
in terms of token limitations and context handling for the LLMs. Additionally, for effective real-
world deployment, agent models must not only exhibit high performance but also be able to handle
real-time, interactive scenarios, such as streaming, to provide an optimal user experience. Further-
more, our exploration reveals that current research often gravitates towards idealized performance
metrics, sometimes sidelining critical real-world considerations, such as the trade-offs between sys-
tem responsiveness and accuracy, and the nuanced complexities introduced when application-based
failures arise, potentially obfuscating the true capabilities of the LLMs.
For future directions and extensions based on OpenAgents, we envision the expansion of new agents,
agent-related methods, models, and tools for researchers. Plus, the built-in web UI paves the way
1https://chat.openai.com