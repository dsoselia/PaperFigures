Nougat
Blecher et al.
Example
This is an abstract. Lorem
ipsum dolor sit amet,
consectetur adipisicing elit,
sed do eiusmod tempor
incididunt ut labore et dolore
magna aliqua.
Figure 1: Our simple end-to-end architecture followin Donut [28]. The Swin Transformer encoder takes a document
image and converts it into latent embeddings, which are subsequently converted to a sequence of tokens in a auto-
regressive manner
2
Related Work
Optical Character Recognition (OCR) is an extensively researched field in computer vision for a variety applications,
such as document digitalization [2, 5], handwriting recognition and scene text recognition [6–8].
More concretely, recognizing mathematical expressions is a heavily researched subtopic. Grammar based methods
[9–11] for handwritten mathematical expressions were improved upon by different encoder-decoder models. The
fully convolutional model [12] was succeeded by various RNN decoder models [13–17], both for handwritten and
printed formulas. Recently, the decoder [18, 19] as well as the encoder [20] were replaced with the Transformer [21]
architecture.
Visual Document Understanding (VDU) is another related topic of deep learning research and focuses on extracting
relevant information of a variety of document types. Previous works depend on pre-trained models that learn to extract
information by jointly modeling text and layout information using the Transformer architecture. The LayoutLM model
family [22–24] uses masked layout prediction task to capture the spatial relationships between different document
elements.
Open source solutions with a related goal as ours include GROBID [4], which parses digital-born scientific documents
to XML with a focus on the bibliographic data and pdf2htmlEX [25], that converts digital-born PDFs to HTML
while preserving the layout and appearance of the document. However, both solutions can not recover the semantic
information of mathematical expressions.
3
Model
Previous VDU methods either rely on OCR text from a third party tool [22, 23, 26] or focus on document types such
as receipts, invoices or form-like documents [27]. Recent studies [28, 29] show that an external OCR engine is not
necessarily needed to achieve competitive results in VDU.
The architecture is a encoder-decoder transformer [21] architecture, that allows for an end-to-end training procedure.
We build on the Donut [28] architecture. The model does not require any OCR related inputs or modules. The text is
recognized implicitly by the network. See Fig. 1 for an overview of the approach.
Encoder
The visual encoder receives a document image x ∈ R3×H0×W0, crops the margins and resizes the image to
fit in a fixed rectangle of size (H, W). If the image is smaller than the rectangle, additional padding is added to ensure
each image has the same dimensionality. We use a Swin Transformer [30], a hierarchical vision transformer [31] that
splits the image into non-overlapping windows of fixed size and applies a series of self-attention layers to aggregate
information across these windows. The model output a sequence of the embedded patches z ∈ Rd×N where d is the
latent dimension and N is the number of patches.
Decoder
The encoded image z is decoded into a sequence of tokens using a transformer decoder architecture with
cross-attention. The tokens are generated in an auto-regressive manner, using self-attention and cross-attention to attend
to different parts of the input sequence and encoder output respectively. Finally, the output is projected to the size of the
vocabulary v, yielding the logits ℓ ∈ Rv.
Following Kim et al. [28], we use the implementation of the mBART [32] decoder. We use the same tokenizer as Taylor
et al. [33] because their model is also specialized in the scientific text domain.
2