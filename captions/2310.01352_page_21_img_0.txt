RA-DIT: Retrieval-Augmented Dual Instruction Tuning
Figure 2: RA-IT model performance (combined with DRAGON+) across sizes 7B, 13B and 65B on
our development tasks. 0-shot performance: dashed lines; 5-shot performance: solid lines.
previous work. We also describe the language model instruction templates and retriever queries used
in our evaluation in Table 12.
E
ADDITIONAL EXPERIMENTS
E.1
SCALING LAWS OF RETRIEVAL AUGMENTED LANGUAGE MODEL FINE-TUNING
We investigate the impact of the base language model size when retrieval-augmented instruction
tuning is applied, and summarize the results in Figure 2. We combine the fine-tuned models with
the base DRAGON+ retriever in this set of experiments.
Overall, all models substantially benefit from retrieval augmentation, with smaller models witness-
ing even bigger improvements. We further note that retrieval augmentation can be an effective
strategy for enhancing the performance of smaller models (hence reducing pre-training and infer-
ence costs), given the 7B model leveraging > 1 retrieved chunks surpassed the performance of the
vanilla 65B model on several tasks. This trend also differs across tasks. For tasks that primarily
measure one-hop fact look-up abilities (such as Zero-Shot RE and T-REx), retrieval augmentation
provides significant improvements across all model sizes and can bring the performance of smaller
models closer to that of their larger counterparts. For more complex tasks (such as HotpotQA and
WoW), the advantage of using a larger LLM remains prominent.
F
EXAMPLES
In this section, we show the task prompts, the corresponding retrieved passages and model pre-
dictions generated by LLAMA 65B instruction-tuned with retrieval augmentation (RA-IT 65B) and
LLAMA 65B instruction-tuned conventionally (IT 65B) on selected task examples.
F.1
HOTPOTQA
We analyze the performance of the two models on the development set of HotpotQA in the zero-shot
setting, under which RA-IT 65B outperforms IT 65B by a large margin (Table 4). Table 13 show two
22