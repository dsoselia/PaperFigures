Nougat
Blecher et al.
model won the VQA Challenge in 2017 and achieves 66.25% accuracy on VQA v2.0 test-dev.
Pythia[41]3 extends the BUTD model by incorporating co-attention [27] between question and image regions. Pythia uses features extracted from Detectron [8]
pretrained on Visual Genome. An ensemble of Pythia models won the 2018 VQA Challenge using extra training data from Visual Genome [21] and using Resnet[11]
features. In this study, we use Pythia models which do not use Resnet features.
Footnote 3: https://github.com/facebookresearch/pythia
**Bilinear Attention Networks (BAN) [19]**4 combines the idea of bilinear models and co-attention [27] between image regions and words in questions in a
residual setting. Similar to [3], it uses Faster-RCNN [33] pretrained on Visual Genome [21] to extract image features. In all our experiments, for a fair comparison,
we use BAN models which do not use additional training data from Visual Genome. BAN achieves the current state-of-the-art single-model accuracy of 69.64 % on
VQA v2.0 test-dev without using additional training data from Visual Genome.
Footnote 4: https://github.com/jnhwkim/ban-vqa
Implementation Details For all models trained with our cycle-consistent framework, we use the values 
, 
, 
 and 
. When
reporting results on the validation split and VQA-Rephrasings we train on the training split and when reporting results on the test split we train on both training and
validation splits of VQA v2.0. Note that we never explicitly train on the collected VQA-Rephrasings dataset and use it purely for evaluation purposes. We use
publicly available implementations of each backbone VQA model.
We measure the robustness of each of these models on our proposed VQA-Rephrasings dataset using the consensus score (Eq. 2). Table 1 shows the consensus
scores at different values of  for several VQA models. We see that all models suffer significantly when measured for consistency across rephrasings. For e.g., the
performance of Pythia (winner of 2018 VQA challenge) is reduced to a consensus score of 39.49% at 
. Similar trends are observed for MUTAN, BAN and
BUTD. The drop increases with increasing , the number of rephrasings used to measure consistency. Models like BUTD, BAN and Pythia which use word-level
encodings of the question suffer significant drops. It is interesting to note that even MUTAN which uses skip-thought based sentence encoding [20] suffers a drop
when checked for consistency across rephrasings. We observe that BAN + CC model trained with our proposed cycle-consistent training framework outperforms its
counterpart BAN and all other models at all values of .
Fig 4 qualitatively compares the textual and visual attention (over image regions) over 4 rephrasings of a question. The top row shows attention and predictions
from a Pythia model, while the bottom row shows attention and predictions from the same Pythia model, but trained using our framework. Our model attends at
relevant image regions
Model
CS(k)
VQA Accuracy
k=1
k=2
k=3
k=4
ORI
REP
MUTAN [5]
56.68
43.63
38.94
32.76
59.08
46.87
BUTD [3]
60.55
46.96
40.54
34.47
61.51
51.22
BUTD + CC
61.66
50.79
44.68
42.55
62.44
52.58
Pythia [41]
63.43
52.03
45.94
39.49
64.08
54.20
Pythia + CC
64.36
55.45
50.92
44.30
64.52
55.65
BAN [19]
64.88
53.08
47.45
39.87
64.97
55.87
BAN + CC
65.77
56.94
51.76
48.18
65.87
56.59
Table 1: Consensus performance on VQA-Rephrasings dataset. CS(k) as defined in Eq. 2 is consensus score which is non-zero only if at least  rephrasings are
answered correctly, zero otherwise; averaged across all group of questions. ORI represent a split of questions from VQA-Rephrasings which are original questions
from VQA v2.0 and their corresponding rephrasings are represented by the split REP. Models trained with our cycle-consistent (CC) framework consistently
outperform their baseline counterparts at all values of .
Model
val
test-dev
MUTAN [5]
61.04
63.20
BUTD [3]
65.05
66.25
+ Q-consistency
65.38
66.83
+ A-consistency
60.84
62.18
+ Gating
65.53
67.55
Pythia [41]
65.78
68.43
+ Q-consistency
65.39
68.58
+ A-consistency
62.08
63.77
+ Gating
66.03
68.88
BAN [19]
66.04
69.64
+ Q-consistency
66.27
69.69
+ A-consistency
64.96
66.31
+ Gating
66.77
69.87
Table 2: VQA Performance and ablation studies on VQA v2.0 validation and test-dev splits. Each row in blocks represents a component of our cycle-consistent
framework added to the previous row. First row in each block represents the baseline VQA model . Q-consistency implies addition of a VQG module  to
generate rephrasings 
 from the image  and the predicted answer 
 with an associated VQG loss 
. A-consistency implies passing all the generated
questions 
 to the VQA model  and an associated loss 
. Gating implies the use of gating mechanism to filter undesirable generated questions in 
and passing the remaining to VQA model . Models trained with our cycle-consistent (last row in each block) framework consistently outperform baselines.
10/03/2023, 17:22
latex-ocr-demo
3.86.159.32
1/1
duced by beam search tend to be short and generic. Completely random sampling can introduce very unlikely words, which
can damage generation as the model has not seen such mistakes at training time. The restriction of sampling from the 10 most
likely candidates reduces the risk of these low-probability samples.
For each model, we tune a temperature parameter for the softmax at generation time. To ease human evaluation, we generate
stories of 150 words and do not generate unknown word tokens.
For prompt generation, we use a self-attentive GCNN language model trained with the same prompt-side vocabulary as the
sequence-to-sequence story generation models. The language model to generate prompts has a validation perplexity of 63.06.
Prompt generation is conducted using the top-k random sampling from the 10 most likely candidates, and the prompt is
completed when the language model generates the end of prompt token.
We propose a number of evaluation metrics to quantify the performance of our models. Many commonly used metrics, such
as BLEU for ma
Model
Human
Preference
Language model
32.68%
Hierarchical Model
67.32%
Table 4: Effect of Hierarchical Generation. Human judges prefer stories that were generated hierarchically by first creating a
premise and creating a full story based on it with a seq2seq model.
Figure 5: Human accuracy at pairing stories with the prompts used to generate them. People find that our fusion model
significantly improves the link between the prompt and generated stories.
Model
# Parameters (mil)
Valid Perplexity
Test Perplexity
GCNN LM
123.4
54.50
54.79
GCNN + self-attention LM
126.4
51.84
51.18
LSTM seq2seq
110.3
46.83
46.79
Conv seq2seq
113.0
45.27
45.54
Conv seq2seq + self-attention
134.7
37.37
37.94
Ensemble: Conv seq2seq + self-attention
270.3
36.63
36.93
Fusion: Conv seq2seq + self-attention
255.4
36.08
36.56
Table 3: Perplexity on WritingPrompts. We dramatically improve over standard seq2seq models.
Figure 6: Accuracy of prompt ranking. The fusion model most accurately pairs prompt and stories.
Figure 7: Accuracy on the prompt/story pairing task vs. number of generated stories. Our generative fusion model can
produce many stories without degraded performance, while the KNN can only produce a limited number relevant stories.
Evaluation
Figure B.4: Pages with tables. Upper: Fan et al. [47] page 6, Lower: Shah et al. [48] page 6
17