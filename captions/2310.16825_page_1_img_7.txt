Prompt
SD2-base
CommonCanvas-S-C CommonCanvas-S-NC CommonCanvas-L-NC
a cute black cat
inside of a pumpkin
a robot holding a
paint palette
an oil painting of
a tall ship sailing
through a field of
wheat at sunset
Figure 1: Selection of text prompts. Using entirely Creative-Commons images and our synthetic
captioning approach, we achieve comparable qualitative performance to Stable Diffusion 2 (SD2-
base), as seen in CommonCanvas generations, while only requiring a small fraction (< 3%) of
the amount of training data. We include results for two CommonCanvas architectures, small (S)
and large (L) (Section 6), and two CC-image datasets, commercial (C) and non-commercial (NC)
(Section 4). We label our results accordingly as CommonCanvas-<architecture>-<dataset>.
We address the data incompleteness problem by using a pre-trained BLIP-2 model [34], which we
use to produce high-quality, synthetic captions for a set of curated, open licensed CC images. This is
an intuitive transfer-learning solution: leveraging powerful pre-trained generative models to produce
synthetic labels for an unlabeled dataset, which we can then use to train a different multimodal
generative model. We note that this is an increasingly common pattern in the literature, which we
shorthand with the name telephoning.
To deal with data scarcity, we propose a data- and compute-efficient training recipe that obtains the
same quality as SD2, but (perhaps surprisingly) requires as little as 3% of the LAION-2B data (i.e.,
roughly 70 million examples) originally used to train SD2. We call this model SD2-base. These
results indicate that we have a sufficient number of CC images (also roughly 70 million) for training
high-quality models. Our training recipe also implements a variety of optimizations that achieve
∼3X training speed-ups, and that allow for rapid model iteration.
The above methods enable us to create CommonCanvas, a suite of latent diffusion model (LDM)
architectures trained on our curated dataset of CC images and synthetic captions, which we denote
CommonCatalog. For CommonCanvasL-NC, we swap SD2’s UNet for SDXL to demonstrate how
even with less data, larger models do not overfit to this smaller dataset. Our largest model achieves
performance comparable to SD2-base on human evaluation of Parti Prompts [66], even though our
CommonCatalog training dataset is < 3% the size of LAION and has synthetically generated cap-
tions. Figure 1 shows select samples from our CommonCanvas models compared to corresponding