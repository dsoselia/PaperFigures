Ring Attention with Blockwise
Transformers for Near-Infinite Context
Hao Liu, Matei Zaharia, Pieter Abbeel
UC Berkeley
hao.liu@cs.berkeley.edu
Abstract
Transformers have emerged as the architecture of choice for many state-of-the-art AI mod-
els, showcasing exceptional performance across a wide range of AI applications. However,
the memory demands imposed by Transformers limit their ability to handle long sequences,
thereby creating challenges for tasks involving extended sequences or long-term dependencies.
We present a distinct approach, Ring Attention, which leverages blockwise computation of
self-attention to distribute long sequences across multiple devices while overlapping the commu-
nication of key-value blocks with the computation of blockwise attention. Ring Attention enables
training and inference of sequences that are up to device count times longer than those of prior
memory-efficient Transformers, effectively eliminating the memory constraints imposed by indi-
vidual devices. Extensive experiments on language modeling tasks demonstrate the effectiveness
of Ring Attention in allowing large sequence input size and improving performance.
1
Introduction
Transformers [39] have become the backbone of many state-of-the-art AI systems that have demon-
strated impressive performance across a wide range of AI problems. Transformers achieve this success
through their architecture design that uses self-attention and position-wise feedforward mechanisms.
Figure 1:
Maximum context length under end-
to-end large-scale training on TPUv4-1024. Base-
lines are vanilla transformers [39], memory effi-
cient transformers [31], and memory efficient at-
tention and feedforward (blockwise parallel trans-
formers) [24]. Our proposed approach Ring Atten-
tion allows training up to device count times longer
sequence than baselines and enables the training of
sequences that exceed millions in length without
making approximations to attention.
These components facilitate the efficient cap-
ture of long-range dependencies between input
tokens, and enable scalability through highly
parallel computations.
However, scaling up the context length of Trans-
formers is a challenge [30], since the inherited
architecture design of Transformers, i.e. the self-
attention has memory cost quadratic in the input
sequence length, which makes it challenging to
scale to longer input sequences. Large context
Transformers are essential for tackling a diverse
array of AI challenges, ranging from processing
books and high-resolution images to analyzing
long videos and complex codebases. They ex-
cel at extracting information from the intercon-
nected web and hyperlinked content, and are
crucial for handling complex scientific experi-
ment data. There have been emerging use cases
of language models with significantly expanded
context than before: GPT-3.5 [33] with context
length 16K, GPT-4 [30] with context length 32k,
MosaicML’s MPT [26] with context length 65k, and Anthropic’s Claude [1] with context length 100k.
Preprint.
arXiv:2310.01889v3  [cs.CL]  12 Oct 2023