as an oil painting in the style of
Interleaved Vision-Language Prompt
Figure 2: KOSMOS-G comprises an MLLM for multimodal perception, coupled with an AlignerNet
that bridges the MLLM to the diffusion U-Net image decoder. KOSMOS-G can pass the fine concept-
level guidance from interleaved input to image decoder, and offer a seamless alternative to CLIP.
Orange denotes the trainable modules; Blue denotes the frozen ones.
from the multimodal language modeling stage, leading to the KOSMOS-1 [HDW+23] MLLM. It
envisions language models as a universal task layer, perceiving free-form interleaved vision-language
inputs and consolidating various task predictions into textual formats. Given the aligned vision-
language representation, we then use the language modality as an anchor and align the output space
of the MLLM with the CLIP text encoder. Finally, we perform instruction tuning on the curated data.
KOSMOS-G accepts captions as input, where each entity is followed by its segmented image. The
model is trained to faithfully reproduce all entities, render the text content, and follow the instructions.
In this process, the frozen pre-trained diffusion image decoder serves as a score metric. We distill the
learned data distribution to pass the differentiable gradient to the MLLM This enables KOSMOS-G to