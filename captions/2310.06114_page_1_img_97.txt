Simulated 
navigation
Human  
manipulation
Internet 
scenes
Simulated 
manipulation
Real-world 
navigation
Internet 
robots
Robot 
manipulation
Internet texts, images, videos
Panarama 
scans
UniSim
Figure 1: A universal Simulator (UniSim). UniSim is a simulator of the real-world that learns from broad
data rich in different axes including objects, scenes, human activities, motions in navigation and manipulation,
panorama scans, and simulations and renderings.
divergence in information is natural and hard to overcome, posing difficulties to building a real-world
simulator that seeks to capture realistic experience of the world we live in.
In this work, we take the first steps towards building a universal simulator (UniSim) of real-world in-
teraction through generative modeling. Specifically, we propose to combine a wealth of data—ranging
from internet text-image pairs, to motion and action rich data from navigation, manipulation, human
activities, robotics, and data from simulations and renderings—in a conditional video generation
framework. With careful orchestration of data rich along different axes, we show that UniSim can
successfully merge the different axes of experience and generalize beyond the data, enabling rich
interaction through fine-grained motion control of otherwise static scenes and objects. In simulating
long-horizon interactions with UniSim, we develop a fundamental connection between autoregres-
sive video generation during inference and performing rollouts in a partially observable Markov
decision processes (POMDPs) [14, 15]. As a result, UniSim can simulate long-horizon interactions
consistently across video generation boundaries.
While the potential applications of UniSim are vast, we demonstrate a few practical use cases centered
around using simulated experience from UniSim. We first demonstrate how an embodied vision-
language planner can be trained to complete long-horizon goal-conditioned tasks through hindsight
relabeling of simulated experience [16]. In addition to high-level planning, we further illustrate how
UniSim can enable learning low-level control policies by leveraging model-based reinforcement
learning [17]. We show that both the high-level vision-language planner and the low-level control
policy, while trained purely in simulation, can generalize to real robot settings in a zero-shot manner,
achieving one step towards bridging the sim-to-real gap in embodied learning [18, 19, 20]. This is
enabled by using simulators that are nearly visually indistinguishable from the real world. Lastly, we
note that UniSim can be used to simulate rare events where data collection is expensive or dangerous
(e.g., crashes in self-driving cars). Such simulated videos can then be used to improve other machine
intelligence such as rare event detectors, suggesting broad applications of UniSim beyond embodied
learning. The main contributions can be summarized as follows:
• We take the first step toward building a universal simulator (UniSim) of real-world interaction
by combining diverse datasets rich in different axes—such as objects, scenes, actions, motions,
language, and motor controls—in a unified video generation framework.
• We establish the connection between conditional video generation and partially observable Markov
decision process (POMDP), and leverage multi-frame history conditioning to simulate consistent
long-horizon interactions from otherwise static scenes and objects.
• We illustrate how UniSim can simulate realistic experiences for training embodied planners, low-
level control policies, and video captioning models, equipping these other forms of machine
intelligence with the ability to generalize to the real world when trained purely in simulation,
thereby bridging the sim-to-real gap.
2