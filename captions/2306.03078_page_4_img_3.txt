0
1024
2048
3072
4096
5120
6144
7168
8192
input dim. (cols)
0
1024
2048
3072
4096
5120
6144
7168
8192
output dim. (rows)
Sample: self_attn.o_proj, Layer 79
6016
6144
6272
6400
6528
6656
6784
6912
7040
512
640
768
896
1024
Attention head pattern
5632
5696
5760
5824
5888
5952
6016
6080
6144
2688
2752
2816
2880
2944
Row outlier pattern
0
32
64
96
128
160
192
224
256
384
416
448
480
512
Rotary embedding pattern
2720
2752
2784
2816
2848
2880
2912
2944
2976
0
32
64
96
128
Column outlier pattern
Figure 2: Weight log-sensitivities from the last attention layer of LLaMA-65B. Dark-blue shades