Context type 1: Captions
A group of people standing outside of a black vehicle with various luggage.
Luggage surrounds a vehicle in an underground parking area
People try to ﬁt all of their luggage in an SUV.
The sport utility vehicle is parked in the public garage, being packed for a trip
Some people with luggage near a van that is transporting it.
Context type 2: Boxes
person: [0.681, 0.242, 0.774, 0.694], person: [0.63, 0.222, 0.686, 0.516], person: [0.444, 0.233,
0.487, 0.34], backpack: [0.384, 0.696, 0.485, 0.914], backpack: [0.755, 0.413, 0.846, 0.692],
suitcase: [0.758, 0.413, 0.845, 0.69], suitcase: [0.1, 0.497, 0.173, 0.579], bicycle: [0.282, 0.363,
0.327, 0.442], car: [0.786, 0.25, 0.848, 0.322], car: [0.783, 0.27, 0.827, 0.335], car: [0.86, 0.254,
0.891, 0.3], car: [0.261, 0.101, 0.787, 0.626]
Response type 1: conversation
Question: What type of vehicle is featured in the image?
Answer: The image features a black sport utility vehicle (SUV).
Question: Where is the vehicle parked?
Answer: The vehicle is parked in an underground parking area, likely in a public garage.
Question: What are the people in the image doing?
Answer: The people are trying to ﬁt all of their luggage into the SUV, likely preparing for a trip.
Response type 2: detailed description
The image is an underground parking area with a black sport utility vehicle (SUV) parked. There
are three people in the scene, with one person standing closer to the left side of the vehicle, another
person in the middle, and the third person on the right side. They are all working together to pack
their luggage into the SUV for a trip.
In the parking area, various luggage items are scattered around the vehicle. There are two backpacks,
one located near the left rear wheel and the other closer to the right side of the vehicle. Additionally,
there are two suitcases, one on the right side of the car and another further away near the center of
the parking area. A bicycle can also be seen on the left side of the vehicle.
Other cars are parked around the main SUV, with one car positioned behind it and slightly to the left,
another behind and slightly to the right, and the third car further behind on the right side.
Response type 3: complex reasoning
Question: What challenges do these people face?
Answer: In the image, a group of people is standing outside a black SUV in a parking area,
surrounded by various pieces of luggage, including suitcases and backpacks. They are facing the
challenge of ﬁtting all their luggage into the black SUV. There are multiple suitcases and backpacks
to be packed, which suggests that the group has a signiﬁcant amount of belongings to accommodate.
They might have to strategize and arrange the luggage efﬁciently to ensure that everything ﬁts
properly into the vehicle. Additionally, they need to consider the comfort of the passengers and
visibility while driving, so the placement of the luggage must not obstruct the driver’s view or make
the passengers uncomfortable during the trip.
Table 1: One example to illustrate the instruction-following data. The top block shows the contexts
such as captions and boxes used to prompt GPT, and the bottom block shows the three types of
responses. Note that the visual image is not used to prompt GPT, we only show it here as a reference.
GPT models in text-annotation tasks [15], we propose to leverage ChatGPT/GPT-4 for multimodal
instruction-following data collection, based on the widely existing image-pair data.
For an image Xv and its associated caption Xc, it is natural to create a set of questions Xq with the
intent to instruct the assistant to describe the image content. We prompt GPT-4 and curate such a
questions list in Table 8 in the Appendix. Therefore, a simple way to expand an image-text pair to
its instruction-following version is Human : Xq Xv<STOP>\n Assistant : Xc<STOP>\n. Though
cheap to construct, this simple expanded version lacks diversity and in-depth reasoning in both the
instructions and responses.
To mitigate this issue, we leverage language-only GPT-4 or ChatGPT as the strong teacher (both
accept only text as input), to create instruction-following data involving visual content. Speciﬁcally,
in order to encode an image into its visual features to prompt a text-only GPT, we use two types of
symbolic representations: (i) Captions typically describe the visual scene from various perspectives.
(ii) Bounding boxes usually localize the objects in the scene, and each box encodes the object concept
and its spatial location. One example is shown in the top block of Table 1.
3