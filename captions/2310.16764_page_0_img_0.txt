2023-10-26
ConvNets Match Vision Transformers at Scale
Samuel L Smith1, Andrew Brock1, Leonard Berrada1 and Soham De1
1Google DeepMind
Many researchers believe that ConvNets perform well on small or moderately sized datasets, but are not
competitive with Vision Transformers when given access to datasets on the web-scale. We challenge this
belief by evaluating a performant ConvNet architecture pre-trained on JFT-4B, a large labelled dataset of
images often used for training foundation models. We consider pre-training compute budgets between
0.4k and 110k TPU-v4 core compute hours, and train a series of networks of increasing depth and width
from the NFNet model family. We observe a log-log scaling law between held out loss and compute
budget. After fine-tuning on ImageNet, NFNets match the reported performance of Vision Transformers
with comparable compute budgets. Our strongest fine-tuned model achieves a Top-1 accuracy of 90.4%.
Keywords: ConvNets, CNN, Convolution, Transformer, Vision, ViTs, NFNets, JFT, Scaling, Image
Introduction
Convolutional Neural Networks (ConvNets) were
responsible for many of the early successes of
deep learning.
Deep ConvNets were first de-
ployed commercially over 20 years ago (Le-
Cun et al., 1998), while the success of AlexNet
on the ImageNet challenge in 2012 re-ignited
widespread interest in the field (Krizhevsky et al.,
2017). For almost a decade ConvNets (typically
ResNets (He et al., 2016a,b)) dominated com-
puter vision benchmarks. However in recent years
they have increasingly been replaced by Vision
Transformers (ViTs) (Dosovitskiy et al., 2020).
Simultaneously, the computer vision commu-
nity has shifted from primarily evaluating the
performance of randomly initialized networks
on specific datasets like ImageNet, to evaluat-
ing the performance of networks pre-trained on
large general purpose datasets collected from the
web. This raises an important question; do Vision
pute budgets beyond 500k TPU-v3 core hours
(Zhai et al., 2022), which significantly exceeds
the compute used to pre-train ConvNets.
We evaluate the scaling properties of the NFNet
model family (Brock et al., 2021), a pure con-
volutional architecture published concurrently
with the first ViT papers, and the last ConvNet
to set a new SOTA on ImageNet. We do not
make any changes to the model architecture or
the training procedure (beyond tuning simple
hyper-parameters such as the learning rate or
epoch budget). We consider compute budgets
up to a maximum of 110k TPU-v4 core hours,1
and pre-train on the JFT-4B dataset which con-
tains roughly 4 billion labelled images from 30k
classes (Sun et al., 2017). We observe a log-log
scaling law between validation loss and the com-
pute budget used to pre-train the model. After
fine-tuning on ImageNet, our networks match the
performance of pre-trained ViTs with comparable
compute budgets (Alabdulmohsin et al., 2023;
arXiv:2310.16764v1  [cs.CV]  25 Oct 2023