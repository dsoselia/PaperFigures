answering tasks after applying instruction tuning.
Specifically, we make the following contributions:
1. We introduce Retro 48B, the largest LLM pretrained with retrieval. To save the computation
budget, we continue to pretrain a 43B parameter GPT model (originally trained on 1.1T
tokens) on adddtional 100B tokens by retrieving from 1.2T tokens. In contrast to Retro-
fitting (Borgeaud et al., 2022), that freezes pretrained decoder weights, we unfreeze the
decoder, jointly train all the parameters and find better perplexity.1 Notably, we find the
perplexity improvement of Retro 48B over its GPT 43B counterpart is still significant even at
this scale.
2. After instruction tuning, InstructRetro 48B demonstrates strong zero-shot capability to incor-
porate context for question answering (QA), and significantly outperforms its GPT counterpart
using the same instruction tuning recipe. The full pipeline to train InstructRetro is shown in
Figure 1.
3. Perhaps surprisingly, we find that one can directly ablate the encoder from IntructRetro and
still obtain comparable results on zero-shot QA tasks. This highlights the promising direction
of obtaining better decoder-only LLMs through continued pretraining with retrieval before
instruction tuning.
We organize the rest of the paper as follows. We discuss related work in Section 2, and introduce the
continued pretraining of Retro 48B in Section 3. We present the instructing tuning in Section 4. We
report results in Section 5 and conclude the paper in Section 6.
2
RELATED WORK
Retrieval-augmented language models have been established for open domain question answering
for years (Karpukhin et al., 2020; Lewis et al., 2020; Guu et al., 2020; Borgeaud et al., 2022;
Izacard et al., 2022c). In the previous study, language models have been augmented with retrieval at
inference (Khandelwal et al., 2020; Yogatama et al., 2021), fine-tuning (Karpukhin et al., 2020; Lewis
et al., 2020; Guu et al., 2020; Huang et al., 2023), and pretraining (Borgeaud et al., 2022; Izacard
et al., 2022c; Wang et al., 2023a). Retrieval-augmented pretraining is particularly interesting, as it
can largely reduce model perplexity (Borgeaud et al., 2022), enhance factuality (Wang et al., 2023a),
and improve downstream task accuracy after task-specific fine-tuning (Izacard et al., 2022c).
In contrast to the state-of-the-art decoder-only LLMs with hundreds of billion parameters (Brown
et al., 2020b; Rae et al., 2021; Smith et al., 2022; Chowdhery et al., 2022), the sizes of pretrained
retrieval-augmented LLMs are still around 10B parameters (Borgeaud et al., 2022; Wang et al., 2023a;
Izacard et al., 2022b), which largely limits the zero-shot generalization capability after instruction
tuning (Wei et al., 2022a; Ouyang et al., 2022; Chung et al., 2022). For example, Wei et al. (2022a)
finds instruction tuning becomes effective when the decoder-only LLM has around 50B parameters.
1Note that, it turns out that unfreezing of decoder is an important design not only for better perplexity, and it
eventually leads to the interesting finding after instruction tuning.
2