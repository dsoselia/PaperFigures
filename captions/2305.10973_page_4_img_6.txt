Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold
SIGGRAPH â€™23 Conference Proceedings, August 6â€“10, 2023, Los Angeles, CA, USA
Inputs
Ours
UserControllableLT
Fig. 4. Qualitative comparison of our approach to UserControllableLT [Endo 2022] on the task of moving handle points (red dots) to target points (blue dots).
Our approach achieves more natural and superior results on various datasets. More examples are provided in Fig. 10.
space, depending on whether the user wants a more constrained
image manifold or not. As W+ space is easier to achieve out-of-
distribution manipulations (e.g., cat in Fig. 16), we use W+ in this
work for better editability. In practice, we observe that the spatial
attributes of the image are mainly affected by the ğ‘¨ï¿½ï¿½ï¿½ for the first
6 layers while the remaining ones only affect appearance. Thus,
inspired by the style-mixing technique [Karras et al. 2019], we only
update the ğ‘¨ï¿½ï¿½ï¿½ for the first 6 layers while fixing others to preserve the
appearance. This selective optimization leads to the desired slight
movement of image content.
3.3
Point Tracking
The previous motion supervision results in a new latent code ğ‘¨ï¿½ï¿½ï¿½â€²,
new feature maps Fâ€², and a new image Iâ€². As the motion supervision
step does not readily provide the precise new locations of the handle
points, our goal here is to update each handle point ğ‘¨ï¿½ï¿½ï¿½ğ´ï¿½ï¿½ï¿½ such that it
tracks the corresponding point on the object. Point tracking is typi-
cally performed via optical flow estimation models or particle video
approaches [Harley et al. 2022]. Again, these additional models can
significantly harm efficiency and may suffer from accumulation
error, especially in the presence of alias artifacts in GANs. We thus
present a new point tracking approach for GANs. The insight is that
the discriminative features of GANs well capture dense correspon-
dence and thus tracking can be effectively performed via nearest
neighbor search in a feature patch. Specifically, we denote the fea-
ture of the initial handle point as ğ‘¨ï¿½ï¿½ï¿½ğ´ï¿½ï¿½ï¿½ = F0(ğ‘¨ï¿½ï¿½ï¿½ğ´ï¿½ï¿½ï¿½). We denote the patch
around ğ‘¨ï¿½ï¿½ï¿½ğ´ï¿½ï¿½ï¿½ as Î©2(ğ‘¨ï¿½ï¿½ï¿½ğ´ï¿½ï¿½ï¿½,ğ´ï¿½ï¿½ï¿½2) = {(ğ´ï¿½ï¿½ï¿½,ğ‘¦) | |ğ´ï¿½ï¿½ï¿½ âˆ’ ğ´ï¿½ï¿½ï¿½ğ´ï¿½ï¿½ï¿½,ğ´ï¿½ï¿½ï¿½ | < ğ´ï¿½ï¿½ï¿½2, |ğ‘¦ âˆ’ ğ‘¦ğ´ï¿½ï¿½ï¿½,ğ´ï¿½ï¿½ï¿½ | < ğ´ï¿½ï¿½ï¿½2}.
Then the tracked point is obtained by searching for the nearest
neighbor of ğ´ï¿½ï¿½ï¿½ğ´ï¿½ï¿½ï¿½ in Î©2(ğ‘¨ï¿½ï¿½ï¿½ğ´ï¿½ï¿½ï¿½,ğ´ï¿½ï¿½ï¿½2):
ğ‘¨ï¿½ï¿½ï¿½ğ´ï¿½ï¿½ï¿½ :=
arg min
ğ‘¨ï¿½ï¿½ï¿½ğ´ï¿½ï¿½ï¿½ âˆˆÎ©2(ğ‘¨ï¿½ï¿½ï¿½ğ´ï¿½ï¿½ï¿½,ğ´ï¿½ï¿½ï¿½2)
âˆ¥Fâ€²(ğ‘¨ï¿½ï¿½ï¿½ğ´ï¿½ï¿½ï¿½) âˆ’ ğ‘¨ï¿½ï¿½ï¿½ğ´ï¿½ï¿½ï¿½ âˆ¥1.
(2)
In this way, ğ‘¨ï¿½ï¿½ï¿½ğ´ï¿½ï¿½ï¿½ is updated to track the object. For more than one
handle point, we apply the same process for each point. Note that
here we are also considering the feature maps Fâ€² after the 6th block
of StyleGAN2. The feature maps have a resolution of 256 Ã— 256 and
are bilinear interpolated to the same size as the image if needed,
which is sufficient to perform accurate tracking in our experiments.
We analyze this choice at Sec. 4.2.
3.4
Implementation Details
We implement our approach based on PyTorch [Paszke et al. 2017].
We use the Adam optimizer [Kingma and Ba 2014] to optimize
the latent code ğ‘¨ï¿½ï¿½ï¿½ with a step size of 2e-3 for FFHQ [Karras et al.
2019], AFHQCat [Choi et al. 2020], and LSUN Car [Yu et al. 2015]
datasets and 1e-3 for others. The hyper-parameters are set to be
ğ›¹ï¿½ï¿½ï¿½ = 20,ğ´ï¿½ï¿½ï¿½1 = 3,ğ´ï¿½ï¿½ï¿½2 = 12. In our implementation, we stop the optimiza-
tion process when all the handle points are no more than ğ´ï¿½ï¿½ï¿½ pixel
away from their corresponding target points, where ğ´ï¿½ï¿½ï¿½ is set to 1
for no more than 5 handle points and 2 otherwise. We also develop
a GUI to support interactive image manipulation. Thanks to the
computational efficiency of our approach, users only need to wait
for a few seconds for each edit and can continue the editing until
satisfied. We highly recommend readers refer to the supplemental
video for live recordings of interactive sessions.
4
EXPERIMENTS
Datasets. We evaluate our approach based on StyleGAN2 [Karras
et al. 2020] pretrained on the following datasets (the resolution of
the pretrained StyleGAN2 is shown in brackets): FFHQ (512) [Karras
et al. 2019], AFHQCat (512) [Choi et al. 2020], SHHQ (512) [Fu et al.
2022], LSUN Car (512) [Yu et al. 2015], LSUN Cat (256) [Yu et al.
2015], Landscapes HQ (256) [Skorokhodov et al. 2021], microscope
5