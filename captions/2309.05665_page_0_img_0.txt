Robot Parkour Learning
Ziwen Zhuang*13 Zipeng Fu*2 Jianren Wang4 Christopher Atkeson4 SÂ¨oren Schwertfeger3
Chelsea Finn2 Hang Zhao15
1Shanghai Qi Zhi, 2Stanford, 3ShanghaiTech, 4CMU, 5Tsinghua, *project co-leads
project website:
https://robot-parkour.github.io
Figure 1: We present a framework for learning parkour skills on low-cost robots. Our end-to-end vision-based
parkour learning system enable the robot to climb high obstacles, leap over large gaps, crawl beneath low
barriers, squeeze through thin slits and run. Videos are on the project website.
Abstract: Parkour is a grand challenge for legged locomotion that requires robots to
overcome various obstacles rapidly in complex environments. Existing methods can
generate either diverse but blind locomotion skills or vision-based but specialized
skills by using reference animal data or complex rewards. However, autonomous
parkour requires robots to learn generalizable skills that are both vision-based and
diverse to perceive and react to various scenarios. In this work, we propose a system
for learning a single end-to-end vision-based parkour policy of diverse parkour
skills using a simple reward without any reference motion data. We develop a
reinforcement learning method inspired by direct collocation to generate parkour
skills, including climbing over high obstacles, leaping over large gaps, crawling
beneath low barriers, squeezing through thin slits, and running. We distill these
skills into a single vision-based parkour policy and transfer it to a quadrupedal robot
using its egocentric depth camera. We demonstrate that our system can empower
two different low-cost robots to autonomously select and execute appropriate
parkour skills to traverse challenging real-world environments.
Keywords: Agile Locomotion, Visuomotor Control, Sim-to-Real Transfer
1
Introduction
Humans and animals possess amazing athletic intelligence. Parkour is an examplar of athletic
intelligence of many biological beings capable of moving swiftly and overcoming various obstacles
in complex environments by running, climbing, and jumping [1]. Such agile and dynamic movements
require real-time visual perception and memorization of surrounding environments [2, 3], tight
7th Conference on Robot Learning (CoRL 2023), Atlanta, USA.
arXiv:2309.05665v2  [cs.RO]  12 Sep 2023