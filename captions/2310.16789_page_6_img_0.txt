100
The Violin of Auschwitz
Maria Àngels Anglada
2010
100
North American Stadiums
Grady Chambers
2018
100
White Chappell Scarlet Tracings
Iain Sinclair
1987
100
Lost and Found
Alan Dean
2001
100
A Different City
Tanith Lee
2015
100
Our Lady of the Forest
David Guterson
2003
100
The Expelled
Mois Benarroch
2013
99
Blood Cursed
Archer Alex
2013
99
Genesis Code: A Thriller of the Near Future
Jamie Metzl
2014
99
The Sleepwalker’s Guide to Dancing
Mira Jacob
2014
99
The Harlan Ellison Hornbook
Harlan Ellison
1990
99
The Book of Freedom
Paul Selig
2018
99
Three Strong Women
Marie NDiaye
2009
99
The Leadership Mind Switch: Rethinking How We Lead in the
New World of Work
D. A. Benton, Kylie Wright-Ford
2017
99
Gold
Chris Cleave
2012
99
The Tower
Simon Clark
2005
98
Amazon
Bruce Parry
2009
98
Ain’t It Time We Said Goodbye: The Rolling Stones on the
Road to Exile
Robert Greenfield
2014
98
Page One
David Folkenflik
2011
98
Road of Bones: The Siege of Kohima 1944
Fergal Keane
2010
6
CASE STUDY: DETECTING DOWNSTREAM DATASET CONTAMINATION
Assessing the leakage of downstream task data into pretraining corpora is an important issue, but it is
challenging to address given the lack of access to pretraining datasets. In this section, we investigate
the possibility of using MIN-K% PROB to detect information leakage and perform ablation studies
to understand how various training factors impact detection difficulty. Specifically, we continually
pretrain the 7B parameter LLaMA model (Touvron et al., 2023a) on pretraining data that have been
purposefully contaminated with examples from the downstream task.
6.1
EXPERIMENTS
Experimental setup.
To simulate downstream task contamination that could occur in real-world
settings, we create contaminated pretraining data by inserting examples from downstream tasks into a
pretraining corpus. Specifically, we sample text from the RedPajama corpus (TogetherCompute, 2023)
and insert formatted examples from the downstream datasets BoolQ (Clark et al., 2019), IMDB (Maas
et al., 2011), Truthful QA (Lin et al., 2021), and Commonsense QA (Talmor et al., 2019) in contiguous
segments at random positions in the uncontaminated text. We insert 200 (positive) examples from each
of these datasets into the pretraining data while also isolating a set of 200 (negative) examples from
7