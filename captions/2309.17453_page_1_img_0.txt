Preprint
(a) Dense Attention
(b) Window Attention
(d) StreamingLLM (ours)
Attention Sink
⋯
T cached tokens
⋯
L cached 
tokens
⋯
T-L evicted 
tokens
Current Token
(c) Sliding Window  
w/ Re-computation
L re-computed 
tokens
⋯
previous tokens 
are truncated
⋯
L cached 
tokens
⋯
evicted 
tokens
O(T2)
O(TL)
O(TL2)
O(TL)
PPL: 5158
PPL: 5641
PPL: 5.43
PPL: 5.40
Has poor efficiency and 
performance on long text.
Breaks when initial 
tokens are evicted.
Has to re-compute cache 
for each incoming token.
Can perform efficient and stable 
language modeling on long texts.
Figure 1: Illustration of StreamingLLM vs. existing methods. The language model, pre-trained on
texts of length L, predicts the Tth token (T ≫ L). (a) Dense Attention has O(T 2) time complexity
and an increasing cache size. Its performance decreases when the text length exceeds the pre-training
text length. (b) Window Attention caches the most recent L tokens’ KV. While efficient in inference,
performance declines sharply once the starting tokens’ keys and values are evicted. (c) Sliding
Window with Re-computation rebuilds the KV states from the L recent tokens for each new token.
While it performs well on long texts, its O(TL2) complexity, stemming from quadratic attention
in context re-computation, makes it considerably slow. (d) StreamingLLM keeps the attention sink
(several initial tokens) for stable attention computation, combined with the recent tokens. It’s efficient
and offers stable performance on extended texts. Perplexities are measured using the Llama-2-13B
model on the first book (65K tokens) in the PG-19 test set.
When applying LLMs for infinite input streams, two primary challenges arise:
1. During the decoding stage, Transformer-based LLMs cache the Key and Value states (KV)
of all previous tokens, as illustrated in Figure 1 (a), which can lead to excessive memory
usage and increasing decoding latency (Pope et al., 2022).
2. Existing models have limited length extrapolation abilities, i.e., their performance de-
grades (Press et al., 2022; Chen et al., 2023) when the sequence length goes beyond the
attention window size set during pre-training.
An intuitive approach, known as window attention (Beltagy et al., 2020) (Figure 1 b), maintains only
a fixed-size sliding window on the KV states of most recent tokens. Although it ensures constant
memory usage and decoding speed after the cache is initially filled, the model collapses once the
sequence length exceeds the cache size, i.e., even just evicting the KV of the first token, as illustrated
in Figure 3. Another strategy is the sliding window with re-computation (shown in Figure 1 c), which
rebuilds the KV states of recent tokens for each generated token. While it offers strong performance,
this approach is significantly slower due to the computation of quadratic attention within its window,
making this method impractical for real-world streaming applications.
To understand the failure of window attention, we find an interesting phenomenon of autoregressive
LLMs: a surprisingly large amount of attention score is allocated to the initial tokens, irrespective
of their relevance to the language modeling task, as visualized in Figure 2. We term these tokens
“attention sinks". Despite their lack of semantic significance, they collect significant attention scores.
We attribute the reason to the Softmax operation, which requires attention scores to sum up to one
for all contextual tokens. Thus, even when the current query does not have a strong match in many
previous tokens, the model still needs to allocate these unneeded attention values somewhere so it
sums up to one. The reason behind initial tokens as sink tokens is intuitive: initial tokens are visible
to almost all subsequent tokens because of the autoregressive language modeling nature, making
them more readily trained to serve as attention sinks.
Based on the above insights, we propose StreamingLLM, a simple and efficient framework that
enables LLMs trained with a finite attention window to work on text of infinite length without fine-
tuning. StreamingLLM exploits the fact that attention sinks have high attention values, and preserving
them can maintain the attention score distribution close to normal. Therefore, StreamingLLM simply
keeps the attention sink tokens’ KV (with just 4 initial tokens sufficing) together with the sliding
window’s KV to anchor the attention computation and stabilize the model’s performance. With
StreamingLLM, models including Llama-2-[7, 13, 70]B, MPT-[7, 30]B, Falcon-[7, 40]B, and Pythia-
2