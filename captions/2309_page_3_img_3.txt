Preprint
Dense Attention
Window Attention
Sliding Window 
w/ Re-computation
StreamingLLM
Dense Attention
Window Attention
StreamingLLM
Figure 3: Language modeling perplexity on texts with 20K tokens across various LLM. Observations
reveal consistent trends: (1) Dense attention fails once the input length surpasses the pre-training
attention window size. (2) Window attention collapses once the input length exceeds the cache size,
i.e., the initial tokens are evicted. (3) StreamingLLM demonstrates stable performance, with its
perplexity nearly matching that of the sliding window with re-computation baseline.
and Li et al., success in the previously mentioned two directions does not necessarily translate to
competent utilization of lengthy contexts. Addressing this effective usage of prolonged contexts
within LLMs is still a challenge. Our work concentrates on stably harnessing the most recent tokens,
enabling the seamless streaming application of LLMs.
3
STREAMINGLLM
3.1
THE FAILURE OF WINDOW ATTENTION AND ATTENTION SINKS
While the window attention technique offers efficiency during inference, it results in an exceedingly
high language modeling perplexity. Consequently, the model’s performance is unsuitable for deploy-
ment in streaming applications. In this section, we use the concept of attention sink to explain the
failure of window attention, serving as the inspiration behind StreamingLLM.
Identifying the Point of Perplexity Surge.
Figure 3 shows the perplexity of language modeling on
a 20K token text. It is evident that perplexity spikes when the text length surpasses the cache size, led
by the exclusion of initial tokens. This suggests that the initial tokens, regardless of their distance
from the tokens being predicted, are crucial for maintaining the stability of LLMs.
Why do LLMs break when removing initial tokens’ KV?
We visualize attention maps from
all layers and heads of the Llama-2-7B and models in Figure 2. We find that, beyond the bottom
two layers, the model consistently focuses on the initial tokens across all layers and heads. The
implication is clear: removing these initial tokens’ KV will remove a considerable portion of the
denominator in the SoftMax function (Equation 1) in attention computation. This alteration leads to a
significant shift in the distribution of attention scores away from what would be expected in normal
inference settings.
SoftMax(x)i =
exi
ex1 + �N
j=2 exj ,
x1 ≫ xj, j ∈ 2, . . . , N
(1)
There are two possible explanations for the importance of the initial tokens in language modeling:
(1) Either their semantics are crucial, or (2) the model learns a bias towards their absolute position.
To distinguish between these possibilities, we conduct experiments (Table 1), wherein the first four
tokens are substituted with the linebreak token “\n". The observations indicate that the model still
significantly emphasizes these initial linebreak tokens. Furthermore, reintroducing them restores
the language modeling perplexity to levels comparable to having the original initial tokens. This
suggests that the absolute position of the starting tokens, rather than their semantic value, holds
greater significance.
LLMs attend to Initial Tokens as Attention Sinks.
To explain why the model disproportionately
focuses on initial tokens—regardless of their semantic relevance to language modeling, we introduce
the concept of “attention sink". The nature of the SoftMax function (Equation 1) prevents all attended
tokens from having zero values. This requires aggregating some information from other tokens across
all heads in all layers, even if the current embedding has sufficient self-contained information for its
prediction. Consequently, the model tends to dump unnecessary attention values to specific tokens. A
similar observation has been made in the realm of quantization outliers (Xiao et al., 2023; Bondarenko
et al., 2023), leading to the proposal of SoftMax-Off-by-One (Miller, 2023) as a potential remedy.
4