Step 1: push blue triangle to …
Step 1: push red star to left …
Step 2: (re-plan)
t
t + k
t current
t goal
Step 2: (re-plan)
Step 2: (re-plan)
Figure 1: Video Language Planning uses forward tree search via vision-language models and text-to-video
models to construct long-horizon video plans. From an image observation, the VLM policy (top left) generates
next-step text actions, which a video model converts into possible future image sequences (top right). Future
image states are evaluated using a VLM heuristic function (bottom left), and the best sequence is recursively
expanded with tree search (middle). Video plans can be converted to action execution with goal-conditioned
policies (bottom right).
ways that are more information-rich than text, and (ii) can absorb another source of Internet data e.g.,
YouTube videos. This leads to the natural question of how to build a planning algorithm that can
leverage both long-horizon abstract planning from LLMs / VLMs and detailed dynamics and motions
from text-to-video-models.
In this work, we propose to integrate vision-language models and text-to-video models to enable video
language planning (VLP), where given the current image observation and a language instruction, the
VLM
i f
hi h l
l
i
d
id
d l
di
h l
l
l