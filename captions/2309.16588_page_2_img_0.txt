input image
DINO norms
DINOv2 norms
0
20
40
60
80
100
0
200
400
600
L2 norm
10 5
10 3
10 1
DINO norms
0
200
400
600
L2 norm
10 5
10 3
10 1
DINOv2 norms
Figure 3: Comparison of local feature norms for DINO ViT-B/16 and DINOv2 ViT-g/14. We ob-
serve that DINOv2 has a few outlier patches, whereas DINO does not present these artifacts. For
DINOv2, although most patch tokens have a norm between 0 and 100, a small proportion of tokens
have a very high norm. We measure the proportion of tokens with norm larger than 150 at 2.37%.
servation suggests that the model discards the local information contained in these patches during
inference. On the other hand, learning an image classifier on outlier patches yields significantly
stronger accuracy than doing so on the other patches, suggesting that they contain global informa-
tion about the image. We propose the following interpretation to these elements: the model learns
to recognize patches containing little useful information, and recycle the corresponding tokens to
aggregate global image information while discarding spatial information.
This interpretation is consistent with an inner mechanism in transformer models that allows per-
forming computations within a restricted set of tokens. In order to test this hypothesis, we append
additional tokens - that we call registers - to the token sequence, independent of the input image. We
train several models with and without this modification and observe that the outlier tokens disappear
from the sequence entirely. As a result, the performance of the models increases in dense prediction
tasks, and the resulting feature maps are significantly smoother. These smooth feature maps enable
object discovery methods like LOST mentioned above with the updated models.
2
PROBLEM FORMULATION
As shown in Fig. 2, most modern vision transformers exhibit artifacts in the attention maps. The
unsupervised DINO backbone (Caron et al., 2021) has been previously praised for the quality of
local features and interpretability of attention maps. Surprisingly, the outputs of the subsequent
DINOv2 models have been shown to hold good local information but exhibit undesirable artifacts in
attention maps. In this section, we propose to study why and when these artifacts appear. While this
work focuses on alleviating artefacts in all vision transformers, we focus our analysis on DINOv2.
2.1
ARTIFACTS IN THE LOCAL FEATURES OF DINOV2
Artifacts are high-norm outlier tokens.
We want to find a quantitative way of characterizing
artefacts that appear in the local features. We observe that an important difference between “artifact”
patches and other patches is the norm of their token embedding at the output of the model. In Fig. 3
(left), we compare the norm of local features for a DINO and DINOv2 model given a reference
image. We clearly see that the norm of artifact patches is much higher than the norm of other
patches. We also plot the distribution of feature norms over a small dataset of images in Fig. 3
(right), which is clearly bimodal, allowing us to choose a simple criterion for the rest of this section:
tokens with norm higher than 150 will be considered as “high-norm” tokens, and we will study their
properties relative to regular tokens. This hand-picked cutoff value can vary across models. In the
rest of this work, we use “high-norm” and “outlier” interchangeably.
O tli
d
i
th t
i i
f l
d l
W
k
l dditi
l b
ti