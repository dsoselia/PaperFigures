Technical report. In progress
Matryoshka Diffusion Models
Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Josh Susskind & Navdeep Jaitly
Apple
{jgu32,szhai,yizzhang,jsusskind,njaitly}@apple.com
Figure 1: (←↑) Images generated by MDM at 642, 1282, 2562, 5122 and 10242 resolutions using
the prompt “a Stormtrooper Matryoshka doll, super details, extreme realistic, 8k”; (←↓) 1 and 16
frames of 642 video generated by our method using the prompt “pouring milk into black coffee”; All
other samples are at 10242 given various prompts. Images were resized for ease of visualization.
Abstract
Diffusion models are the de-facto approach for generating high-quality images
and videos but learning high-dimensional models remains a formidable task due
to computational and optimization challenges. Existing methods often resort to
training cascaded models in pixel space, or using a downsampled latent space
of a separately trained auto-encoder.
In this paper, we introduce Matryoshka
Diffusion (MDM), an end-to-end framework for high-resolution image and video
synthesis. We propose a diffusion process that denoises inputs at multiple reso-
lutions jointly and uses a NestedUNet architecture where features and parameters
for small scale inputs are nested within those of the large scales. In addition,
MDM enables a progressive training schedule from lower to higher resolutions
which leads to significant improvements in optimization for high-resolution gener-
ation. We demonstrate the effectiveness of our approach on various benchmarks,
including class-conditioned image generation, high-resolution text-to-image, and
text-to-video applications. Remarkably, we can train a single pixel-space model at
resolutions of up to 1024 × 1024 pixels, demonstrating strong zero shot general-
ization using the CC12M dataset, which contains only 12 million images.
1
arXiv:2310.15111v1  [cs.CV]  23 Oct 2023