Crop Box
Point
Box
HandDrawing
Circle
Coordinate
(0.47, 0.48, 0.55, 0.87)
0
1
1
Arrow
Crop Box
Point
Box
HandDrawing
Circle
Coordinate
(0.47, 0.48, 0.55, 0.87)
Crop Mask
0
1
1
Arrow
Figure 5: Different modes of “visual pointing” in multimodal interaction.
pixels, instead of the conventional text prompts, to perform the task of interest. For example, it could
be a simple grounded description, which focuses on describing the pointed object while maintaining
the understanding of the global image context, as shown in Figure 6 (1,2). Visual referring prompting
also enables other novel use cases, such as associating the pointed object with an index written in
scene text (Figure 6 (3)), or solving the question asked near the queried edge or angle (Figure 6 (4)).
Section 5 will discuss visual referring prompting in more detail.
3.3
Visual + Text Prompting
Visual referring prompting can be smoothly used together with other image-text prompts, presenting
a nuanced interface that succinctly represents the problem of interest. Figure 7 presents two examples
to showcase the flexibility of GPT-4V’s prompt, particularly its proficiency in integrating different
input formats and seamlessly mixing instructions with examples in the inputs. GPT-4V’s genericity
and flexibility result in a human-like comprehension of multimodal instructions and an unprecedented
ability to adapt to unseen tasks.
Integrated multimodal instruction inputs.
Existing models usually have implicit constraints on
how interleaved image-text inputs should be formatted, e.g., in-context few-shot learning requires
image-text pairs to share a similar format as the query input. In contrast, GPT-4V shows the genericity
in processing an arbitrary mix of images, sub-images, texts, scene texts, and visual pointers. For
example, to illustrate the “adding a line” pattern in Figure 7, one could either point to the first column
in the matrix image with a circle as in sub-figure (1), or incorporate the sub-images inline as in
sub-figure (2). Similarly, for input query, one could either present a large figure with the question
as scene texts as in sub-figure (1), or send the mix of texts and sub-images as in sub-figure (2). In
contrast to GPT-4V’s flexibility, existing multimodal models are highly restricted in terms of how
they can combine images and texts, and the number of images they can process, thereby imposing
limitations on the model’s capability and genericity.
Multimodal example-grounded instruction.
In addition to supporting more flexible input formats,
GPT-4V’s genericity also opens up more effective ways of illustrating the task to perform, compared
with the instruction-following mode and in-context few-shot learning. Instruction-following tech-
niques [102, 96, 134, 111], originally proposed for NLP tasks, intuitively focus on task instructions
purely in the textual format. The text instruction is loosely related to the visual query input and thus
may not provide a clear task demonstration. While in-context few-shot learning [23, 125, 7] provides
test-time examples that contain both images and texts, these examples must align perfectly with
the format of the inference query, making them complex and lengthy to incorporate. Furthermore,
in-context examples are usually used separately from instructions, requiring the model to infer the
task objective and thereby compromising the demonstration’s effectiveness. In contrast, GPT-4V’s
capability to comprehend multimodal instructions enables task demonstrations to be grounded onto
corresponding in-context examples, therefore more effectively illustrating the task of interest. For
example, in Figure 7, grounding instructions of “finding the pattern in the first column” onto the key
steps in demonstration examples (i.e., the circled pattern in (1) and corresponding sub-figures in (2))
simplifies the learning process and enhances the model’s performance. This approach also mirrors
the human learning process, which involves abstract instructions paired with intuitive examples.
16