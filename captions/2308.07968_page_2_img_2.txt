Teach LLMs to Personalize – An Approach inspired by Writing Education
XXX, XXX, XXX
Immediate context of 
the current document
1. Retrieve
Key elements
Top entries
2. Rank
Summary
Personal context
3. Summarize
4. Synthesize
Personalized 
document
Same Author?
Multitasking
A document pair
Predict
Generate
LLM
Figure 1: The overview of the multistage multitask frame-
work for personalized text generation.
user 𝐴��� for the current document 𝐴���𝐴���𝐴��� at time step 𝐴���, and D𝐴���𝐴��� =
{𝐴���𝐴���1,𝐴���𝐴���2, ...,𝐴���𝐴���,𝐴���−1} is the personal context of past documents, we
want to train a personalized generation model G that generates
𝐴���′
𝐴���𝐴��� based on (𝐴���𝐴���𝐴���, D𝐴���𝐴���) so that we can maximize the similarity
between 𝐴���′
𝐴���𝐴��� and 𝐴���𝐴���𝐴���. Whenever it is clear from the context, we will
omit the subscript 𝐴��� and directly use 𝐴���𝐴���, D𝐴��� and 𝐴���𝐴��� instead. We will
also use “user” and “author” interchangeably.
4
METHOD OVERVIEW
The overview of our multistage multitask framework for personal-
ized text generation is presented in Figure 1.
Given the immediate context 𝐴���𝐴��� of the current document written
by a user 𝐴���, a retriever Re(𝐴���𝐴���, D𝐴���) retrieves entries from past user
document set D𝐴��� using 𝐴���𝐴��� as the query. The returned entries are
fed to a ranker Ra to produce ranked entries E𝐴��� = Ra(Re(𝐴���𝐴���, D𝐴���)),
which are consumed by: (1) a summarization model Su(𝐴���𝐴���, E𝐴���)
to produce a summary of the multiple documents retrieved; and
(2) a synthesis model Sy(𝐴���𝐴���, E𝐴���) to produce key elements in these
documents. The personalized generation model G generates the cur-
rent document 𝐴���′
𝐴��� = G(𝐴���𝐴���, Su(𝐴���𝐴���, E𝐴���), Sy(𝐴���𝐴���, E𝐴���), E𝐴���) and is trained
against the ground-truth current document 𝐴���𝐴���.
We additionally consider an auxiliary task, called author dis-
tinction, to help the model better understand the user context and
generate better personalized content. Given a document 𝐴���𝐴���𝐴��� writ-
ten by a user 𝐴���, we randomly sample another document 𝐴���𝐴���𝐴��� to
form a document pair. The model G is then trained on a set of
tuples {(𝐴���𝐴���𝐴���,𝐴���𝐴���𝐴���),𝑦}, where the label 𝑦 = 𝐴���𝐴���𝐴���𝐴��� if 𝐴��� = 𝐴���, otherwise
𝑦 = 𝐴��� 𝐴���𝐴���𝐴���𝐴���. Note that we use text {𝐴���𝐴���𝐴���𝐴���, 𝐴��� 𝐴���𝐴���𝐴���𝐴���} instead of numerical
labels for 𝑦 since G is a sequence-to-sequence model.
5
PERSONALIZED TEXT GENERATION
We discuss the detail of each stage as outlined in Section 4.
BM25 [28] as the sparse retriever. We use a T5X Retrieval model [19,
21], GTR-Large, as our dense retriever. We do not choose models of
larger sizes since they demonstrate similar performance but much
worse effectiveness-latency trade-offs on benchmark datasets [21].
For dense retrieval, we experiment with two levels of granularity
when indexing personal document entries: a document level and
a snippet level. We do not choose a sentence level since many
sentences are too short to offer enough context information. We
create a snippet in this way: we keep appending sentences from
the same document until we reach 250 characters or we reach the
end of the document.
Since the snippets to retrieve are quite short, we only examine
the performance of sparse retrieval at the document level.
5.2
Ranking
Since we experiment with indexing entries at both document and
snippet level, we can rank entries accordingly:
• RankDocBM25. For sparse retrieval, we retrieve and rank
documents based on BM25 scores.
• RankDocDense. For dense retrieval, when we retrieve en-
tries at the document level, we rank retrieved documents
based on their embedding similarity with the embedding of
the immediate context 𝐴���𝐴���.
• RankSnippet. Similarly, for dense retrieval, when we re-
trieve entries at the snippet level, we rank retrieved snippets
based on embedding similarity.
During analysis, we find that issues exist for both RankDoc-
Dense and RankSnippet. The retrieved results via RankDocDense
can be less relevant since embeddings are less effective when the
documents to encode are long. While for RankSnippet, many sim-
ilar snippets are retrieved, providing insufficient information for
generation. For example, if the immediate context is I really enjoyed
reading the book, we might retrieve similar snippets like I enjoy the
book, The book is fun, or I love this book. They are all relevant but
do not provide enough details on why this user enjoys a particular
book, which is critical for passage-level generation.
To alleviate the two issues, we propose another dense retrieval
strategy, RankDocBySnpt, inspired by past work on using passage
evidence in retrieval [2]. RankDocBySnpt retrieves relevant text at
the snippet level, which addresses the issue that document embed-
dings are less effective. At the ranking stage, instead of directly rank-
ing snippets, we rank documents that contain the retrieved snippets,
to mitigate lack of diversity in snippets retrieved via RankSnip-
pet. Specifically for each document 𝐴���𝐴���, we compute the embedding
similarity score between each retrieved snippet 𝐴���������
∈ 𝐴������ and the