Base 0-shot
76.1%
Base 1-shot
76.0%
Base 2-shot
75.7%
Base + COT 0-shot
77.5%
OpenAI 0-shot
77.4%
OpenAI 1-shot
76.2%
OpenAI 2-shot
76.3%
OpenAI 8-shot
69.8%
OpenAI + COT 0-shot
78.0%
OpenAI + COT 1-shot
77.4%
OpenAI + COT 2-shot
76.8%
Table 2: We observe that prompting with the detailed
OpenAI preamble and eliciting chain-of-thought reason-
ing gives the highest AI Labeler Alignment. In-context
learning does not improve accuracy, and possibly even
makes it worse.
COT 0-shot" vs. 76.1% "Base 0-shot"). Though the
improvement from combining the two techniques
does not match the sum of their individual gains,
the techniques are still complementary, together
yielding +1.9% improvement.
We observe that few-shot in-context learning
does not improve alignment, even potentially de-
grading it. For "OpenAI + COT k-shot" prompts,
we see accuracy monotonically decrease as k in-
creases from 0 to 2. One hypothesis is that the LLM
is able to generate more useful chain-of-thought
rationales on its own than when it follows the ratio-
nales given in our 1-shot and 2-shot exemplars (see
Table 9 for examples).
To understand if adding more exemplars might
shot prompt and found that accuracy decreased by
-7.6% (69.8% "OpenAI 8-shot" vs. 77.4% "OpenAI
0-shot"). We verified that all examples used in
this experiment fit within our AI labelerâ€™s context
length.
Overall, we observe that the optimal configura-
tion employs a detailed preamble, chain-of-thought
reasoning, and no in-context learning ("OpenAI +
COT 0-shot"). This combination achieves an AI La-
beler Alignment of 78.0%, which is +1.9% higher
than using our most basic prompt ("Base 0-shot").
As a point of comparison, Stiennon et al. (2020) es-
timated that human inter-annotator agreement was
73-77% on the human preference dataset, suggest-
ing that our LLM performs rather well. We use
the "OpenAI + COT 0-shot" prompt for all other
experiments.
5.3
Self-Consistency
Self-Consistency
AI Labeler Alignment
1 sample, T=0
78.0%
4 samples, T=1
72.6%
16 samples, T=1
72.8%
Table 3: Sampling several chain-of-thought rationales
with T = 1 results in lower alignment with human pref-
erences. Note: 1, 4, and 16 samples represent 2, 8, and
32 inferences given our position de-biasing technique
(see Section 3.1.1).
We experiment with self-consistency using 4 and
16 samples with decoding temperature of 1 as de-
scribed in Section 3.1.3, and both settings show