LARGE LANGUAGE MODELS
Sean O’Brien1,2∗
Mike Lewis2
1University of California, San Diego
2Meta AI
seobrien@ucsd.edu, mikelewis@meta.com
ABSTRACT
We demonstrate that Contrastive Decoding – a simple, computationally light, and
training-free text generation method proposed by Li et al 2022 – achieves large
out-of-the-box improvements over greedy decoding on a variety of reasoning
tasks. Originally shown to improve the perceived quality of long-form text gen-
eration, Contrastive Decoding searches for strings that maximize a weighted dif-
ference in likelihood between strong and weak models. We show that Contrastive
Decoding leads LLaMA-65B to outperform LLaMA 2, GPT-3.5 and PaLM 2-L on
the HellaSwag commonsense reasoning benchmark, and to outperform LLaMA
2, GPT-3.5 and PaLM-540B on the GSM8K math word reasoning benchmark,
in addition to improvements on a collection of other tasks. Analysis suggests
that Contrastive Decoding improves over existing methods by preventing some
abstract reasoning errors, as well as by avoiding simpler modes such as copy-
ing sections of the input during chain-of-thought. Overall, Contrastive Decoding
outperforms nucleus sampling for long-form generation and greedy decoding for
reasoning tasks, making it a powerful general purpose method for generating text
from language models.
arXiv:2309.09117v2  [cs.CL]  29 Sep 2023