o0
o1
o2
o3
a0
Δx1, Δω1, Δx2, Δω2
a1
wipe table
a2
<camera> 90°, <zoom> 1.5
…
…
T
UniSim
Figure 2: Training and inference of UniSim. UniSim (T) is a video diffusion model trained to predict the
next (variable length) observation frames (o1) given the noisy version of the previous observation (o0) and
action input (a0). UniSim can handle actions of varying modalities such as motor controls of varying length
(∆x1, ∆ω1, ∆x2, ...), language descriptions of the action (“wipe table”), and actions extracted from camera
motions and other sources. Dotted arrows add noise to the true video during training or to the previously
generated video during inference to autoregressively rollout observations in supporting long-horizon interactions.
2
Learning Interactive Real-World Simulators
The major differences between an interactive real-world simulator and typical video generation
models are that a simulator requires support for (1) a diverse set of actions and (2) long-horizon
rollouts. In this section, we first enable action-rich interaction by combining datasets rich in different
axes through joint training, and then enable long-horizon interaction through history conditioning
inspired by rollouts in a POMDP.
2.1
Orchestrating Datasets Rich in Different Axes
A realistic world simulator should be able to simulate diverse scenes, objects, human activities,
robot actions, camera motions, other aspects of the world. While this seems difficult, there already
exist billions of text, image, and video samples on the internet, as well as various robotic, 3D,
and navigation datasets scattered across institutions. The main difficulty comes down effectively
extracting information from broad datasets rich in these different axes and fusing this information
into a single learned simulator.
Extracting Information from Broad Data. Although data exists across many different modalities
on the Internet, our focus in this paper will be on visual observations of the world and actions that
cause changes to these visual observations. Note that this choice inevitably misses states that are
not visual (e.g., temperature-dependent friction), but we only focus on problems that can be visually
captured. In such a scenario, if we can express the two modalities in terms of a universal interface
that relates videos and text, we can fuse the information between different datasets by training a
simulator that operates through this universal interface. Thus, the key challenge is to extract then fuse
observations and actions from different types of datasets into a common format, which we describe
below. The datasets we included in this study are as follows (further details of the datasets used to
train UniSim are given in Appendix 9).
• Simulated execution and renderings. While annotating actions for real-world videos is expensive,
simulation engines such as Habitat [21] and Language Table [22] are able to render a wide variety of
actions. We use datasets previously collected from these simulators, i.e., Habitat object navigation
using HM3D [23] and Language Table Data from [24] to train UniSim. We extract text descriptions
as actions when available. For simulated continuous control actions, we encode them via language
embeddings and concatenate the text embeddings with discretized control values.
• Real robot data. An increasing amount of video data of real-robot executions paired with task
descriptions such as the Bridge Data [25, 26] and data that enabled RT-1 and RT-2 [27] are becoming
increasingly available. Despite low-level control actions often being different across robots, the task
descriptions can serve as high-level actions in UniSim. We further include discretize continuous
controls actions when available similar to simulated robotics data.
• Human activity videos. Human activity data such as Ego4D [11], EPIC-KITCHENS [28], and
Something-Something [29] have videos filled with human activities. Different from low-level
robot controls, these activities are high-level actions that humans take to interact with the world.
But these actions are sometimes provided as labels for video classification or activity recognition
3