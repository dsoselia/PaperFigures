Who is the CEO of Twitter?
Prompt
The CEO of Twitter at the time 
of writing this answer is Jack 
Dorsey. He co-founded Twitter 
in 2006 ……
LLM Response
Claim
Extraction
Query  
Generation
Claims
(1) Jack Dorsey is the CEO of Twitter
(2) Jack Dorsey co-founded Twitter in 2006
……
Queries
(1.1) Is Jack Dorsey the CEO 
of Twitter?
(1.2) Who is the current CEO 
of Twitter?
(2.1) Did Jack Dorsey co-found 
Twitter in 2006?
……
Evidence
(1.2) Former NBC Universal advertising 
chief Linda Yaccarino will become…
(1.1) Noah Glass, Evan Williams, and Biz 
Stone co-founded Odeo……
……
Claim-level Factuality: [0, 1, …] 
Response-level Factuality: 0
Scores
Return a string containing space-delimited 
numbers starting from 0 up to n inclusive.
Prompt
LLM Response
def string_sequence(n): 
result = ""   
for i in range(n+1):      
result += str(i) + " "     
return result.strip()
Claim
Extraction
Marie ordered 5 packs of milk that costs 
$3 each, and some boxes of pizza. Marie 
paid a total of $45. How many boxes of 
pizza did Marie order if each box costs $3
Prompt
Claims
def string_sequence(n): 
result = ""   
for i in range(n+1):      
result += str(i) + " "     
return result.strip()
Test Cases
(1) string_sequence(4)
(2) string_sequence(0)
(3) string_sequence(3)
Query  
Generation
Exec Results
(1) 0 1 2 3 4 expected: 0 1 2 3 4
(2) 0 expected: 0
(3) 0 1 2 3 expected: 0 1 2 3
Response-level Factuality: 1
Scores
Math Claims
(1) 5*3 = $15
(2) 45 - 15 = $30
(3) 30 / 3 = 10
LLM Response
- Marie ordered 5 packs of milk that costs $3 
each. The total cost of these items is: 5*3 = $15
- To find the cost of the boxes of pizza, we 
subtract the total cost of the other items from 
the total amount paid by Marie: 45 - 15 = $30
- To find the number of boxes of pizza, we 
divide the remaining amount by the cost of 
each box: 30 / 3 = 10
- Therefore, Marie ordered 10 boxes of pizza.
Claim 
Extraction
Query  
Generation
Queries
(1) print(round(5*3, 7) == 15)
(2) print(round(45-15, 7)==30)
(3) print(round(30/3, 7) == 10)
Exec 
Results
(1) True
(2) True
(3) True
Response-level  
Factuality:1
Scores
Prompt
Discuss the applications and limitations 
of quantum computing, citing at least 
one relevant paper. When citing papers, 
please include the title, the author(s), 
and the publication year.
Quantum computing has the potential to 
revolutionize various fields such as 
cryptography, optimization, and simulation. 
However, there are also limitations such as 
the need for error correction. One papers 
that have contributed to this field is 
“Quantum Computing in the NISQ era and 
beyond” by John Preskill (2018).
LLM Response
Claim 
Extraction
Claims
(1) {title: Quantum Computing in the 
NISQ era and beyond, authors: John 
Preskill, publication_year: 2018}
Queries
(1) Quantum Computing in the NISQ 
era and beyond
Evidence
(1) {title: Quantum Computing in the 
NISQ era and beyond, authors: John 
Preskill, publication_year: 2018}
Query  
Generation
Claim-level Factuality: [1] 
Response-level Factuality: 1
Scores
Knowledge-based QA
Code Generation
Math Problem SolvingFacTool
!
Scientific Literature Review Writing
Figure 2: Our proposed framework for factuality detection in four domains: knowledge-based QA, code generation,
math problem solving and scientific literature review writing.
{ci}i=1···n. Detailed prompting instructions can
be found in Appendix A.
KB-based QA
The claim is defined using the
concept of atomic content units (ACUs) (Liu
et al., 2022). Each ACU corresponds to a single
atomic fact within a generated answer. In practice,
we leverage ChatGPT4 (specifically, the “gpt-3.5-
turbo” version) to extract claims based on two cri-
teria: (i) each claim should not exceed 15 words,
and (ii) it should clearly describe a fact. We also
include two in-context examples from the RoSE
dataset (Liu et al., 2022) in our prompt to obtain
more fine-grained claims. Additionally, we ask
ChatGPT to resolve any coreferences or ambiguity,
such as unclear pronouns and other related expres-
sions within the claims.
Code Generation
We consider each generated
code snippet within the response as a single claim
to be verified. We extract all such code snippets
that are enclosed with brackets, in other words,
within a code block.
Math Problems
We define each claim in a step-
by-step math solution as the arithmetic operation
performed between known real numbers. Each of
these operations contains two parts: the calculation
4We have also explored other entailment-based models
with BERT, and the result is no better than ChatGPT.
and the calculated answer. We prompt ChatGPT to
extract all such claims.
Scientific Literature Review
Each claim within
the generated review is defined as a tuple of “(paper
title, year, authors)” contained from the generated
review. We then prompt ChatGPT to extract all
such tuples within the generated review.
4.2
Query Generation
For each claim ci, we convert it into a list of queries
{qij}j=1···m that can be used to query external tools
such as search engines, the Python interpreter, or
Google scholar.
KB-based QA
We prompt ChatGPT or GPT-4 to
generate two search engine queries from each claim
ci. These queries are intended to help humans in
verifying the factuality of ci. Detailed prompting
instructions can be found in Appendix A.
Code Generation
For each claim ci we gener-
ate two different types of queries: simulated test
case inputs, denoted as {qtij}j=1···m, and poten-
tial solutions, denoted as {qsij}j=1···m. Both types
of queries are generated by ChatGPT or GPT-4.
The simulated test case inputs are function calls
generated for a given code snippet, while potential
solutions are repeatedly generated solutions that
ChatGPT generates in response to the user prompt