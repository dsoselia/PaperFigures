Figure 4: Instances of object hallucination within LVLMs (Li et al., 2023e). Ground-truth objects in annotations
are indicated in bold, while red objects represent hallucinated objects by LVLMs. The left case occurs in the
conventional instruction-based evaluation approach, while the right cases occur in three variations of POPE.
challenges, we introduce the Hallucinator, which
efficiently generates additional positive samples
to enhance contrast. The Hallucinator is differ-
entiable, operating in the feature space, making
it amenable to direct optimization within the pre-
training task and incurring minimal computational
overhead.
Efforts to enhance LVLMs for complex multi-
modal tasks, inspired by LLMs, face a significant
challenge: object hallucination, where LVLMs gen-
erate inconsistent objects in descriptions. This
study (Li et al., 2023e) systematically investigates
object hallucination in LVLMs and finds itâ€™s a
common issue. Visual instructions, especially fre-
quently occurring or co-occurring objects, influ-
ence this problem. Existing evaluation methods
are also affected by input instructions and LVLM
generation styles. To address this, the study intro-
duces an improved evaluation method called POPE,
providing a more stable and flexible assessment of
object hallucination in LVLMs.
Instruction-tuned Large Vision Language Mod-
els (LVLMs) have made significant progress in han-
dling various multimodal tasks, including Visual
Question Answering (VQA). However, generating
detailed and visually accurate responses remains
a challenge for these models. Even state-of-the-
art LVLMs like InstructBLIP exhibit a high rate
of hallucinatory text, comprising 30 percent of
non-existent objects, inaccurate descriptions, and
erroneous relationships. To tackle this issue, the
study (Gunjal et al., 2023)introduces MHalDetect1,
a Multimodal Hallucination Detection Dataset de-
signed for training and evaluating models aimed
at detecting and preventing hallucinations.
M-
HalDetect contains 16,000 finely detailed anno-
tations on VQA examples, making it the first com-
prehensive dataset for detecting hallucinations in
detailed image descriptions.
4
Hallucination in Large Video Models
Hallucinations can occur when the model makes in-
correct or imaginative assumptions about the video
frames, leading to the creation of artificial or erro-
neous visual information Fig. 5.
Figure 5: A video featuring three captions generated by
various captioning models (Liu and Wan, 2023), with
factual errors highlighted in red italics.
The challenge of understanding scene affor-
dances is tackled by introducing a method for
inserting people into scenes in a lifelike manner
(Kulal et al., 2023). Using an image of a scene
with a marked area and an image of a person, the
model seamlessly integrates the person into the