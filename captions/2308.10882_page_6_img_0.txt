Figure 2: Comparison of the standard RoPE basis vs the power basis and the truncated basis. The x-axis
spans over the embedding dimension, and the y-axis is the frequency value of the sine-cosine basis.
5
Results & Discussion
In the following experiments, we finetuned a base LLaMA-13B model on a portion of the RedPajama dataset
[5] which has been modified so that each data sample has a size of exactly 4096 tokens. We trained with
each positional encoding approach until the evaluation loss roughly plateaued. Loss curves can be found in
Appendix B.
We then further applied instruction finetuning (IFT) with the Vicuna dataset [23] and using LoRA [24]
on the base model. However, we discovered that although IFT did boost accuracies on LongChat-Lines, it
did not significantly change the range of contexts that the base model was able to deal with (see Figure 5 in
Appendix C). This we found to be a marked contrast with the WikiQA variants; there, IFT was necessary
for the model to produce any meaningful results at all. Hence for LongChat-Lines, we used non-IFT models;
for WikiQA, we performed evaluation on a subset of the more promising models with additional IFT.
5.1
Finetuned Context Length Extrapolation
LongChat-Lines
We conducted evaluations on LongChat-Lines with the techniques described in Section
4 and report the results in Table 1. We expected all models to be able to perform with a non-zero accuracy