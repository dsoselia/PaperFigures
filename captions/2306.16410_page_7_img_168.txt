What is the color of this text?
What about this one?
What does it say?
The text says "red".
 The text color is purple.
 The text color is red.
 The text says "green"
What does it say?
What is this?
A light fixture that has a plant inside of it.
Are the plants inside of the lightbulb?
Is it glowing?
Yes.
No.
What is this?
An incandescent light bulb surronded
by green leaves 
Are the leaves inside of the light bulb?
Is it glowing?
No, the leaves are outside the lightbulb
Yes, the lightbulb is glowing
Tell me something about the history 
of this place.
The Great Wall of China is a fortification 
built by the ancient Chinese to keep 
out invaders.
something else?
The Great Wall of China is a UNESCO 
World Heritage Site.
Write down the facts that you know
 about this flower.
Phalaenopsis sanderiana is a species of orchid. It is
also known as the moth orchid, butterfly orchid, or
moth plant. It is a perennial plant with a waxy or 
velvety texture. It may have a single large bloom 
or several smaller blooms.
Where does it come from?
Phalaenopsis sanderiana is native to the
Philippines.
Is this photo unsual?
Why?
Yes
The house is upside down.
The house has a tunnel coming out of it.
How could you get out of the house?
Figure 4: Selected examples of LENS using Tag and Attributes Modules with OpenCLIP-H/14 as
the vision encoder, Intensive Captioning Module and Flan-T5xxl as the LLM.
4.4
Ablations on LENS components
Prompt Template
Acc. (Avg.)
Objects
76.6
Attributes
74.7
Objects + Attributes
77.0
Table 3: Ablations on vision datasets. We report
average accuracy on the vision datasets discussed
in Section 4.1. The object information helps more
than the attributes but together they are complimen-
tary and lead to overall better performance.
Object recognition: In Table 3, we conducted
an ablation study of LENSâ€™s components on
object recognition using Flan-T5XL and CLIP-
H/14, as discussed in Section 4.3. We present
the average accuracy across the benchmarks. By
utilizing only the tag module, we exclusively
rely on CLIP and observe similar performance
as CLIP-H/14 in Table 1. However, we noticed
a drop in performance when using only the at-
tributes module. When combined with tags, at-
tributes significantly contribute to enhancing the performance of LENS by +0.4%. This demonstrates
that LENS serves as a robust baseline compared to using only a single vision module such as CLIP.
For a detailed overview on each dataset, please refer to Table 7 in supplementary material.
Visual Reasoning: For the VQA 2.0 dataset (Goyal et al., 2017), we conducted ablations using our
model name, which is equipped with Flan-T5XXL, on the minival split. As shown in Table 5, we
noticed that increasing the number of captions generated by our Intensive Captioning module led to a
gradual improvement in performance. However, it eventually reaches a saturation point, indicating
that the module provides valuable information about the image only up to a certain threshold.
We also conducted ablation studies on the LENS components using the dev set of the Hateful-Memes
benchmark [25]. Table 4 demonstrates that a combination of the global captioning, tags, and attributes
modules is essential for achieving high performance on this task. Specifically, we observed that
both tags and attributes contribute more to the performance improvement compared to the global
captioning module when combined with OCR. However, it is important to note that all of these
8