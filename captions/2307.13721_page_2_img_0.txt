3
Fig. 2. Overview of four different architecture styles described in this survey. Left to right: a) Dual-encoder designs use a parallel visual and language
encoder with aligned representations. b) Fusion designs jointly process both image and text representations via a decoder. Here the image encoder
can also process visual prompts (e.g., points and boxes in the case of SAM [140]). c) Encoder-decoder designs apply joint feature encoding and
decoding sequentially. d) Adapter LLM designs input visual and text prompts to the LLMs to leverage their superior generalization ability. Examples
for each category are shown in the bottom row. More detail about these architectures is discussed in Sec. 2.2.
heterogeneous modalities-based models (e.g., ImageBind
[92], Valley [186]). We present the background theory behind
foundation models, briefly covering from architectures to
prompt engineering (Sec. 2). Our work provides an exten-
sive and up-to-date overview of the recent vision foundation
models (Sec. 3, 5, 6, and 7). Finally, we present a detailed dis-
cussion on open challenges and potential research directions
of foundation models in computer vision (Sec. 8).
2
PRELIMINARIES
We first define the foundational models and scope of this
survey. Then, we present a concise background overview to
help readers understand the rest of the material. We focus
on three main contributing factors for Foundational Mod-
els in computer vision: a) Model architecture, b) Training
objectives, and c) Large-scale training and prompting.
2.1
Foundational Models and Scope of the Survey
The term “foundational models” was first introduced by
Bommasani et al. [18] at Stanford Institute for Human-
Centered AI. Foundational models are defined as “the
base models trained on large-scale data in a self-supervised or
semi-supervised manner that can be adapted for several other
downstream tasks”. The paradigm shift towards foundational
models is significant because it allows replacing several
narrow task-specific models with broader and generic base
models that can be once trained and quickly adapted for
multiple applications. It not only enables rapid model de-
velopment and provides better performance for both in-
domain and out-domain scenarios, but also leads to the so-
called “emergent properties” of intelligence from large-scale
generative models aimed to model data distribution such
as GANs, VAEs, and Diffusion models owing to dedicated
surveys already existing in this area [24, 330, 308, 56] and
because the former model class can cover a broader range
of downstream applications.
2.2
Architecture Types
As depicted in Fig. 2, Vision-Language (VL) models primar-
ily use four architectural designs. We start by introducing
the Dual-Encoder architecture, wherein separate encoders
are utilized for processing visual and textual modalities. The
output of these encoders is subsequently optimized through
an objective function. The second architecture type, fusion,
incorporates an additional fusion encoder, which takes the
representations generated by the vision and text encoders
and learns fused representations. The third type, Encoder-
Decoder, consists of an encoder-decoder-based language
model and a visual encoder. Lastly, the fourth architecture
type, Adapted LLM, leverages a Large Language Model
(LLM) as its core component, with a visual encoder em-
ployed to convert images into a format compatible with
the LLM. For a more comprehensive understanding of these
architectures, we refer the readers to the corresponding sec-
tions of the survey where each work is discussed. Next, we
discuss the loss functions used to train different architecture
types.
2.3
Training Objectives
2.3.1
Contrastive Objectives
To learn from unlabeled image-text data
[215 129] utilized