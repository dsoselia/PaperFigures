Segment Anything
Alexander Kirillov1,2,4
Eric Mintun2
Nikhila Ravi1,2
Hanzi Mao2
Chloe Rolland3
Laura Gustafson3
Tete Xiao3
Spencer Whitehead
Alexander C. Berg
Wan-Yen Lo
Piotr Doll´ar4
Ross Girshick4
1project lead
2joint ﬁrst author
3equal contribution
4directional lead
Meta AI Research, FAIR
(b) Model: Segment Anything Model (SAM)
prompt
image
valid mask
image 
encoder
prompt 
encoder
lightweight mask decoder
(a) Task: promptable segmentation
segmentation prompt
image
model
cat with
black ears
valid mask
(c) Data: data engine (top) & dataset (bottom)
• 1+ billion masks
• 11 million images 
• privacy respecting
• licensed images
annotate
train
data
model
Segment Anything 1B (SA-1B):
Figure 1: We aim to build a foundation model for segmentation by introducing three interconnected components: a prompt-
able segmentation task, a segmentation model (SAM) that powers data annotation and enables zero-shot transfer to a range
of tasks via prompt engineering, and a data engine for collecting SA-1B, our dataset of over 1 billion masks.
Abstract
We introduce the Segment Anything (SA) project: a new
task, model, and dataset for image segmentation. Using our
efﬁcient model in a data collection loop, we built the largest
d
d
(b
f
)
h
1 billi
matching in some cases) ﬁne-tuned models [10, 21]. Empir-
ical trends show this behavior improving with model scale,
dataset size, and total training compute [56, 10, 21, 51].
Foundation models have also been explored in computer
vision albeit to a lesser extent
Perhaps the most promi-
43v1  [cs.CV]  5 Apr 2023