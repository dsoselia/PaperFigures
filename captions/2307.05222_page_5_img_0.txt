1:15
1:20
the female puts the 
egg in a shallow pit 
the male bird
1:25
1:30
1:35
1:21
covers it with 
leaves for 
protection
the male even 
safeguards the chicks till
they attained maturity
1:19
the female puts 
the egg in a 
shallow pit the 
male bird
covers it with 
leaves for 
protection
incubation of eggs is the job of 
the male bird in nature.
the male even  safeguards the 
chicks till they attained maturity
Interleaved Video-text Data
Video Storyboard Images with Subtitles 
Sort by timestamp
1:26
incubation of eggs 
is the job of the 
male bird in nature
1:40
1:38
this is an emu egg 
which weighs 600 to
800 grams
this is an emu 
egg which 
weighs 600 to
800 grams
Figure 3: Interleaved video-text data. The combination of storyboard thumbnails and subtitles
captions creates a natural interleaved sequence of video and text that is ordered by the timestamps.
combination of storyboard thumbnails and subtitles creates a natural interleaved sequence of video
and text ordered by timestamps. An example is provided in Figure 3.
More details about the pretraining datasets are deferred to Appendix A.1.1.
3.2
Pretraining
We initialize Emu’s Visual Encoder with the 1B version of EVA-02-CLIP [55], and Multimodal
Modeling LLM with the 13B version of LLaMA [57]. LLaMA is a decoder-only Transformer [58]
and EVA-02-CLIP is a 40-layer ViT [17]. The Causal Transformer comprises 12 blocks, each of
which consists of a causal self-attention layer, a cross-attention layer, and a feed-forward layer.
Random initialization is used for Causal Transformer. The total number of parameters of Emu is 14B
and is trained end-to-end.
We use a batch size of 128 for image-text pair data, 64 for interleaved image-text data, 16 for
video-text pair and interleaved video-text data. We adopt the AdamW optimizer [41] with β1 = 0.9,
β2 = 0.98, and a weight decay of 0.05. We use a cosine learning rate decay with a peak learning rate
of 1e-4 for the Causal Transformer, 3e-5 for LLaMA [57] and 5e-5 for EVA-02-CLIP [55], and a
linear warmup of 2k steps. For each video, we randomly sample 8 frames for pretraining, and all
images/frames are resized into 224×224 resolution. For image-text pair and interleaved data, we
randomly put each image before or after its corresponding sentence. We train the model on 128
NVIDIA 80G-A100 GPUs for 10k steps with around 82M samples (150B tokens in total), and the
pretraining takes approximately 2 days.
3.3
Visual Decoding