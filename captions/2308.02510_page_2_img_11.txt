EEG Signals
Visual Stimuli
Reconstructed Image
Multi-level Information
CLIP Embedding 
Decoding From 
EEG Signals
Pixel Level 
Sample Level 
Fine-grained Control
Latent Diffusion Model
t = 0
Diffusion Process
Denoising Process
Coarse-grained Control
“An image of airliner”
CLIP
Caption
CLIP Embedding
Supervision
Supervision
p
T
p
M
s
M
ldm
E
ldm
D
Figure 1: Overview of our NEUROIMAGEN. All the modules with dotted lines, i.e. pixel-level supervision and sample-level
supervision, are only used during the training phase. and would be removed during the inference phase.
GEN, including pixel-level semantics and sample-level se-
mantics with the corresponding training details of the decod-
ing procedure. Finally, we detail the image reconstruction
procedure of NEUROIMAGEN, which integrates the coarse-
grained and fine-grained semantics with a pretrained latent
diffusion model to reconstruct the observed visual stimuli
from EEG signals.
Problem Statement
In this section, we formulate the problem and give
an
overview
of
NEUROIMAGEN.
Let
the
paired
{(EEG, image)} dataset as Ω
=
{(xi, yi)}n
i=1, where
yi ∈ RH×W ×3 is the visual stimuli image to evoke the
brain activities and xi ∈ RC×T represents the recorded
corresponding EEG signals. Here, C is the channel number
of EEG sensors and T is the temporal length of a sequence
associated with the observed image. The general objective
of this research is to reconstruct an image y using the
corresponding EEG signals x, with a focus on achieving a
high degree of similarity to the observed visual stimuli.
Multi-level Semantics Extraction Framework
Figure 1 illustrates the architecture of NEUROIMAGEN.
In our approach, we extract multi-level semantics, repre-
sented as {M1(x), M2(x), · · · , Mn(x)}, which capture var-
ious granularity ranges from coarse-grained to fine-grained
information from EEG signals corresponding to visual stim-
uli. The coarse-grained semantics serves as a high-level
overview, facilitating a quick understanding of primary at-
tributes and categories of the visual stimuli On the other
is defined as the saliency map of silhouette information
Mp(x) ∈ RHp×Wp×3. This step enables us to analyze the
EEG signals in the pixel space and provide the rough struc-
ture information. Subsequently, we define the sample-level
semantics as Ms(x) ∈ RL×Ds, to provide coarse-grained
information such as image category or text caption.
To fully utilize the two-level semantics, the high-quality
image reconstructing module F is a latent diffusion model.
It begins with the saliency map Mp(x) as the initial
image and utilizes the sample-level semantics Ms(x) to
polish the saliency map and finally reconstruct ˆy
=
F(Mp(x), Ms(x)).
Pixel-level Semantics Extraction
In this section, we describe how we decode the pixel-level
semantics, i.e. the saliency map of silhouette information.
The intuition of this pixel-level semantics extraction is to
capture the color, position, and shape information of the ob-
served visual stimuli, which is fine-grained and extremely
difficult to reconstruct from the noisy EEG signal. However,
as is shown in Figure 3, despite the low image resolution and
limited semantic accuracy, such a saliency map successfully
captures the rough structure information of visual stimuli
from the noisy EEG signals. Specifically, our pixel-level se-
mantics extractor Mp consists of two components: (1) con-
trastive feature learning to obtain discriminative features of
EEG signals and (2) the estimation of the saliency map of
silhouette information based on the learned EEG features.
Contrastive Feature Learning
We use contrastive learn-
ing techniques to bring together the embeddings of EEG