Segment Everything Everywhere All at Once
Xueyan Zou∗§2, Jianwei Yang∗‡1, Hao Zhang∗♯, Feng Li∗♯, Linjie Li†, Jianfeng Wang†
Lijuan Wang†, Jianfeng Gao¶‡, Yong Jae Lee¶§
§ University of Wisconsin-Madison
‡ Microsoft Research, Redmond
♯ HKUST
† Microsoft Cloud & AI
∗Equal Contribution ¶ Equal Advisory Contribution 1. Project Lead 2. Main Technical Contribution
{xueyan,yongjaelee}@cs.wisc.edu
{jianwyan,jfgao,linjli}@microsoft.com
{hzhangcx,fliay}@connect.ust.hk
Figure 1: SEEM supports generic segmentation tasks—including semantic, instance, and panoptic
segmentation—in an open-set fashion when no prompt is provided. SEEM also enables the use of
visual, textual, and referring region prompts in flexbile combinations, making it a promptable and
interactive segmentation interface.
Abstract
In this work, we present SEEM, a promptable and interactive model for segmenting
everything everywhere all at once in an image, as shown in Fig. 1. In SEEM,
we propose a novel decoding mechanism that enables diverse prompting for all
types of segmentation tasks, aiming at a universal segmentation interface that
behaves like large language models (LLMs). More specifically, SEEM is designed
with four desiderata: i) Versatility. We introduce a new visual prompt to unify
different spatial queries including points, boxes, scribbles and masks, which can
further generalize to a different referring image; ii) Compositionality. We learn
a joint visual-semantic space between text and visual prompts, which facilitates
the dynamic composition of two prompt types required for various segmentation
tasks; iii) Interactivity. We further incorporate learnable memory prompts into the
decoder to retain segmentation history through mask-guided cross-attention from
decoder to image features; and iv) Semantic-awareness. We use a text encoder
to encode text queries and mask labels into the same semantic space for open-
vocabulary segmentation. We conduct a comprehensive empirical study to validate
the effectiveness of SEEM across diverse segmentation tasks. Notably, our single
SEEM model achieves competitive performance across interactive segmentation,
generic segmentation, referring segmentation, and video object segmentation on
9 datasets with minimum 1/100 supervision. Furthermore, SEEM showcases a
remarkable capacity for generalization to novel prompts or their combinations,
rendering it a readily universal image segmentation interface.
1
Introduction
Image segmentation is arguably the most important yet challenging problem in computer vision. In
the past, we have witnessed significant progress in a wide range of segmentation tasks including
Preprint. Under review.
arXiv:2304.06718v4  [cs.CV]  11 Jul 2023