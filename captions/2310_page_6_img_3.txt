Tags: bed, 
chair, tv, table
Image
Captioner
Caption: A bedroom with a 
bed, a chair, a tv, and a table
Large
Language
Model
Image Segmentation Model
A bedroom with a bed         
, a chair         
, a tv       
, and a table
Figure 4: Overview of our data construction pipeline for compositional generation instruction tuning.
caption <image> embedding of the original image </image> edit instruction </s>‚Äù. We also mix
some text-to-image data to preserve the language alignment already achieved.
Our goal is to leverage MLLMs to model image distributions through direct latent space sampling.
In this setup, the pre-trained frozen Stable Diffusion U-Net serves as a score metric, distilling the
learned data distribution. This strategy is similar to Score Distillation Sampling [PJBM22]. From
the perspective of score distillation, the KL divergence between KOSMOS-G and the score function
is equivalently minimized for distilling learned probability density in the image decoder. This
enables KOSMOS-G to leverage rich features from the image encoder to generate an image faithfully
reproducing the contents across various contexts.
3
Model Training
3.1
Multimodal Training Data
The multimodal language modeling stage in Section 2.1 using the same setting of KOSMOS-
1 [HDW+23], where the models are trained on web-scale multimodal corpora, consisted of text
corpora, image-caption pairs, and interleaved data of images and texts. For the image decoder
aligning stage in Section 2.2, we only use the caption from image-caption pairs. For the instruction
tuning stage in Section 2.3, we use constructed data from Open Images V7 dataset [KRA+20], the
image-caption pairs, as well as the image editing data from InstructPix2Pix [BHE23].
Captions
The image-caption pairs are sourced from multiple datasets, including English
LAION-2B [SBV+22], LAION-400M [SVB+21], COYO-700M [BPK+22], and Conceptual Cap-
tions [SDGS18, CSDS21]. English LAION-2B, LAION-400M, and COYO-700M are collected from
Common Crawl web data by extracting images and the corresponding alt-texts. Conceptual captions
are also derived from web pages.
Constructed Data
We use approximately 9M images from the Open Images V7 dataset [KRA+20]
to construct our compositional generation instruction tuning data. As illustrated in Figure 4, we
begin by generating captions with BLIP-2-OPT-6.7b [LLSH23]. Subsequently, we employ an LLM
MPT-7B-Instruct [Tea23] to extract entities from the captions. The original image, along with the text
of each entity, is then input into the text-prompted segmentation model CLIPSeg [LE22] to derive the
corresponding image of each entity.