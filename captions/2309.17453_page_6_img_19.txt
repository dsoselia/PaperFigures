Preprint
Dense Attention
Window Attention
StreamingLLM
Figure 5: Language modeling perplexity of StreamingLLM for super long texts with 4 million tokens
across various LLM families and model scales. The perplexity remains stable throughout. We use the
concatenated test set of PG19 (100 books) to perform language modeling, with perplexity fluctuations
attributable to the transition between books.
4.2
RESULTS OF PRE-TRAINING WITH A SINK TOKEN
To validate our suggestion that introducing a sink token to all pre-training samples improves stream-
ing LLMs, we trained two language models, each with 160 million parameters, under identical
conditions. While one model adhered to the original training settings, the other incorporated a sink
token at the start of every training sample. Our experiments employed the Pythia-160M (Bider-
man et al., 2023) codebase and followed its training recipe. We train the models on an 8xA6000
NVIDIA GPU server using the deduplicated Pile (Gao et al., 2020) dataset. Apart from reducing
the training batch size to 256, we retained all Pythia training configurations, including learning rate
schedules, model initialization, and dataset permutations. Both models were trained for 143,000 steps.
Figure 6:
Pre-training loss
curves of models w/ and w/o sink
tokens. Two models have a simi-
lar convergence trend.
Table 4: Zero-shot accuracy (in %) across 7 NLP benchmarks,
including ARC-[Challenge, Easy], HellaSwag, LAMBADA, Open-
bookQA, PIQA, and Winogrande. The inclusion of a sink token
during pre-training doesn’t harm the model performance.
Methods
ARC-c ARC-e
HS
LBD OBQA PIQA WG
Vanilla
18.6
45.2
29.4 39.6
16.0
62.2
50.1
+Sink Token
19.6
45.6
29.8 39.9
16.6
62.6
50.8
Convergence and Normal Model Performance.
Including a sink token during pre-training has no
negative impact on model convergence and subsequent performance on a range of NLP benchmarks.
As depicted in Figure 6, models trained with a sink token exhibit similar convergence dynamics
compared to their vanilla counterparts. We evaluate the two models on seven diverse NLP bench-
marks, including ARC-[Challenge, Easy] (Clark et al., 2018), HellaSwag (Zellers et al., 2019),
LAMBADA (Paperno et al., 2016), OpenbookQA (Mihaylov et al., 2018), PIQA (Bisk et al., 2020),
and Winogrande (Sakaguchi et al., 2019). As shown in Table 4, the model pre-trained with a sink
token performs similarly to that trained using the vanilla approach.
Streaming Performance.
As illustrated in Table 3, the streaming perplexities differ between
models trained using traditional methods and those augmented with a sink token. Remarkably,
the vanilla model requires the addition of multiple tokens as attention sinks to maintain stable
streaming perplexity. In contrast, the model trained with a sink token achieves satisfactory streaming
performance using just the sink token.
Below is a record of lines I want you to remember. 
The REGISTER_CONTENT in line 0 is <8806> 
[omitting 9 lines…] 
The REGISTER_CONTENT in line 10 is <24879> 
[omitting 8 lines…] 
The REGISTER_CONTENT in line 20 is <45603> 
Query: The REGISTER_CONTENT in line 0 is 
The REGISTER_CONTENT in line 21 is <29189> 
[omitting 8 lines…] 
The REGISTER_CONTENT in line 30 is <1668> 
Query: The REGISTER_CONTENT in line 10 is 
The REGISTER_CONTENT in line 31 is <42569> 
[omitting 8 lines…] 
The REGISTER_CONTENT in line 40 is <34579> 
Query: The REGISTER_CONTENT in line 20 is 
[omitting remaining 5467 lines…]
Input Content
Desired Output
[“<8806>”, “<24879>”, “<45603>”, …]
Figure 8: The first sample in StreamEval.
Attention Visualization.
Figure 7 contrasts atten-
tion maps for models pre-trained with and without a
sink token. The model without the sink token, similar
to Llama-2-7B (Figure 2), shows early-layer local
attention and deeper-layer focus on initial tokens.
In contrast, models trained with a sink token con-
sistently concentrate on the sink across layers and
heads, indicating an effective attention offloading
mechanism. This strong focus on the sink, with re-
duced attention to other initial tokens, explains the
sink token’s efficacy in enhancing model’s streaming
performance.
7