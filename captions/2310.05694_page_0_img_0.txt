concerns associated with deploying LLMs in Healthcare settings
are investigated, particularly regarding fairness, accountability,
transparency and ethics. Our survey provide a comprehensive
investigation from perspectives of both computer science and
Healthcare specialty. Besides the discussion about Healthcare
concerns, we supports the computer science community by
compiling a collection of open source resources, such as accessible
datasets, the latest methodologies, code implementations, and
evaluation benchmarks in the Github1. Summarily, we contend
that a significant paradigm shift is underway, transitioning
from PLMs to LLMs. This shift encompasses a move from
discriminative AI approaches to generative AI approaches, as
well as a shift from model-centered methodologies to data-
centered methodologies.
Index Terms—Large Language Model, Medicine, Healthcare
Application
I. INTRODUCTION
Pretrained Language Models (PLMs) were primarily em-
ployed as a constituent part of Natural language Processing
(NLP) systems [1]–[4], such as those used in Speech Recogni-
tion [5], [6], Metaphor Processing [7], Sentiment Analysis [8],
[9], Information Extraction [10]–[12], and Machine Trans-
lation [13], [14]. However, with recent advancements, these
PLMs are demonstrating an increasing capacity to function
* Corresponding author: Mengling Feng.
Kai He, Qika Lin, Yucheng Ruan, Xiang Lan and Mengling Feng are
with the School of Saw Swee Hock School of Public Health and Institute
of Data Science, National University of Singapore. E-mail: {kai he, linqika,
yuchengruan, ephlanx, ephfm}@nus.edu.sg.
Rui Mao and Erik Cambria are with the School of Computer Science and
Engineering, Nanyang Technological University, Singapore, 639798. E-mail:
{rui.mao, cambria}@ntu.edu.sg.
Manuscript received on September 30, 2023.
1https://github.com/KaiHe-better/LLM-for-Healthcare
Fig. 1. The development from PLMs to LLMs. GPT-3 [17] marks a significant
milestone in the transition from PLMs to LLMs, signaling the beginning of
a new era.
as independent systems in their own right. Recently, OpenAI
launched their Large Language Models (LLMs) ChatGPT and
GPT-4, which shows superior performance in various NLP-
related tasks, as well as scientific knowledge, such as Biology,
Chemistry, and Medical exams [15]. Med-PaLM 2 [16] is
Google’s LLMs, which are tailored to the medical domain. It is
the first LLM that can achieve an “expert” level of performance
on the MedQA dataset of US Medical Licensing Examination
(USMLE2)-style questions, with an accuracy of over 85%.
A notable symbol of these advancements is the exponential
growth in the sizes of PLMs. In the past five years, model sizes
have increased by an astonishing 5,000 times, as depicted in
Figure 1. Despite sharing many technical components, it is
remarkable that simply scaling up these models leads to the
emergence of novel behaviors, enabling qualitatively distinct
capabilities [18]. In this context, the study [19] is relevant as
it proposes a power-law positive relationship between model
performance and three crucial factors: model size, dataset size,
and amount of compute. We are currently witnessing a transi-
tion period where all three of these factors are skyrocketing,
marking the evolution from PLMs to LLMs and opening up
2The United States Medical Licensing Examination (USMLE), a three-step
examination program used to assess clinical competency and grant licensure
in the United States. This is a main dataset for the evaluation of Healthcare
LLMs.
arXiv:2310.05694v1  [c