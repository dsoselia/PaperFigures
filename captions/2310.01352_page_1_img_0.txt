In this work, we show lightweight instruction tuning (Chung et al., 2022b; Iyer et al., 2022; Zhou
et al., 2023) alone can significantly boost the performance of RALMs, especially in scenarios
that require access to large, external knowledge sources. We propose Retrieval-Augmented Dual
Instruction Tuning (RA-DIT), an approach that retrofits any LLM with retrieval capabilities via
fine-tuning over a set of tasks selected to cultivate knowledge utilization and contextual awareness
in the language model predictions. We initialize the framework using pre-trained LLAMA (Tou-
vron et al., 2023a) and a state-of-the-art dual-encoder based dense retriever, DRAGON+ (Lin et al.,
2023). Following Shi et al. (2023b), we retrieve relevant text chunks based on the language model
prompt. Each retrieved chunk is prepended to the prompt, and the predictions from multiple chunks
are computed in parallel and ensembled to produce the final output.
We perform instruction-tuning in two separate steps. For language model fine-tuning (LM-ft), we
adopt the supervised fine-tuning objective (Chung et al., 2022b; Iyer et al., 2022) while augmenting
each fine-tuning prompt with a retrieved “background” field prepended to the instructions (Figure 1).
We also leverage the design of existing NLP tasks and populate this field with the ground truth con-
text for tasks such as reading comprehension and summarization. By incorporating the background
text during fine-tuning, we guide the LLM to optimally utilize the retrieved information and ignore
distracting content (Shi et al., 2023a). For retriever fine-tuning (R-ft), we update the query encoder
using a generalized LM-Supervised Retrieval (LSR, Shi et al., 2023b) training objective computed
over a combination of supervised tasks and unsupervised text completion. This way we enable the
retriever to yield more contextually relevant results, aligned with the preferences of the LLM.
We demonstrate that each fine-tuning step offers significant performance gains, and that the fine-
tuned LLM and retriever can be combined to achieve further improvements. Our largest model,
RA-DIT 65B, attains state-of-the-art performance in zero- and few-shot settings on knowledge
intensive benchmarks, notably surpassing the un-tuned in-context RALM approach on datasets
including MMLU (Hendrycks et al., 2021b) (+8.2% 0-shot; +0.7% 5-shot) and Natural Ques-
tions (Kwiatkowski et al., 2019) (+22% 0-shot; +3.8% 5-shot). In addition, RA-DIT 65B also
substantially outperforms ATLAS 11B on 8 knowledge-intensive tasks (+7.2% on average in the
64-shot fine-tuning setting). This suggests that language models and retrievers, when optimized in-
dependently and then fused through instruction-tuning, can compete effectively with RALMs that
have undergone extensive continuous pre-training. We further conduct a comprehensive model anal-
ysis, showing the effectiveness of our approach across LLMs of varying sizes, as well as evaluating
the influence of different fine-tuning strategies and retriever configurations.
2
METHOD
2.1
ARCHITECTURE
Language Model
We focus on retrieval-augmenting pre-trained auto-regressive language mod-
els (Brown et al., 2020). In particular, we use LLAMA (Touvron et al., 2023a), a family of open-
sourced language models pre-trained on trillions of tokens.
2