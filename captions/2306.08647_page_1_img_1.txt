User
The robot dog's torso is upright, 
balanced over its hind feet, which 
are ﬂat and shoulder-width apart. 
The front legs hang loosely, poised 
mid-air, mimicking a human's 
relaxed arms.
# Set torso rewards
set_torso_rewards(height=0.7, pitch=np.deg2rad(90))
# Set feet rewards
set_feet_pos_rewards('front_left', height=0.7)
set_feet_pos_rewards('back_left', height=0.0)
set_feet_pos_rewards('front_right', height=0.7)
set_feet_pos_rewards('back_right', height=0.0)
Make robot dog 
stand up on two feet.
LLM
User
Make robot dog 
stand up on two feet.
Reward Translator
(LLM)
Motion Controller
set_joint_target(0.0, 0.2, 0.7, 
0.0, -0.3, 0.8, 0.0, 0.2, 0.7, 
0.0, -0.3, 0.8)
User
Make robot dog 
stand up on two feet.
LLM
Low-level action
Motion description
Reward code
Optimized 
low-level actions
Figure 1: LLMs have some internal knowledge about robot motions, but cannot directly translate them into actions
(left). Low-level action code can be executed on robots, but LLMs know little about them (mid). We attempt to bridge
this gap, by proposing a system (right) consisting of the Reward Translatorthat interprets the user input and transform
it into a reward specification. The reward specification is then consumed by a Motion Controller that interactively
synthesizes a robot motion which optimizes the given reward.
bridges the gap between language and low-level robot actions. This is motivated by the fact that language
instructions from humans often tend to describe behavioral outcomes instead of low-level behavioral details
(e.g. “robot standing up” versus “applying 15 Nm to hip motor”), and therefore we posit that it would be
easier to connect instructions to rewards than low-level actions given the richness of semantics in rewards.
In addition, reward terms are usually modular and compositional, which enables concise representations
of complex behaviors, goals, and constraints. This modularity further creates an opportunity for the user
to interactively steer the robot behavior. However, in many previous works in reinforcement learning (RL)
or model predictive control (MPC), manual reward design requires extensive domain expertise [17, 18, 19].
While reward design can be automated, these techniques are sample-inefficient and still requires manual
specification of an objective indicator function for each task [20]. This points to a missing link between
the reward structures and task specification which is often in natural language. As such, we propose to
utilize LLMs to automatically generate rewards, and leverage online optimization techniques to solve them.
Concretely, we explore the code-writing capabilities of LLMs to translate task semantics to reward functions,
and use MuJoCo MPC, a real-time optimization tool to synthesize robot behavior in real-time [21]. Thus
reward functions generated by LLMs can enable non-technical users to generate and steer novel and intricate
robot behaviors without the need for vast amounts of data nor the expertise to engineer low-level primitives.
The idea of grounding language to reward has been explored by prior work for extracting user preferences
and task knowledge [22, 23, 24, 25, 26, 27]. Despite promising results, they usually require training
data to learn the mapping between language to reward. With our proposed method, we enable a data
efficient interactive system where the human engages in a dialogue with the LLM to guide the generation
f
d
d
tl
b t b h
i
(Fi
1)