Figure 2: A diagram depicting RLAIF (top) vs. RLHF (bottom)
the RM to provide rewards.
Our results show that RLAIF achieves compara-
ble performance to RLHF, measured in two ways.
First, we observe that both RLAIF and RLHF poli-
cies are preferred by humans over a supervised
fine-tuned (SFT) baseline 71% and 73% of the
time, respectively, and the two win rates are not
statistically significantly different. Second, when
asked to directly compare generations from RLAIF
vs. RLHF, humans prefer both at equal rates (i.e.
50% win rate). These results suggest that RLAIF is
a viable alternative to RLHF that does not depend
on human annotation and offers appealing scaling
properties.
Additionally, we study techniques to maximize
the alignment of AI-generated preferences with hu-
man preferences. We find that prompting our LLM
with detailed instructions and soliciting chain-of-
thought reasoning improve alignment. Surprisingly,
we observe that both few-shot in-context learning
and self-consistency - a process in which we sam-
ple multiple chain-of-thought rationales and aver-
age the final preferences - do not improve accuracy
or even degrade it. Finally, we conduct scaling
experiments to quantify the trade-offs between the
size of the LLM labeler and the number of prefer-
ence examples used in training vs. alignment with
human preferences.
Our main contributions are the following:
• We demonstrate that RLAIF achieves com-
parable performance to RLHF on the task of
summarization
• We compare various techniques for generat-
ing AI labels and identify optimal settings for
RLAIF practitioners
2
Preliminaries
We first review the RLHF pipeline introduced in
Stiennon et al. (2020); Ouyang et al. (2022), which
consists of 3 phases: supervised fine-tuning, reward
model training, and reinforcement learning-based
fine-tuning.
2.1
Supervised Fine-tuning
A pre-trained LLM is fine-tuned on a high qual-
ity labeled dataset for a downstream task using
token-level supervision to produce a supervised
fine-tuned (SFT) model πSFT .
2.2
Reward Modeling
Given an input x, we sample a pair of responses
from one or more models (y1, y2) ∼ π, where of-
tentimes the SFT model is used. The input and
responses are sent to human annotators to rate
which response is better according to some cri-
teria. These annotations form a dataset of triplets
D = {(x, yw, yl)}, where yw and yl are the pre-
ferred and non-preferred responses, respectively.
A reward model rϕ is trained by minimizing the
following loss:
Lr(ϕ) =
−E
(x,yw,yl)∼D[log σ(rϕ(x, yw) − rϕ(x, yl))]
where σ is the sigmoid function.