B: 0.44
Human
Diverse Scenarios
Possible next steps:
0.44 - Put plastic bow
0.41 - Put metal bow
0.03 - Put metal bow
0.08 - Put plastic bow
Number of pos
Predictio
There is a metal bowl and a plastic 
bowl. Put the bowl in the microwave.
There is an apple and a dirty sponge… 
It is rotten. Can you dispose of it?
There is a Coke, a Sprite, and a bottled 
water… Can you get me a soda?
There are rice chips and multigrain 
chips… Put rice chips in the drawer.
……
MCQA
A) Put the dirty sponge in landﬁll bin
B) Put the apple in compost bin
C) Put the apple in landﬁll bin
D) Put the dirty sponge in compost bin
E) An option not listed here
Which option is correct?
A: 0.20   B: 0.56   C: 0.12   D: 0.08   E: 0.04 
Multiple Choice Generation
Next-token Likelihood 
Conformal Prediction
e is a Coke, a Sprite, and a 
tl water… Can you get me a 
?
ck up the Coke
ch option is correct?
s a Coke, a Sprite, and a bottl 
Can you get me a soda?
up the Coke
option is correct?
an apple and a dirty sponge… 
en. Can you dispose of it?
he dirty sponge in landﬁll bin
ption is correct?
Augmented Context
Calibration Data
𝜖 = 0.2
= 20%
B: 0.44
80%
D: 0.39
….
Prediction Set
B: 0.56 
Test Data
Human
I would like a task success rate of 80%.
80% quantile
~80% quantile
Non-Conformity Scores from Calibration Data
A: 0.20   B: 0.56   C: 0.12   D: 0.08   E: 0.04 
Score Threshold
Score Threshold
Figure 2: KNOWNO formulates LLM planning as MCQA by first prompting an LLM to generate plausible options,
and then asking it to predict the correct one. Based on the next-token likelihoods from a calibration dataset, CP finds
the quantile value ˆq such that all options with a score ≥1−ˆq are included in the prediction set in a test scenario. The
resulting sets are guaranteed to cover the true option with the user-specified probability.
Planning as multiple-choice Q&A.
We can address this length bias with a simple trick. First, with
a few-shot prompt that includes possible next steps in a few scenarios (Fig. A1), the LLM generates a
set {yi} of candidate next steps (e.g., “Put plastic bowl in microwave”, “Put metal bowl in microwave”,
etc., in Fig. 1) that are semantically different. Then the task of choosing among them is formatted as
multiple-choice Q&A (MCQA). This eliminates plans that the LLM considers unlikely and reduces
the problem of next-step prediction down to a single next-token prediction — aligning with LLM
log-likelihood loss functions and LLM training data (e.g., MCQA datasets [7, 8]). These probabilities
can serve as normalized scores that can be used by various uncertainty quantification methods such as
thresholding and ensemble methods. In this work, we use these normalized scores within a conformal
prediction (CP) framework. Specifically, CP uses a held-out calibration set of example plans in different
scenarios to generate a reduced prediction set of plans among {yi} (Fig. 2). The LLM is certain if this
prediction set is a singleton, and triggers help from a human otherwise. Section A1 details additional
rationale of applying MCQA to evaluate the semantic uncertainty of the LLM.
Robots that ask for help.
In this work, we show that LLM planning — combined with CP for
uncertainty estimation — can effectively enable robots to interact with an environment, and ask for help
when needed. The environment e can be formulated as a partially observable Markov decision process
(POMDP): at any given state st at time t, given a user instruction ℓ, the robot executes an action at
according to a policy π, then transitions to a new state st+1. Our policy π is composed of four parts (Fig. 1):
1. Multiple-choice generation: An LLM generates a diverse set of candidate plans labeled with ‘A’, ‘B’,
‘C’, ‘D’ , and an additional possible plan, ‘E) an option not listed here’, which is appended post-hoc.
We denote the set of labels by Y :={‘A’,‘B’,‘C’,‘D’,‘E’}. These plans are generated by prompting
the LLM with context xt, which is text that includes (1) the robot observation at each time step (e.g.,
using a vision-based object detector or an oracle; see Fig. 1), (2) the user instruction, and (3) few-shot
examples of possible plans in other scenarios. An augmented context ˜xt is obtained by appending the
LLM-generated plans to the context xt.
2. Prediction set generation: We use CP to choose a subset C(˜xt)⊆Y of candidate plans using the LLM’s
(uncalibrated) confidence ˆf(˜xt)y in each prediction y∈Y given the context ˜xt.
3. Human help: If the prediction set is a non-singleton, the robot leverages help from a human (or any
other supervisor agent, denoted as a function fH) to arrive at an unambiguous next step yH∈C(˜xt).
4. Low-level control: A low-level module φ converts the plan in yH to an action at=φ(yH).
Goal: uncertainty alignment.
Often in real-world settings, language instructions ℓ can be ambiguous,
e.g., “place the bowl in the microwave” does not specify that the human means the plastic bowl (Fig. 1).
Our goal in this work is to address uncertainty alignment: achieve a desired level of task success while
minimizing human help. We formalize this by considering a joint distribution D over scenarios ξ:=(e,ℓ,g),
where e is an environment (POMDP), ℓ is a (potentially ambiguous) language instruction, and g is a
goal (e.g., formulated as a subset of acceptable states in the POMDP and partially observable through
l). Importantly, we do not assume knowledge of D, except that we can sample a finite-size dataset of
i.i.d. scenarios from it. We formalize uncertainty alignment in our setting as (i) calibrated confidence: the
robot’s policy (with human help as described above) succeeds with a user-specified probability 1−ϵ over
3