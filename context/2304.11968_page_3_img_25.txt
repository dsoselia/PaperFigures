Frame 0
0%
25%
50%
75%
100%
Figure 2: Qualitative results on video sequences from DAVIS-16 and DAVIS-17 datasets [13].
#000
#040
#080
#120
#160
#200
Frame 0
0%
25%
50%
75%
100%
Seq (a)
Seq (b)
Figure 3: Failed cases.
Step 4: Correction with human participation. After the above three steps, the TAM can now
successfully solve some common challenges and predict segmentation masks. However, we notice
that it is still difﬁcult to accurately distinguish the objects in some extremely challenging scenarios,
especially when processing long videos. Therefore, we propose to add human correction during
inference, which can bring a qualitative leap in performance with only very small human efforts. In
detail, users can compulsively stop the TAM process and correct the mask of the current frame with
positive and negative clicks.
4
Experiments
4.1
Quantitative Results
To evaluate TAM, we utilize the validation set of DAVIS-2016 and test-development set of DAVIS-
2017 [13]. Then, we execute the proposed TAM as demonstrated in Section 3.2. The results are
given in Table 1. As shown, our TAM obtains J&F scores of 88.4 and 73.1 on DAVIS-2016-val and
DAVIS-2017-test-dev datasets, respectively. Note that TAM is initialized by clicks and evaluated in
one pass. Notably, we found that TAM performs well when against difﬁcult and complex scenarios.
4.2
Qualitative Results
We also give some qualitative results in Figure 2. As shown, TAM can handle multi-object separation,
target deformation, scale change, and camera motion well, which demonstrates its superior tracking
and segmentation abilities within only click initialization and one-round inference.
4