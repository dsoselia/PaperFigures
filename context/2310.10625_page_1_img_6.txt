Step 1: push blue triangle to …
Step 1: push red star to left …
Step 2: (re-plan)
t
t + k
t current
t goal
Step 2: (re-plan)
Step 2: (re-plan)
Figure 1: Video Language Planning uses forward tree search via vision-language models and text-to-video
models to construct long-horizon video plans. From an image observation, the VLM policy (top left) generates
next-step text actions, which a video model converts into possible future image sequences (top right). Future
image states are evaluated using a VLM heuristic function (bottom left), and the best sequence is recursively
expanded with tree search (middle). Video plans can be converted to action execution with goal-conditioned
policies (bottom right).
ways that are more information-rich than text, and (ii) can absorb another source of Internet data e.g.,
YouTube videos. This leads to the natural question of how to build a planning algorithm that can
leverage both long-horizon abstract planning from LLMs / VLMs and detailed dynamics and motions
from text-to-video-models.
In this work, we propose to integrate vision-language models and text-to-video models to enable video
language planning (VLP), where given the current image observation and a language instruction, the
agent uses a VLM to infer high-level text actions, and a video model to predict the low-level outcomes
of those actions. Specifically, VLP (illustrated in Fig. 1) synthesizes video plans for long-horizon
tasks by iteratively: (i) prompting the VLM as a policy to generate multiple possible next-step text
actions, (ii) using the video model as a dynamics model to simulate multiple possible video rollouts
for each action, and (iii) using the VLM again but as a heuristic function to assess the favorability of
each rollout in contributing task progress, then recursively re-planning with (i).
The combination of both models enables forward tree search over the space of possible video se-
quences to discover long-horizon plans (of hundreds of frames) that respect visual dynamics. In partic-
ular, VLP offers advantages in that it (i) can generate higher quality plans at inference time by expand-
ing the branching factor of the search, allowing plan quality to scale with increasing compute budget,
and (ii) benefits from training on incomplete language-labeled video data, which may contain short-
horizon snippets (that can be re-composed and sequenced into long-horizon ones), or segments of
videos with missing language labels (but contain dynamics that the video model can still learn from).
Experiments in both simulated and real settings (on 3 robot hardware platforms) show that VLP
generates more complete and coherent multimodal plans than baselines, including end-to-end models
trained directly to generate long videos conditioned on long-horizon task instructions. VLPs exhibit
improved grounding in terms of the consistency of scene dynamics in video plans, and when used in
conjunction with inverse dynamics models or goal-conditioned policies to infer control trajectories
(Du et al., 2023b), can be deployed on robots to perform multi-step tasks – from picking and stowing
a variety of objects over countertop settings, to pushing groups of blocks that rearrange them into new
formations. In terms of execution success, VLP-based systems are far more likely to achieve task
completion for long-horizon instructions than state-of-the-art alternatives, including PaLM-E (Driess
et al., 2023) and RT-2 (Brohan et al., 2023) directly fine-tuned for long-horizon tasks. We also observe
that when co-trained on Internet-scale data, VLP generalizes to new objects and configurations.
2