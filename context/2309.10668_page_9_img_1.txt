Figure 6 | In-context compression rate over sequence length. For every dataset, we compute the
compression rate for all subsequences of 2048 bytes, averaged over 100 sequences.
3.5. Sequential Evolution of In-Context Compression
Language models take a very different “approach” to compression compared to classical compressors.
Classical compressors have a small program size and optimize for a large context length to exploit
sequential dependencies in the data. In contrast, foundation models consist of billions of parameters,
which enable rapid adaptation in their (relatively) short context window (Genewein et al., 2023).
Thus, arithmetic coding-based compressors rely heavily on the predictive models’ in-context learning
capabilities to achieve competitive compression performance. We investigate this phenomenon in
Fig. 6, which visualizes the compression rate across sequence lengths for gzip, Chinchilla 1B and a
Transformer pretrained on enwik8. Intuitively, the longer the sequence, the more data the model
can process in its context, and therefore, the better the compression. As expected, most compression
rates decrease quickly with increasing sequence length, indicating that the models learn some data
statistics in-context, without any gradient-based training. As in Table 1, the Chinchilla model achieves
the best compression rates accross all three data modalities and sequence lengths.