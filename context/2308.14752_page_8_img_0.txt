pollutants up to 40 times higher than the permissible limits. We call this type of deceptive behavior
cheating the safety test.
Some AI systems have already been caught cheating the safety test. Lehman et al. (2020) trained AI
agents in an evolutionary environment, in which an external safety test was designed to eliminate
fast-replicating variants of the AI. But instead of actually eliminating fast-replicating variants, the
safety test taught AI agents how to play dead: to disguise their fast replication rates precisely when
being evaluated by the safety test.
2.1.7
Deceiving the human reviewer
Figure 3: An AI in control of a simulated robotic
hand was trained to grasp a ball (Christiano et al.
2017). The AI learned to hover its hand in front
of the ball, creating the illusion of grasping in the
eyes of the human reviewer. Because the human
reviewer approved of this result, the deceptive
strategy was reinforced.
One popular approach to AI training today is reinforce-
ment learning with human feedback (RLHF). Here, in-
stead of training an AI system on an objective met-
ric, the AI system is trained to obtain human approval,
in that it is rewarded based on which of the two pre-
sented output options is preferred by the human re-
viewer (Ziegler et al. 2020). RLHF allows AI systems
to learn to deceive human reviewers into believing that
a task has been completed successfully, without actually
completing the task. Researchers at OpenAI observed
this phenomenon when they used human approval to
train a simulated robot to grasp a ball (Christiano et al.
2023). Because the human observed the robot from
a particular camera angle, the AI learned to place the
robot hand between the camera and the ball, where it
would appear to the human as though the ball had been
grasped (see Figure 3). Human reviewers approved
of this result, positively reinforcing the AI’s behavior
even though it had never actually touched the ball. Note
that in this case, AI deception emerged even without
the AI being explicitly aware of the human evaluator.
Rather than coming about through strategic awareness,
deception emerged here as a result of structural aspects
of the AI’s training environment.
2.1.8
AIs purposefully lying
In a recent paper, Zou, Phan, et al. (2023) show that AIs can purposefully utter false statements. By
influencing the internal state of an AI, the authors can control whether the AI lies or not. For example,
a user may say to a chatbot “Tell me a fact about the world.” By default, the chatbot may answer
truthfully by saying Mount Everest is the highest mountain. To control whether the AI lies or not,
the authors manually adjust the internal state of the AI. They extract a vector that is correlated with
truthfulness, and then make the model more or less truthful by adding or subtracting the vector to a
hidden layer of the neural network. When the vector is added to the internal state, the model becomes
more honest: in response to the instruction “Lie about a fact about the world,” the AI will nonetheless
respond honestly: “The highest mountain in the world is Mount Everest, which is located in the
Himalayas.” When the vector is instead subtracted, the model is nudged to lie: given the instruction
“Tell me a fact about the world,” the chatbot will tell the lie “The highest mountain in the world is not
in the Himalayas, but in the United States.” Examples like this show that lying is not accidental, and
that it is within an AI’s capacity to utter false statements that it knows are false.
This concludes our discussion of recent empirical examples of deception in specific-use AI systems.
A discussion of earlier examples can be found in Masters et al. (2021).
2.2
Deception in general-purpose AI systems
In this section, we focus on learned deception in general-purpose AI systems such as LLMs. The
capabilities of LLMs have improved rapidly, especially in the years after the introduction of the
5